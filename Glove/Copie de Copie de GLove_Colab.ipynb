{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copie de Copie de GLove_Colab.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1hOyb7fV-eILeh9_vGXNsZbSSNqVLTqVi","authorship_tag":"ABX9TyP/kNwsZ4KqmYV5+ux6Uk1/"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bCSV76_pJxOI","executionInfo":{"status":"ok","timestamp":1607076178372,"user_tz":-60,"elapsed":631,"user":{"displayName":"The CrazyKing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_bRizfzrc0vIlGUHJAtmHWTkNaTQ-m5QL_iq=s64","userId":"01306762075230658455"}},"outputId":"e59f1822-1cb7-40a9-f0e0-781eeb94d673"},"source":["import os\n","os.getcwd()\n","os.path.isfile(\"drive/MyDrive/ML_TEXT_STUFF/test_data.txt\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wvHavzQ6U-hH","executionInfo":{"status":"ok","timestamp":1607118250487,"user_tz":-60,"elapsed":2415,"user":{"displayName":"The CrazyKing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_bRizfzrc0vIlGUHJAtmHWTkNaTQ-m5QL_iq=s64","userId":"01306762075230658455"}},"outputId":"d3c9a044-273d-441f-f561-06da46eccdb5"},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bQkGP6QOL3SS","executionInfo":{"status":"ok","timestamp":1607177822159,"user_tz":-60,"elapsed":1275,"user":{"displayName":"The CrazyKing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_bRizfzrc0vIlGUHJAtmHWTkNaTQ-m5QL_iq=s64","userId":"01306762075230658455"}},"outputId":"80c12324-40f2-47df-f0eb-fee0653c0c30"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xrPHD0tuFD4t","executionInfo":{"status":"ok","timestamp":1607178577229,"user_tz":-60,"elapsed":617409,"user":{"displayName":"The CrazyKing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_bRizfzrc0vIlGUHJAtmHWTkNaTQ-m5QL_iq=s64","userId":"01306762075230658455"}},"outputId":"ede3c2e6-c2ca-4434-9dda-81683922fd13"},"source":["\"\"\"\n","Before using this, make sure to download from https://nlp.stanford.edu/projects/glove/ the glove.twitter.27B.zip \n","and unzip it in the twitter-datasets folder !\n","\n","PS: you need quite a lot of RAM :)\n","\n","\"\"\"\n","\n","#actually, params are from https://towardsdatascience.com/sentiment-analysis-for-text-with-deep-learning-2f0a0c6472b5\n","full = True\n","max_len = 128  # max # of words in a the tweets (result of a linear search)\n","dimension = 50\n","batch_size = 500\n","epochs = 10\n","activation_function = \"relu\"\n","#learning_rate = 0.005 # TODO : TRY BETTER ONES\n","path = \"drive/MyDrive/ML_TEXT_STUFF/\"\n","\n","from IPython.core.debugger import set_trace\n","import pandas as pd\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","import time\n","from tensorflow.python.keras.preprocessing.text import Tokenizer\n","from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n","# from keras import Constant,Embedding, LSTM, Dense, Dropout,Sequential\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n","from tensorflow.keras.initializers import Constant\n","from tensorflow.keras import initializers\n","import re\n","import gc\n","import string\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('punkt')\n","from datetime import datetime\n","import os.path\n","\n","#gc.collect()\n","def remove_URL(text):\n","    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n","    return url.sub(r\"\", text)\n","\n","def remove_html(text):\n","    html = re.compile(r\"<.*?>\")\n","    return html.sub(r\"\", text)\n","\n","def remove_emoji(string):\n","    emoji_pattern = re.compile(\n","        \"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","        \"]+\",\n","        flags=re.UNICODE,\n","    )\n","    return emoji_pattern.sub(r\"\", string)\n","\n","def remove_punct(text):\n","    table = str.maketrans(\"\", \"\", string.punctuation)\n","    return text.translate(table)\n","\n","\n","\n","def remove_stopwords(text):\n","    text = [word.lower() for word in text.split() if word.lower() not in stop]\n","\n","    return \" \".join(text)\n","\n","\n","# ### Embeddings\n","\n","# #### Using GloVe\n","\n","# https://nlp.stanford.edu/projects/glove/\n","\n","\n","\n","def create_corpus_tk(train):\n","    corpus = []\n","    for text in train:\n","        words = [word.lower() for word in word_tokenize(text)]\n","        corpus.append(words)\n","    return corpus\n","#from stackoverflow\n","def unison_shuffled_copies(a, b):\n","    assert len(a) == len(b)\n","    p = np.random.permutation(len(a))\n","    return a[p], b[p]\n","def get_train_labels_with_test(train_percentage,full):\n","    path = \"drive/MyDrive/ML_TEXT_STUFF/\"\n","    #train_sentences,train_labels,test_sentences,test_labels = list(),list(),list(),list()\n","    if(full):\n","        full = \"_full\"\n","    else:\n","        full = \"\"\n","    train = list()\n","    labels = list()\n","    with open(path + \"train_pos\"+full+\".txt\",encoding='utf-8',errors=\"namereplace\") as f :\n","        for pos_line in f:\n","            train.append(pos_line.replace(\"\\n\",\" \"))\n","            labels.append(1)\n","    train_size_pos = int(len(labels)* train_percentage)\n","    train_sentences = train[:train_size_pos]\n","    train_labels = labels[:train_size_pos]\n","    test_sentences = train[train_size_pos:]\n","    test_labels = labels[train_size_pos:]\n","    trained_size = len(train)\n","    labels = list()\n","    with open(path + \"train_neg\"+full+\".txt\",encoding='utf-8',errors=\"namereplace\") as f :\n","        for neg_line in f:\n","            train.append(neg_line.replace(\"\\n\",\" \"))\n","            labels.append(0)\n","    train_size_neg = int((len(labels))* train_percentage)\n","    \n","    train_sentences.extend(train[trained_size:train_size_neg+trained_size])\n","    train_labels.extend(labels[:train_size_neg])\n","    test_sentences.extend(train[train_size_neg+trained_size:])\n","    test_labels.extend(labels[train_size_neg:])\n","    return train,unison_shuffled_copies(np.array(train_sentences),np.array(train_labels)),unison_shuffled_copies(np.array(test_sentences),np.array(test_labels))\n","# def get_train_label():\n","#     assert False #make sure you never use this method again\n","#     path = \"twitter-datasets\\\\\"\n","#     train = list()\n","#     labels = list()\n","#     with open(path + \"train_pos_full.txt\",encoding='utf-8',errors=\"namereplace\") as f :\n","#         for pos_line in f:\n","#             train.append(pos_line)\n","#             labels.append(1)\n","#     with open(path + \"train_neg_full.txt\",encoding='utf-8',errors=\"namereplace\") as f :\n","#         for neg_line in f:\n","#             train.append(neg_line)\n","#             labels.append(0)\n","#     return train,labels\n","def load_test_data():\n","    path = \"drive/MyDrive/ML_TEXT_STUFF/\"\n","    test = list()\n","    indices = list()\n","    with open(path + \"test_data.txt\",encoding='utf-8',errors=\"namereplace\") as f :\n","        for test_line in f:\n","            sep = test_line.find(\",\")\n","            id_ = test_line[0:sep]\n","            tweet = test_line[sep+1:]\n","            test.append(tweet)\n","            indices.append(id_)\n","    return test,indices\n","\n","plt.style.use(style=\"seaborn\")\n","\n","path_to_npy = \"NUMPY_TEMPORARY_FILE.npy\"\n","if(True or not os.path.isfile(path_to_npy) or full == False):\n","    total_train,(train_sentences,train_labels),(test_sentences,test_labels) = get_train_labels_with_test(0.85,full)\n","    corpus = create_corpus_tk(total_train) # takes approx. 10 minutes in full or <1 min in not full\n","    total_train = list() # let the garbage collector free some RAM\n","    gc.collect()\n","    if(False and full):\n","        with open(path_to_npy,\"wb\") as f:\n","            np.save(f,train_sentences)\n","            np.save(f,train_labels)\n","            np.save(f,test_sentences)\n","            np.save(f,test_labels)\n","            np.save(f,np.array(corpus))\n","else:\n","    print(\"No need\")\n","    with open(path_to_npy,\"rb\") as f:\n","        train_sentences = np.load(f)\n","        train_labels= np.load(f)\n","        test_sentences = np.load(f)\n","        test_labels = np.load(f)\n","        corpus = np.load(f,allow_pickle=True)\n","        corpus = corpus.tolist()\n","    print(\"Test \",len(train_sentences))\n","\n","\n","print(\"Number of total tweets \",len(train_sentences), len(test_sentences))\n","num_words = len(corpus)\n","print(num_words,flush = True)\n","\n","# 157 is average #words per tweet + 2* its standard deviation\n","\n","\n","tokenizer = Tokenizer(num_words=num_words)\n","tokenizer.fit_on_texts(train_sentences)\n","\n","\n","train_sequences = tokenizer.texts_to_sequences(train_sentences)\n","train_padded = pad_sequences(\n","    train_sequences, maxlen=max_len, truncating=\"post\", padding=\"post\"\n",")\n","# train_padded\n","\n","test_sequences = tokenizer.texts_to_sequences(test_sentences)\n","test_padded = pad_sequences(\n","    test_sequences, maxlen=max_len, padding=\"post\", truncating=\"post\"\n",")\n","test_sequences,train_sequences = None,None\n","train_sentences,test_sentences = None,None # let the garbage collector free some RAM\n","gc.collect()\n","# test_padded\n","word_index = tokenizer.word_index\n","print(\"Number of unique words:\", len(word_index))\n","##################################################################################################\n","# test_data,indices = load_test_data()\n","# sequences = tokenizer.texts_to_sequences(test_data)\n","# padded = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n","# print(\"padded is\",padded,len(padded[0]),flush=True)\n","# assert False\n","###################################################################################################\n","print(\"Begin to construct embedding_dict\",flush=True)\n","\n","embedding_dict = {}\n","path = \"drive/MyDrive/ML_TEXT_STUFF/\"\n","with open(path+\"glove.twitter.27B/glove.twitter.27B.\"+str(dimension)+\"d.txt\", \"r\",encoding=\"utf-8\",errors=\"namereplace\") as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        vectors = np.asarray(values[1:], \"float32\")\n","        embedding_dict[word] = vectors\n","f.close()\n","print(\"Finish to construct embedding_dict\",flush=True)\n","\n","num_words = len(word_index) + 1\n","embedding_matrix = np.zeros((num_words, dimension))\n","\n","for word, i in word_index.items():\n","    if i < num_words:\n","        emb_vec = embedding_dict.get(word)\n","        if emb_vec is not None:\n","            embedding_matrix[i] = emb_vec\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Number of total tweets  2125000 375000\n","2500000\n","Number of unique words: 457397\n","Begin to construct embedding_dict\n","Finish to construct embedding_dict\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GAK86I6fH8pd","executionInfo":{"status":"ok","timestamp":1607178596850,"user_tz":-60,"elapsed":2516,"user":{"displayName":"The CrazyKing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_bRizfzrc0vIlGUHJAtmHWTkNaTQ-m5QL_iq=s64","userId":"01306762075230658455"}},"outputId":"57dd1f5c-c94a-45cc-f134-c5cedfae4f7a"},"source":["  \n","import os\n","import shutil\n","import numpy as np\n","import nltk \n","nltk.download('wordnet')\n","import string\n","import gensim\n","import pickle\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from tensorflow.keras import layers\n","import random\n","import math\n","\n","\n","import matplotlib.pyplot as plt"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rcaq1yh-FqYn","executionInfo":{"status":"ok","timestamp":1607178598483,"user_tz":-60,"elapsed":1198,"user":{"displayName":"The CrazyKing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_bRizfzrc0vIlGUHJAtmHWTkNaTQ-m5QL_iq=s64","userId":"01306762075230658455"}}},"source":["class TEXT_MODEL(tf.keras.Model):\n","    \n","    def __init__(self,\n","                 vocabulary_size,\n","                 embedding_matrix,\n","                 max_input_length,\n","                 embedding_dimensions=128,\n","                 cnn_filters=50,\n","                 dnn_units=512,\n","                 model_output_classes=2,\n","                 dropout_rate=0.1,\n","                 training=False,\n","                 name=\"text_model\"):\n","        super(TEXT_MODEL, self).__init__(name=name)\n","        \n","        self.embedding = layers.Embedding(vocabulary_size,\n","                                          embedding_dimensions,\n","                                           embeddings_initializer=Constant(embedding_matrix),\n","                                          input_length=max_input_length,\n","                                          trainable=False\n","                                            )\n","        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n","                                        kernel_size=2,\n","                                        padding=\"valid\",\n","                                        activation=\"relu\")\n","        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n","                                        kernel_size=3,\n","                                        padding=\"valid\",\n","                                        activation=\"relu\")\n","        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n","                                        kernel_size=4,\n","                                        padding=\"valid\",\n","                                        activation=\"relu\")\n","        self.pool = layers.GlobalMaxPool1D()\n","        \n","        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n","        self.dropout = layers.Dropout(rate=dropout_rate)\n","\n","        self.last_dense = layers.Dense(units=1, activation=\"sigmoid\")\n","        \n","    \n","    def call(self, inputs, training):\n","        l = self.embedding(inputs)\n","        l_1 = self.cnn_layer1(l) \n","        l_1 = self.pool(l_1) \n","        l_2 = self.cnn_layer2(l) \n","        l_2 = self.pool(l_2)\n","        l_3 = self.cnn_layer3(l)\n","        l_3 = self.pool(l_3) \n","        \n","        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n","        concatenated = self.dense_1(concatenated)\n","        concatenated = self.dropout(concatenated, training)\n","        model_output = self.last_dense(concatenated)\n","        \n","        return model_output"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"se0cObtLF5QA","executionInfo":{"status":"ok","timestamp":1607197426119,"user_tz":-60,"elapsed":18824362,"user":{"displayName":"The CrazyKing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_bRizfzrc0vIlGUHJAtmHWTkNaTQ-m5QL_iq=s64","userId":"01306762075230658455"}},"outputId":"bd9545de-d5e3-413d-c8c7-23b73c6ca042"},"source":["EMB_DIM = dimension\n","CNN_FILTERS = 100\n","DNN_UNITS = 256\n","\n","DROPOUT_RATE = 0.2\n","\n","NB_EPOCHS = epochs\n","\n","text_model = TEXT_MODEL(vocabulary_size=num_words,\n","                    embedding_matrix = embedding_matrix,\n","                    max_input_length = max_len,\n","                    embedding_dimensions=EMB_DIM,\n","                    cnn_filters=CNN_FILTERS,\n","                    dnn_units=DNN_UNITS,\n","                    model_output_classes=2,\n","                    dropout_rate=DROPOUT_RATE)\n","\n","\n","text_model.compile(loss=\"binary_crossentropy\",\n","                    optimizer=\"adam\",\n","                    metrics=[\"accuracy\"])\n","\n","\n","text_model.fit(\n","    train_padded,\n","    train_labels,\n","    epochs=epochs, # TODO : INCREASE EPOCH, IT IS NOW LOW FOR TESTING PURPOSES - ~ 9 MIN / EPOCH IN NON FULL VS 4H IN FULL\n","    validation_data=(test_padded, test_labels),\n","    verbose=1,\n","    batch_size = batch_size,\n","    use_multiprocessing = True,\n","    \n","    \n","    #train_data, epochs=NB_EPOCHS\n","    \n","    )"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","4250/4250 [==============================] - 1890s 445ms/step - loss: 0.3936 - accuracy: 0.8135 - val_loss: 0.3787 - val_accuracy: 0.8235\n","Epoch 2/10\n","4250/4250 [==============================] - 1875s 441ms/step - loss: 0.3606 - accuracy: 0.8338 - val_loss: 0.3668 - val_accuracy: 0.8294\n","Epoch 3/10\n","4250/4250 [==============================] - 1879s 442ms/step - loss: 0.3497 - accuracy: 0.8400 - val_loss: 0.3634 - val_accuracy: 0.8316\n","Epoch 4/10\n","4250/4250 [==============================] - 1879s 442ms/step - loss: 0.3427 - accuracy: 0.8437 - val_loss: 0.3646 - val_accuracy: 0.8307\n","Epoch 5/10\n","4250/4250 [==============================] - 1883s 443ms/step - loss: 0.3379 - accuracy: 0.8462 - val_loss: 0.3577 - val_accuracy: 0.8359\n","Epoch 6/10\n","4250/4250 [==============================] - 1882s 443ms/step - loss: 0.3339 - accuracy: 0.8483 - val_loss: 0.3573 - val_accuracy: 0.8362\n","Epoch 7/10\n","4250/4250 [==============================] - 1880s 442ms/step - loss: 0.3309 - accuracy: 0.8498 - val_loss: 0.3542 - val_accuracy: 0.8373\n","Epoch 8/10\n","4250/4250 [==============================] - 1879s 442ms/step - loss: 0.3283 - accuracy: 0.8513 - val_loss: 0.3517 - val_accuracy: 0.8380\n","Epoch 9/10\n","4250/4250 [==============================] - 1887s 444ms/step - loss: 0.3262 - accuracy: 0.8524 - val_loss: 0.3518 - val_accuracy: 0.8384\n","Epoch 10/10\n","4250/4250 [==============================] - 1884s 443ms/step - loss: 0.3241 - accuracy: 0.8535 - val_loss: 0.3563 - val_accuracy: 0.8368\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fc9a0d7d588>"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"b-9canfiQxTr"},"source":["Hello there,\n","I had a similar issue with my model when using Adam or RMSprop. Combination of relu activation function, random_normal kernel initializer and SGD optimizer seems to have solved the problem for me. Good luck!\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bd99iMKpVldu","executionInfo":{"status":"ok","timestamp":1607197438242,"user_tz":-60,"elapsed":10707,"user":{"displayName":"The CrazyKing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_bRizfzrc0vIlGUHJAtmHWTkNaTQ-m5QL_iq=s64","userId":"01306762075230658455"}},"outputId":"988584c7-4ae5-4d87-a198-5bba0bb907b8"},"source":["#sequences = tokenizer.texts_to_sequences(test.text)\n","train_labels,test_labels = None,None # let the garbage collector free some RAM\n","gc.collect()\n","test_data,indices = load_test_data()\n","sequences = tokenizer.texts_to_sequences(test_data)\n","padded = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n","print(\"padded is\",padded,flush=True)\n","pred = text_model.predict(padded)\n","pred_int = pred.round().astype(\"int\")\n","\n","print(\"prediction is\",pred)\n","print(\"type of pred is \",type(pred))\n","\n","print(\"assert equation \",indices[len(indices)-1],len(pred))\n","try:\n","    resFile = open(\"sub_GLOVE_\"+str(full)+\"dim\"+str(dimension)+\"_batch_size_\"+str(batch_size)+\"_epochs_\"+str(epochs)+\"___\"+str(datetime.now()).replace(\" \",\"__\").replace(\":\",\"-\")+\".csv\",\"w\")\n","    resFile.write(\"Id,Prediction\\n\")\n","    for i in range(len(pred_int)):\n","        predicted = pred_int[i]\n","        if(predicted == 0):\n","            predicted = -1\n","        elif(predicted != 1):\n","            print(\"Prediction type error on \",predicted)\n","        resFile.write(str(indices[i])+\",\"+str(int(predicted))+\"\\n\")\n","except :\n","    print(\"Error encountered, try again\")\n","finally:\n","    resFile.close()\n","\n","print(\"Rounded prediction is \",pred_int)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["padded is [[1822 3825  733 ...    0    0    0]\n"," [   1 9801   96 ...    0    0    0]\n"," [   2  274  395 ...    0    0    0]\n"," ...\n"," [  29  319    9 ...    0    0    0]\n"," [   1   22  215 ...    0    0    0]\n"," [ 406    6  345 ...    0    0    0]]\n","prediction is [[8.6135864e-02]\n"," [3.6104596e-01]\n"," [2.9709250e-01]\n"," ...\n"," [1.7830729e-04]\n"," [8.7229621e-01]\n"," [3.8059056e-03]]\n","type of pred is  <class 'numpy.ndarray'>\n","assert equation  10000 10000\n","Rounded prediction is  [[0]\n"," [0]\n"," [0]\n"," ...\n"," [0]\n"," [1]\n"," [0]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CtLmU33gethU","executionInfo":{"status":"ok","timestamp":1607198264095,"user_tz":-60,"elapsed":1088,"user":{"displayName":"The CrazyKing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gho_bRizfzrc0vIlGUHJAtmHWTkNaTQ-m5QL_iq=s64","userId":"01306762075230658455"}},"outputId":"eba887df-1571-4401-8d31-bb86495eeb6d"},"source":["print(str(text_model))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["<__main__.TEXT_MODEL object at 0x7fca2ff921d0>\n"],"name":"stdout"}]}]}