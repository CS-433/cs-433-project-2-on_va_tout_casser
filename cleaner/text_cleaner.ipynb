{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd \n",
    "import string\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('words')\n",
    "infl = getAllInflections\n",
    "i = infl('begin')\n",
    "inf = []\n",
    "for key in i.keys():\n",
    "    inf_k = i[key]\n",
    "    inf += list(inf_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['began', 'begun', 'beginning', 'begins', 'begin', 'begin', 'hello']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf + ['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'RB'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-ba4bd241b733>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'RB'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'RB'"
     ]
    }
   ],
   "source": [
    "i['RB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_of_inflections(word):\n",
    "    \"\"\" Function takes a word (as a string) and returns a list of \n",
    "        its possible inflections using a lemminflect method.\n",
    "    \"\"\"\n",
    "    \n",
    "    inflections = getAllInflections(word)\n",
    "    inf = []\n",
    "    # getAllInflections is a dictionary, and we want a list of the words.\n",
    "    for key in inflections.keys():\n",
    "        inf_k = inflections[key]\n",
    "        inf += list(inf_k)  # Transform the tuples into lists\n",
    "\n",
    "    return inf        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(words.words())\n",
    "#dic = set(words.words())\n",
    "#print(len(dic))\n",
    "fname = 'pos_train.txt'\n",
    "with open(fname, 'r') as fh:\n",
    "    lines = fh.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\n",
      "because your logic is so dumb , i won't even crop out your name or your photo . tsk . <url>\n",
      "\" <user> just put casper in a box ! \" looved the battle ! #crakkbitch\n",
      "<user> <user> thanks sir > > don't trip lil mama ... just keep doin ya thang !\n",
      "visiting my brother tmr is the bestest birthday gift eveerrr ! ! !\n",
      "<user> yay ! ! #lifecompleted . tweet / facebook me to let me know please\n",
      "<user> #1dnextalbumtitle : feel for you / rollercoaster of life . song cocept : life , #yolo , becoming famous ? <3 14 #followmeplz ! <3 x15\n",
      "workin hard or hardly workin rt <user> at hardee's with my future coworker <user>\n",
      "<user> i saw . i'll be replying in a bit .\n",
      "this is were i belong\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate(lines):\n",
    "    if i == 10: break\n",
    "    else:\n",
    "        print(line.strip())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary_with_inflections(dictionary):\n",
    "    \"\"\" Function takes an english dictionary (from nltk) as a list, and adds all possible inflections of words\n",
    "        using the lemminflect module. It then returns a set of such words.\n",
    "    \"\"\"\n",
    "    new_ldic = []\n",
    "    for word in dictionary:\n",
    "        inf = get_list_of_inflections(word)\n",
    "        \n",
    "        if not inf:  # If the list of inflections is empty, makes sure to add the word still.\n",
    "            new_ldic += [word]\n",
    "        else:\n",
    "            new_ldic += get_list_of_inflections(word)\n",
    "    \n",
    "    # Remove redundancies :\n",
    "    new_dic = set(new_ldic)\n",
    "    return new_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['aardvarks', 'aardvark']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "small_alt_dic = create_dictionary_with_inflections(small_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_dic = create_dictionary_with_inflections(words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263192"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alt_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text, excepts='!#<\\'', numbers=True):\n",
    "    \"\"\" Removes the punctuation from a text, except the ones in except (a string).\"\"\"\n",
    "    text = text.lower()  # Remove caps\n",
    "    punct = string.punctuation\n",
    "    \n",
    "    for symb in excepts:\n",
    "        punct = punct.replace(symb, '')\n",
    "        \n",
    "    text = \"\".join([char for char in text if char not in punct])\n",
    "    text = re.sub('[0-1]+', '', text)\n",
    "    text = re.sub('[5-9]+', '', text)\n",
    "    if not numbers :\n",
    "        raise NotImplementedError\n",
    "    return text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reference_dictionary(filename, learning=True, limit=1000000000000000):\n",
    "    \"\"\" This function creates a dictionary of references :\n",
    "        a pandas dataframe with 5 columns : the word itself, the position of the word in the file as a tuple, \n",
    "        the correction of the words (initially the word itself), the tokens and an alternative correction column.\n",
    "        learning separates the case where we take the words from a learning file or a testing file (they are written differently).\n",
    "        \"\"\"\n",
    "    dat = {\"filewords\": [], \"position\": [], \"correction\" : [], \"tokens\": [], \"alternative correction\": []}\n",
    "    with open(filename, 'r') as fh:\n",
    "        lines = fh.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if i == limit: break\n",
    "        else:\n",
    "            s_line = line.strip()\n",
    "            if not learning:\n",
    "                # In this case each line starts by its number followed by a comma\n",
    "                k = 0\n",
    "                while s_line[k] != ',':\n",
    "                    k += 1\n",
    "                # Remove everything up to the comma (included)\n",
    "                s_line = s_line[k + 1:]\n",
    "                \n",
    "            \n",
    "            # Remove punctuation except #, !, <, >\n",
    "            s_line = remove_punctuation(s_line)\n",
    "            tweet_words = s_line.split()\n",
    "            for j, w in enumerate(tweet_words):\n",
    "                dat[\"filewords\"].append(w)\n",
    "                dat[\"position\"].append((i, j))\n",
    "                dat[\"correction\"].append(w)\n",
    "                dat[\"tokens\"].append([])\n",
    "                dat[\"alternative correction\"].append(w)\n",
    "    \n",
    "    return pd.DataFrame(dat)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_abbreviations = [('ofc', 'of course'), ('lmao', 'laughing my ass off', 'haha'), ('lmfao', 'laughing my fucking ass off', 'haha'),\n",
    "       ('lyvm', 'love you very much'), ('afk', 'away from keyboard'), \n",
    "      ('aight', 'alright'), ('aka', 'also known as'), ('lol', 'laughing out loud', 'haha'), \n",
    "      ('aisi', 'as i see it'), ('alcon', 'all concerned'), ('asap', 'as soon as possible'), \n",
    "      ('atm', 'at the moment'), ('aweso', 'awesome'), ('b', 'be'), ('bf', 'boyfriend'), ('bae', 'baby'), ('bb', 'baby'),\n",
    "        ('babe', 'baby'), ('bbl', 'be back later'), ('bc', 'because'), ('cuz', 'because'), ('bday', 'birthday'), \n",
    "            ('bff', 'best friend forever'), ('bfn', 'bye for now'), ('bg', 'big grin', 'haha'), ('bih', 'burn in hell'),\n",
    "        ('bl', 'belly laugh', 'haha'), ('bloke', 'man'), ('bn', 'bad news'), ('bol', 'best of luck'), ('boyf', 'boyfriend'),\n",
    "        ('brb', 'be right back'), ('bs', 'bullshit'), ('brd', 'bored'), ('btw', 'by the way'),\n",
    "         ('clab', 'crying like a baby'), ('ciao', 'goodbye'), ('y', 'why'), ('cto', 'check this out'), ('cos', 'because'), \n",
    "            ('cmon', 'come on'), ('c', 'see'), ('cu', 'see you'), ('cya', 'see you'), ('dafuq', 'what the fuck'), ('wtf', 'what the fuck'),\n",
    "            ('dc', 'disconnect'), ('dd', 'dear'), ('derp', 'silly'), ('dgaf', 'do not give a fuck'),\n",
    "            ('dh', 'dear'), ('dhu', 'dinosaur hugs'), ('diy', 'do it yourself'),('dm', 'direct message'), ('dmed', 'direct message'),\n",
    "            ('dnt', 'do not'), ('dw', 'do not worry'), ('ez', 'easy'), ('fab', 'fabulous'), ('fam','family'),\n",
    "                ('fb', 'facebook'), ('ffs', 'for fuck sake'), ('fml', 'fuck my life'), ('ftw', 'for the win'),\n",
    "                ('fyi', 'for your information'), ('gf', 'girlfriend'), ('gb', 'goodbye'), ('gd', 'good'), ('gl', 'good luck'),\n",
    "            ('gn', 'goodnight'), ('gnight', 'goodnight'), ('gnite', 'goodnight'), ('gtg', 'got to go'), ('hmu', 'hit me up'),\n",
    "            ('hw', 'homework'), ('idc', 'i do not care'), ('idk', 'i do not know'), ('idgaf', 'i do not give a fuck'),\n",
    "            ('ik', 'i know'), ('ikr', 'i know right'), ('ily', 'i love you'), ('imo', 'in my opinion'),\n",
    "            ('imu', 'i miss you'), ('irl', 'in real life'), ('k', 'okay'),  ('lmk', 'let me know'),\n",
    "            ('lil', 'little'), ('meh', 'shrug'), ('msg', 'message'), ('ngl', 'not going to lie'),('nvm', 'never mind'),\n",
    "            ('np', 'no problem'), ('nvr', 'never'), ('omg', 'oh my god'), ('omfg', 'oh my fucking god'),\n",
    "            ('omw', 'on my way'), ('otp', 'one true pairing'), ('otw', 'off to work'), ('peeps', 'people'),\n",
    "            ('pls', 'please'), ('plz', 'please'), ('ppl', 'people'),('probs', 'probably'),('prolly', 'probably'),\n",
    "            ('qt', 'cute'), ('rofl', 'rolling on the floor laughing', 'haha'), ('rip','rest in peace'), ('rn','right now'),\n",
    "            ('rly', 'really'), ('sd', 'sweet dreams'), ('smh', 'shake my head'), ('srsly', 'seriously'),\n",
    "            ('sry', 'sorry'), ('sup', 'what is up'), ('hbu', 'how about you'), ('stfu', 'shut the fuck up'),\n",
    "            ('ty', 'thank you'), ('tyvm', 'thank you very much'), ('thx', 'thank you'), ('tbh', 'to be honest'),\n",
    "            ('thot', 'that whore over there'), ('totes', 'totally'), ('wbu', 'what about you'), ('wtf','what the fuck'),\n",
    "            ('x', 'kiss'), ('xd', 'smile', 'haha'), ('<3', 'kiss'), ('pic', 'picture'), ('2', 'to'), ('b4', 'before'),\n",
    "            ('xoxo', 'kiss')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'laughing my fucking ass off'"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_abbreviations[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_obvious_mistakes = [('dat', 'that'), ('dis', 'this'), ('wud', 'would'), ('da', 'the'), ('enuf', 'enough'),\n",
    "                           ('gud', 'good'), ('dunno', 'do not know'), ('im', 'i am'), ('ok', 'okay'), ('r', 'are'), ('u', 'you'),\n",
    "                           ('luv', 'love'), ('ur', 'your'), ('wat', 'what'), ('wut', 'what'), ('y', 'why'), ('ya', 'you'), ('yea', 'yes'), \n",
    "                           ('dont', 'do not'), ('cant', 'cannot'), ('wont', 'will not'), ('aint', 'am not'), ('isnt', 'is not'),\n",
    "                            ('doesnt', 'does not'), ('hasnt', 'has not'), ('havent', 'have not'), ('id', 'i would'),\n",
    "                            ('theres', 'there is'), ('thats', 'that is'), ('wheres', 'where is'), ('werent', 'were not')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall\",\n",
    "\"he'll've\": \"he shall have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'l\": \"i will\",\n",
    "\"i'll\": \"I will\",\n",
    "\"i'll've\": \"I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'al\": \"you all\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "onomatopees = [('pf', 'sigh'), ('mwah', 'kiss'), ('muah', 'kiss')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_exclamation(word):\n",
    "    res = False\n",
    "    i = 0\n",
    "    while i < len(word) - 1:\n",
    "        if word[i] == '!' and word[i+1] == '!':\n",
    "            res = True\n",
    "        i += 1\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_punctuation(data, apostrophe_res):\n",
    "    \"\"\" Function reads a pandas dataframe and removes the remaining punctuation marks.\n",
    "        namely the apostrophes the exclamation marks and the hashtags. \"\"\"\n",
    "    \n",
    "    ex_indices = data.index[data.filewords.str.contains('!')].tolist()\n",
    "    hash_indices = data.index[data.filewords.str.contains('#')].tolist()\n",
    "    ap_indices = data.index[data.filewords.str.contains('\\'')].tolist()\n",
    "    \n",
    "    for exi in ex_indices:\n",
    "        # Take the raw word\n",
    "        w = data.iat[exi, 0]\n",
    "        tokens = data.iat[exi, 3]\n",
    "        tokens.append('exclmt')\n",
    "        if repeat_exclamation(w):\n",
    "            tokens.append('rrrpp')\n",
    "        data.iat[exi, 3] = tokens\n",
    "        w  = \"\".join([char for char in w if char != '!'])\n",
    "        data.iat[exi, 2] = w\n",
    "        \n",
    "    for hi in hash_indices:\n",
    "        w = data.iat[hi, 0]\n",
    "        tokens = data.iat[hi, 3]\n",
    "        tokens.append('hashtagg')\n",
    "        data.iat[hi, 3] = tokens\n",
    "        # Create the word without the \"#\", and put it in the correction column\n",
    "        w = w.replace('#', '')\n",
    "        #print(w)\n",
    "        #w  = \"\".join([char for char in w if char != '#'])\n",
    "        data.iat[hi, 2] = w\n",
    "        \n",
    "    for ai in ap_indices:\n",
    "        w = data.iat[ai, 0]\n",
    "        if w in apostrophe_res.keys():\n",
    "            w = apostrophe_res[w]\n",
    "        else:\n",
    "            w  = \"\".join([char for char in w if char != '\\''])\n",
    "        data.iat[ai, 2] = w\n",
    "        \n",
    "    #return data         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_of_tuples(l):\n",
    "    \"\"\" Retrieve a list of the first elements in each tuples\"\"\"\n",
    "    el1 = []\n",
    "    for t in l:\n",
    "        el1.append(t[0])\n",
    "    return el1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_abbreviations(data, abbr):\n",
    "    \"\"\" Function takes a list of abbreviation and their correction (a list of tuples), and checks the dataset to correct these mistakes\n",
    "    \"\"\"\n",
    "    abbs = split_list_of_tuples(abbr)\n",
    "    for i, abb in enumerate(abbs):\n",
    "        # Find the indices where there's this abbreviation.\n",
    "        abb_ind = data.index[data.filewords == abb].tolist()\n",
    "        \n",
    "        # Correct the abbreviation and keep a token.\n",
    "        for abb_err in abb_ind:\n",
    "            data.iat[abb_err, 2] = abbr[i][1]\n",
    "            tokens = data.iat[abb_err, 3]\n",
    "            tokens.append('abbrvt')\n",
    "            # If there's additional information about the abbreviation (e.g. lol)\n",
    "            if len(abbr[i]) == 3:\n",
    "                tokens.append(abbr[i][2])\n",
    "            data.iat[abb_err, 3] = tokens\n",
    "    #return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_obvious_mistakes(data, om):\n",
    "\n",
    "    mistakes = split_list_of_tuples(om)\n",
    "    for i, mis in enumerate(mistakes):\n",
    "        # Find the indices where there's this mistake.\n",
    "        mis_ind = data.index[data.filewords == mis].tolist()\n",
    "\n",
    "        # Correct the abbreviation and keep a token.\n",
    "        for mis_err in mis_ind:\n",
    "            data.iat[mis_err, 2] = om[i][1]\n",
    "            tokens = data.iat[mis_err, 3]\n",
    "            tokens.append('mspld')\n",
    "            # If there's additional information about the abbreviation (e.g. lol)\n",
    "            if len(om[i]) == 3:\n",
    "                tokens.append(om[i][2])\n",
    "            data.iat[mis_err, 3] = tokens\n",
    "    #return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_onomatopee(data, instances):\n",
    "    \n",
    "    onos = split_list_of_tuples(instances)\n",
    "    for i, ono in enumerate(onos):\n",
    "        # Find the indices where there's this mistake.\n",
    "        ono_ind = data.index[data.filewords == ono].tolist()\n",
    "\n",
    "        # Correct the abbreviation and keep a token.\n",
    "        for ono_err in ono_ind:\n",
    "            data.iat[ono_err, 2] = instances[i][1]\n",
    "            tokens = data.iat[ono_err, 3]\n",
    "            tokens.append('ooo')\n",
    "            # If there's additional information about the abbreviation (e.g. lol)\n",
    "            if len(instances[i]) == 3:\n",
    "                tokens.append(instances[i][2])\n",
    "            data.iat[ono_err, 3] = tokens\n",
    "    \n",
    "    #return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "laughs = ['hah', 'heh', 'hih', 'huh', 'hoh', 'jaj']\n",
    "def resolve_laugh(data, haha):\n",
    "    \"\"\" Function reads the data and replaces any word containing a term in haha by a token that expresses\n",
    "        laughter. \"\"\"\n",
    "    for laugh in haha:\n",
    "        # Find the laugh indices\n",
    "        ha_ind = data.index[data.filewords.str.contains(laugh)].tolist()\n",
    "        \n",
    "        # Interpret these words as laughter :\n",
    "        for ha_err in ha_ind:\n",
    "            data.iat[ha_err, 2] = \"\"\n",
    "            tokens = data.iat[ha_err, 3]\n",
    "            tokens.append('haha')\n",
    "            data.iat[ha_err, 3] = tokens\n",
    "            \n",
    "    #return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_correction_into_words(data):\n",
    "    \"\"\" Method takes the dataframe, and the index, and will separate the correction into\n",
    "        multiple words (if it needs it). It will add rows for the new words and update the position of the words\n",
    "        in the same line that are not affected by the correction.\"\"\"\n",
    "    \n",
    "    for i, corr in enumerate(data.correction):\n",
    "        data.iat[i, 2] = corr.split() \n",
    "    #return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_corrections_and_tokens_into_list(data):\n",
    "    \"\"\" Method returns a list of the corrections and the tokens.\"\"\"\n",
    "    l = []\n",
    "    for i, corr in enumerate(data.correction):\n",
    "        l += corr\n",
    "        l += data.iat[i, 3]  # Add the tokens\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf_analysis(df1, df2=None):\n",
    "    \"\"\" Method takes one or two dataframes and computes a tf idf analysis on them\n",
    "        if there are two dataframes given, it will return three dataframes, (the first with the\n",
    "        tf-idf analysis on df1, the second with the analysis on df2, the third with the analysis on both)\n",
    "        \"\"\"\n",
    "    # Transform the corrections into a long string :\n",
    "    list_of_corrections1 = turn_corrections_and_tokens_into_list(df1)\n",
    "    corr_string1 = [\" \".join(c for c in list_of_corrections1)]\n",
    "    \n",
    "    # Initialize the tf-idf vectorizer.\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    matrix1 = vectorizer.fit_transform(corr_string1).todense()\n",
    "    \n",
    "    # Transform the results into pandas dataframes\n",
    "    matrix1 = pd.DataFrame(matrix1, columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    top_words1 = matrix1.sum(axis=0).sort_values(ascending=False)\n",
    "    \n",
    "    frames = []\n",
    "    frames.append(top_words1)\n",
    "    \n",
    "    if df2 is None:\n",
    "        return frames\n",
    "    else:\n",
    "        # Repeat the same methods as above for the second dataframe\n",
    "        list_of_corrections2 = turn_corrections_and_tokens_into_list(df2)\n",
    "        corr_string2 = [\" \".join(c for c in list_of_corrections2)]\n",
    "        \n",
    "        corrss = [corr_string1[0], corr_string2[0]]\n",
    "        corr_string12 = [\" \".join(c for c in corrss)]\n",
    "        #corr_string12 = corr_string1 + corr_string2  # For the analysis on both.\n",
    "        \n",
    "        matrix2 = vectorizer.fit_transform(corr_string2).todense()\n",
    "    \n",
    "        # Transform the results into pandas dataframes\n",
    "        matrix2 = pd.DataFrame(matrix2, columns=vectorizer.get_feature_names())\n",
    "        \n",
    "        matrix12 = vectorizer.fit_transform([corr_string1, corr_string2]).todense()\n",
    "    \n",
    "        # Transform the results into pandas dataframes\n",
    "        matrix12 = pd.DataFrame(matrix12, columns=vectorizer.get_feature_names())\n",
    "        \n",
    "        top_words2 = matrix2.sum(axis=0).sort_values(ascending=False)\n",
    "        top_words12 = matrix12.sum(axis=0).sort_values(ascending=False)\n",
    "        frames.append(top_words2)\n",
    "        frames.append(top_words12)\n",
    "        return frames\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Etienne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_eg = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(correction, stopword):\n",
    "    \"\"\" function reads the correction part of data and adds a new column of words without any stopword.\n",
    "    \"\"\"\n",
    "    \n",
    "    correction = [word for word in correction if word not in stopword]\n",
    "    return correction\n",
    "\n",
    "def remove_stopwords_from_dataframe(data, stopword):\n",
    "    \n",
    "    for i, corr in enumerate(data.correction):\n",
    "        data.iat[i, 2] = remove_stopwords(corr, stopword)\n",
    "    #return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "def stem(correction):\n",
    "    \n",
    "    correction = [ps.stem(word) for word in correction]\n",
    "    return correction\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(corr):\n",
    "    corr = [wn.lemmatize(word) for word in corr]\n",
    "    return corr\n",
    "\n",
    "def stem_and_lemmatize_data(data):\n",
    "    \n",
    "    for i, corr in enumerate(data.correction):\n",
    "        stemmed_corr = stem(corr)\n",
    "        data.iat[i, 2] = lemmatize(corr)\n",
    "    #return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def still_in_same_tweet(data, j):\n",
    "    \"\"\" Method checks if the next word is in the same tweet\"\"\"\n",
    "    pos1 = data.iat[j, 1]\n",
    "    pos2 = data.iat[j+1, 1]\n",
    "    return pos1[0] == pos2[0]\n",
    "    \n",
    "def write_new_test_file_from_df(data, filename):\n",
    "    \n",
    "    with open(filename, 'w') as fh:\n",
    "        i = 0\n",
    "        while i <len(data.position):\n",
    "            pos = data.iat[i, 1]\n",
    "            line_number = pos[0]\n",
    "            # write the line number at the start of each line.\n",
    "            fh.write(\"{},\".format(line_number))\n",
    "            j = i\n",
    "            while still_in_same_tweet(data, j):\n",
    "                for word in data.iat[j, 2]:  # write each word in correction.\n",
    "                    fh.write(\"{} \".format(word))\n",
    "                for token in data.iat[j, 3]: # write each token\n",
    "                    fh.write(\"{} \".format(token))\n",
    "                j += 1\n",
    "            if j >= len(data.filewords):  # Check if we arrived at the end of the file.\n",
    "                break\n",
    "            # Still need to write the last word (because we basically check if the next word is in the tweet, and write\n",
    "            # it if it has a successor in the same tweet)\n",
    "            \n",
    "            for word in data.iat[j, 2]:  # write each word in the last correction.\n",
    "                fh.write(\"{} \".format(word))\n",
    "            for token in data.iat[j, 3]: # write each token\n",
    "                    fh.write(\"{} \".format(token))\n",
    "                \n",
    "            fh.write(\"\\n\")  # Write a new line\n",
    "            i = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename, learning=True, orthograph=False):\n",
    "    \"\"\" This method takes a filename of tweets and returns a pandas data frame on which the preprocessing has been done.\n",
    "        orthograph = False asks whether we want to try and run the spellchecker on the remaining words (it can be very long).\n",
    "        learning = True asks whether we are in a learning phase or testing phase.\n",
    "        \"\"\"\n",
    "    # Create the pandas dataframe (it strips the line number if we are in the testing phase)\n",
    "    df = create_reference_dictionary(filename, learning)\n",
    "    \n",
    "    \n",
    "    # Resolve successively the different preprocessing phases on df.\n",
    "    \n",
    "    # First remove the abbreviations\n",
    "    resolve_abbreviations(df, twitter_abbreviations)\n",
    "    \n",
    "    # Resolve the twitter obvious mistakes (such as dat for that etc)\n",
    "    resolve_obvious_mistakes(df, twitter_obvious_mistakes)\n",
    "    \n",
    "    # Take care of the onomatopeia\n",
    "    resolve_onomatopee(df, onomatopees)\n",
    "    \n",
    "    # Take care of the laughs\n",
    "    resolve_laugh(df, laughs)\n",
    "    \n",
    "    # Take care of the remaining punctuation\n",
    "    resolve_punctuation(df, contractions)\n",
    "    \n",
    "    # Take care of words with repetitions\n",
    "    # Find the indices where a repetion occurs\n",
    "    rep_df = find_repeat_indices(df)\n",
    "    correct_repeat_word(df, rep_df, onomatopees, twitter_abbreviations)\n",
    "    \n",
    "    # Up until now each correction was a string, we change them into a list of words\n",
    "    resolve_correction_into_words(df)\n",
    "    \n",
    "    # We can now remove the stop words from the corrections\n",
    "    remove_stopwords_from_dataframe(df, stopword_eg)\n",
    "    \n",
    "    # Lastly, we can stem and lemmatize the corrections\n",
    "    stem_and_lemmatize_data(df)\n",
    "    \n",
    "    # Try to correct some spelling mistakes\n",
    "    if orthograph:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Etienne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_name = 'test_data.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "followback\n",
      "unitytour\n",
      "happy42\n",
      "phasell\n",
      "predators\n",
      "nhl\n",
      "yougetmajorpointsif\n",
      "needtoreesthisshit\n",
      "shouldhavereaditwithaglassofwine\n",
      "tellmewhy\n",
      "letdown\n",
      "justranting\n",
      "words\n",
      "creativewriting\n",
      "have\n",
      "\n",
      "nice\n",
      "day\n",
      "everyone\n",
      "crying\n",
      "fornoreason\n",
      "fornoreason\n",
      "teamfollowback\n",
      "\n",
      "starshipsvideo\n",
      "\n",
      "jamesmaslowtok\n",
      "\n",
      "jamesmaslowtok\n",
      "\n",
      "jamesmaslowtok\n",
      "\n",
      "jamesmaslowtok\n",
      "\n",
      "jamesmaslowtok\n",
      "\n",
      "notcool\n",
      "stanford\n",
      "baylor\n",
      "bbwanted\n",
      "dangit\n",
      "sadpanda\n",
      "playoffhockey\n",
      "2hourstogo\n",
      "win\n",
      "unpopularopinion\n",
      "happensallthetime\n",
      "calgary\n",
      "jobs\n",
      "job\n",
      "thanksforfollowing\n",
      "followback\n",
      "dfamily\n",
      "4\n",
      "happybirthdaynathan\n",
      "happybirthdaynathan\n",
      "happybirthdaynathan\n",
      "happybirthdaynathan\n",
      "happybirthdaynathan\n",
      "444\n",
      "littlebrother\n",
      "waystomakemehappy\n",
      "bbthemes\n",
      "nuxstore\n",
      "haveagrweekend\n",
      "shulerawards\n",
      "motown\n",
      "thoughtsduringschool\n",
      "fuckembro\n",
      "2\n",
      "hooligans\n",
      "birthday\n",
      "thingsifindattractive\n",
      "inournorts\n",
      "playcherlloydonradio\n",
      "ripmrsbieber\n",
      "ripmrsbieber\n",
      "takemetothebeach\n",
      "twat\n",
      "offline\n",
      "tumblrgirl4life\n",
      "mybiggestfearis\n",
      "goosebumps\n",
      "unlucky\n",
      "ilovejesus\n",
      "godisgood\n",
      "godislove\n",
      "godismychampion\n",
      "techi\n",
      "babbat\n",
      "moderation\n",
      "3rddegreeseatbeltburns\n",
      "thebest\n",
      "yahoosports\n",
      "sports\n",
      "sportsnews\n",
      "oomf\n",
      "bgt\n",
      "dogbountyhunter\n",
      "electronicearth\n",
      "happybirthdaychloe\n",
      "trancefamily\n",
      "ffs\n",
      "nolawantedstuff\n",
      "exams\n",
      "coldbloodedbitch\n",
      "ilovetospoon\n",
      "spoonspoon\n",
      "waystomakemehappy\n",
      "dead\n",
      "needpeople\n",
      "oomf\n",
      "foh\n",
      "thoughtprovokingthursday\n",
      "catchupisneeded\n",
      "coolmom\n",
      "tomorrowwillbeawasteofaday\n",
      "thevoice\n",
      "donegal\n",
      "ld4\n",
      "frankbruno\n",
      "awsome\n",
      "\n",
      "butreally\n",
      "twinkletts\n",
      "primaryschoolmemories\n",
      "followback\n",
      "followback\n",
      "wannabeagain\n",
      "justkeeptweetingme\n",
      "greysonpict\n",
      "worstfeelingever\n",
      "heyfattybumbum\n",
      "feeldeadwhen\n",
      "excitedtweet\n",
      "fruitninja\n",
      "fattybombom\n",
      "teamsmallfeet\n",
      "fairlylegal\n",
      "helpme\n",
      "totesamazing\n",
      "glamberts\n",
      "glambert\n",
      "savereksandhawkins\n",
      "nochat\n",
      "thathurt\n",
      "eatsomemore\n",
      "whathappened\n",
      "ifonly\n",
      "realtalkbitch\n",
      "amen\n",
      "notgood\n",
      "behindthemusic\n",
      "nonly\n",
      "desperatehousewives\n",
      "happytweet\n",
      "boyfriend\n",
      "cantwait\n",
      "bitchesthatneedtostopquotingme\n",
      "bitchesthatneedtostopquotingme\n",
      "shoot\n",
      "that's\n",
      "foreveralone\n",
      "badfriend\n",
      "mrknowitall\n",
      "lucky\n",
      "imyours\n",
      "emotionallydepressed\n",
      "dlikesbigbums\n",
      "junkcollector\n",
      "enough\n",
      "teamfolloback\n",
      "teamkreamy\n",
      "tears\n",
      "vampirediaries\n",
      "foodpoisoning\n",
      "happybirthdaypresley\n",
      "gatt\n",
      "nadal\n",
      "highlightofthematch\n",
      "rafabs\n",
      "house\n",
      "afc\n",
      "oomf\n",
      "teamfollowback\n",
      "sateklathak\n",
      "theluckyone\n",
      "goodtimes\n",
      "missingyou\n",
      "notbraggn\n",
      "yougetmajorpointsif\n",
      "yougetmajorpointsif\n",
      "awkward\n",
      "oneminutesong\n",
      "23\n",
      "badpickuplines\n",
      "sunset\n",
      "2\n",
      "river\n",
      "evansville\n",
      "coool\n",
      "goodygoods\n",
      "ohwell\n",
      "thunder\n",
      "nba\n",
      "shesneededalotroundhere\n",
      "cannywait\n",
      "sohappy\n",
      "score\n",
      "done\n",
      "toomuchtohandle\n",
      "kidulthood\n",
      "fuckfuckfuck\n",
      "fuckmeher\n",
      "sosad\n",
      "toocute\n",
      "ihategovernment\n",
      "leadershippenne\n",
      "justsaying\n",
      "'s\n",
      "enoughsaid\n",
      "thoughtsduringschool\n",
      "towie\n",
      "yougetmajorpointsif\n",
      "foreveralone\n",
      "womenwins\n",
      "\n",
      "rfamily\n",
      "rfamily\n",
      "rfamily\n",
      "rfamily\n",
      "rfamily\n",
      "rfamily\n",
      "rfamily\n",
      "rfamily\n",
      "rfamily\n",
      "britneyspearsnow\n",
      "patriots\n",
      "nfl\n",
      "sobbing\n",
      "slagbert\n",
      "leaveeeittt\n",
      "initforthelongrun\n",
      "praying\n",
      "teamfollowback\n",
      "yolo\n",
      "jesus\n",
      "damn\n",
      "moneymaker\n",
      "ff\n",
      "whatadickiam\n",
      "throwbackthursday\n",
      "latinabeauty\n",
      "flatshare\n",
      "homesick\n",
      "williamtaughtme\n",
      "honorstudent\n",
      "badass\n",
      "growup\n",
      "nonprofit\n",
      "jobs\n",
      "tx\n",
      "marryme\n",
      "fama\n",
      "anycompany\n",
      "wantedwednesday\n",
      "nathanbirthday\n",
      "theboysareback\n",
      "rss\n",
      "cantsayno\n",
      "dm\n",
      "thatsnevergonnahappen\n",
      "ihaveissues\n",
      "sorry2hear\n",
      "soydeesaspersonasque\n",
      "soready\n",
      "meloveulongtime\n",
      "followers\n",
      "danceacademy\n",
      "lulusboutiquebag\n",
      "waystomakemehappy\n",
      "unnattractivethingsaboutme\n",
      "lgovsm\n",
      "ohyup\n",
      "horriblesongbut\n",
      "tgif\n",
      "weallknowyou'regoingtodowellyou'renotfoolinganyone\n",
      "inagony\n",
      "killing\n",
      "littlethingscount\n",
      "hero\n",
      "angels\n",
      "teafail\n",
      "oomf\n",
      "sadgbpackerday\n",
      "twat\n",
      "missingnetherlands\n",
      "mytwitter\n",
      "thug\n",
      "happytime\n",
      "hate\n",
      "hugemarshug\n",
      "stevetylertofollowreecemastin\n",
      "smack\n",
      "nfb\n",
      "wellseehowlongthislast\n",
      "3wordsaftersex\n",
      "whyareyousofit\n",
      "songsthatgiveyougoosebumps\n",
      "word\n",
      "boyfriendvideo\n",
      "excited\n",
      "waa\n",
      "bigbaby\n",
      "june\n",
      "dnextalbumtitle\n",
      "nofair\n",
      "sexism\n",
      "angel\n",
      "gycdoubleplatinum\n",
      "proudoftw\n",
      "c2k\n",
      "bbc4\n",
      "realdiapers\n",
      "\n",
      "blackhawks\n",
      "salute\n",
      "aboutme\n",
      "teamgnation\n",
      "teamfollowback\n",
      "myhomeawayfromhome\n",
      "holla\n",
      "bored\n",
      "doingall\n",
      "holla\n",
      "bored\n",
      "doingall\n",
      "londonontario\n",
      "jobs\n",
      "itscutewhen\n",
      "itscutewhen\n",
      "czechrepublicwantsbelievetour\n",
      "lazytweet\n",
      "thevoiceuk\n",
      "beezinthetrapvideo\n",
      "mademyday\n",
      "lovecarrides\n",
      "iadmit\n",
      "cantwait\n",
      "fff\n",
      "sens\n",
      "flanjd\n",
      "ipod\n",
      "itunes\n",
      "imissyou\n",
      "yolo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "meniwanttocoverinchocolate\n",
      "ifindthatattractive\n",
      "datenight\n",
      "club\n",
      "ff\n",
      "staystrongzayn\n",
      "thevoice\n",
      "thevoice\n",
      "mlponyshocker\n",
      "tvdfamily\n",
      "oomf\n",
      "ithoughtyoualreadyknew\n",
      "whatilikeaboutyou\n",
      "whatilikeaboutyou\n",
      "friskyfriday\n",
      "no\n",
      "no2\n",
      "kitchenplay\n",
      "supper\n",
      "sin\n",
      "heartbreak\n",
      "gorgeousspringday\n",
      "workinggirlprobz\n",
      "itswhatever\n",
      "bedtweet\n",
      "sadtimes\n",
      "ff\n",
      "harrypotterchatuplines\n",
      "teamfollowback\n",
      "ifollowback\n",
      "siguemeytesigo\n",
      "siguemeytsigo\n",
      "fastestfollow\n",
      "instantfollowback\n",
      "schouperboy\n",
      "myboysforever\n",
      "lcdconf\n",
      "fmibl\n",
      "iloveyou\n",
      "twfanmily\n",
      "chasingthesun\n",
      "proudoftw\n",
      "feelsick\n",
      "gonnabeagreatday\n",
      "thatwouldbefun\n",
      "4\n",
      "gettingmoney\n",
      "listentothemusic\n",
      "eastenders\n",
      "biggirllife\n",
      "\n",
      "hatethis\n",
      "obsessedwithcsi\n",
      "stuntin\n",
      "saggy\n",
      "welcomebacktoindonesiagreyson\n",
      "wholegrainwednesday\n",
      "\n",
      "2\n",
      "2\n",
      "holico\n",
      "sadtweet\n",
      "lovethissong\n",
      "sick\n",
      "shutit\n",
      "oohf\n",
      "sorrynotsorry\n",
      "ihop\n",
      "visitme\n",
      "joinnfi\n",
      "tedbaker\n",
      "goodnight\n",
      "allergies\n",
      "dying\n",
      "needmedication\n",
      "\n",
      "toronto\n",
      "sports\n",
      "tu\n",
      "nowfollowing\n",
      "believeinmclaren\n",
      "countrymusic\n",
      "goodmorning\n",
      "youcansayimnotinthebestofmoods\n",
      "dreamteam\n",
      "32\n",
      "paulydproject\n",
      "rt\n",
      "rt\n",
      "wines\n",
      "winetasting\n",
      "excellentservice\n",
      "wines\n",
      "winetasting\n",
      "excellentservice\n",
      "blamehim\n",
      "ff's\n",
      "jlover4life\n",
      "moneydownadrain\n",
      "ff\n",
      "nanny\n",
      "idontlike\n",
      "nw\n",
      "classic\n",
      "suckonthat\n",
      "echelon\n",
      "vyrt\n",
      "sick\n",
      "newconormaynardcover\n",
      "smartnokialumia\n",
      "bobsbash\n",
      "\n",
      "\n",
      "virgin4life\n",
      "pune\n",
      "gonnahavetomanup\n",
      "sick\n",
      "\n",
      "porn\n",
      "sex\n",
      "pornstar\n",
      "oomf\n",
      "deluxrichmond\n",
      "pbbteens4\n",
      "stonertweets\n",
      "onthelow\n",
      "pissed\n",
      "aunatural\n",
      "invigorating\n",
      "rippearl\n",
      "oomf\n",
      "stalker\n",
      "soap\n",
      "ff\n",
      "qpr\n",
      "spurs\n",
      "ifitwasntforfootball\n",
      "scottish\n",
      "winning\n",
      "oomf\n",
      "nowwatching\n",
      "ineedtogetalife\n",
      "stayconnected\n",
      "barcelona\n",
      "callmyname\n",
      "cya\n",
      "hehe\n",
      "te\n",
      "nowplaying\n",
      "hiphop\n",
      "mybiggestfearis\n",
      "ipad\n",
      "sojealous\n",
      "redbull\n",
      "spw4\n",
      "working\n",
      "3on3concert\n",
      "3on3concert\n",
      "chileanhunters\n",
      "believethat\n",
      "job\n",
      "rn\n",
      "nurse\n",
      "random\n",
      "\n",
      "harrypotterchatuplines\n",
      "fyi\n",
      "cantsayno\n",
      "ramidol\n",
      "wssu\n",
      "lawsonadamfollows\n",
      "jewelry\n",
      "\n",
      "askben\n",
      "projectgetfit\n",
      "yolo\n",
      "nathanappreciationweek\n",
      "3millionswifties\n",
      "twfanmily\n",
      "factsaboutme\n",
      "\n",
      "nyc\n",
      "qubec\n",
      "ggi\n",
      "uqo\n",
      "classe\n",
      "polqc\n",
      "\n",
      "\n",
      "missedopportunity\n",
      "allyourjobs\n",
      "local\n",
      "allyourjobs\n",
      "local\n",
      "\n",
      "ff\n",
      "cougartown\n",
      "somethingeveryonecantdo\n",
      "pensin\n",
      "glee\n",
      "loveyouthough\n",
      "iwishthat\n",
      "smartnokialumia\n",
      "nowyouknow\n",
      "yuckyducky\n",
      "sold\n",
      "wichitawantedstuff\n",
      "yoho\n",
      "messagetomyex\n",
      "fml\n",
      "bored\n",
      "happpytweet\n",
      "lazygirlprobz\n",
      "teamstooge\n",
      "notmyfault\n",
      "42no\n",
      "42no\n",
      "friends\n",
      "thoughtsduringschool\n",
      "dodgers\n",
      "mlb\n",
      "marshugs\n",
      "ff\n",
      "egoboost\n",
      "ff\n",
      "torture\n",
      "wewantjlsirishsummerdates\n",
      "jogon\n",
      "embrace\n",
      "stfu\n",
      "shelovesme\n",
      "keepdreaming\n",
      "meow\n",
      "smartnokialumia\n",
      "scaredycat\n",
      "pti\n",
      "thelittlethings\n",
      "sleephealsall\n",
      "rt\n",
      "keepiinit\n",
      "bestcompliment\n",
      "biketo\n",
      "toronto\n",
      "gofigure\n",
      "sadtweet\n",
      "idol\n",
      "idol\n",
      "idol\n",
      "idol\n",
      "idol\n",
      "ss4ina\n",
      "cantsayno\n",
      "bouttime\n",
      "\n",
      "unleashed\n",
      "onlyifeverydaywaslikethis\n",
      "cleancloud\n",
      "tasteslikecottoncandy\n",
      "hm\n",
      "neglected\n",
      "sadsaturday\n",
      "sexysaturday\n",
      "obvsomeboy\n",
      "cantsayno\n",
      "summer22\n",
      "ladiesspeednetworking\n",
      "ouch\n",
      "aftersunneeded\n",
      "nw\n",
      "ffblues\n",
      "betterthanaprius\n",
      "imajerk\n",
      "represent\n",
      "biggestfan\n",
      "\n",
      "sopurrrdy\n",
      "ifindthatattractive\n",
      "thatgoodfeeling\n",
      "\n",
      "yougetmajorpoints\n",
      "crete\n",
      "generalchicken\n",
      "springrolls\n",
      "cantsayno\n",
      "\n",
      "sorrybaby\n",
      "youaintnogoodif\n",
      "yougetmajorpointsif\n",
      "ff\n",
      "theregoesmymoney\n",
      "efff\n",
      "nicefriend\n",
      "mustfollow\n",
      "\n",
      "thevoiceau\n",
      "twgeordies\n",
      "foreveralone\n",
      "ithinkimgettingsick\n",
      "nigeriaeducationcrisis\n",
      "proudsmiler\n",
      "freekindlebooks\n",
      "freeebooks\n",
      "romance\n",
      "amreading\n",
      "reading\n",
      "feelingdrained\n",
      "smartnokialumia\n",
      "albarbz\n",
      "miss\n",
      "fact\n",
      "3\n",
      "\n",
      "smartnokialumia\n",
      "girlfritweet\n",
      "itsabratthing\n",
      "yougetmajorpointsif\n",
      "thatscute\n",
      "theluckyone\n",
      "believe\n",
      "no\n",
      "lmfao\n",
      "np\n",
      "\n",
      "\n",
      "thatsjustmethough\n",
      "thingsiwanttohappen\n",
      "swaggie\n",
      "makeitcount\n",
      "share\n",
      "ottawa\n",
      "rome\n",
      "nuawards\n",
      "gutted\n",
      "vetsherewecome\n",
      "tired\n",
      "winningg\n",
      "thoughtsduringschool\n",
      "wired\n",
      "fmfl\n",
      "mademyday\n",
      "nowfollowingback\n",
      "nowfollowingback\n",
      "nowfollowingback\n",
      "nowfollowingback\n",
      "zoandtiggalolovethes\n",
      "rubbishbody\n",
      "fubar\n",
      "flyers\n",
      "believe\n",
      "enhanced\n",
      "canada\n",
      "oomf\n",
      "jobshadowing\n",
      "onlyricksiknow\n",
      "welljel\n",
      "toolong\n",
      "proudbrothertweet\n",
      "rawduo\n",
      "torque\n",
      "proudbrothertweet\n",
      "rawduo\n",
      "torque\n",
      "devildeymonitoru\n",
      "notfun\n",
      "cremeegg\n",
      "discount\n",
      "chocolate\n",
      "coachella\n",
      "nblchampionship\n",
      "ilovehockey\n",
      "letsgopens\n",
      "padres\n",
      "mlb\n",
      "coe22\n",
      "plmforum\n",
      "empty\n",
      "yougetmajorpoints\n",
      "ifindthatattractive\n",
      "howsweet\n",
      "amazingword\n",
      "vllanita\n",
      "tired\n",
      "naptime\n",
      "noworktilthursday\n",
      "peacecs\n",
      "someday\n",
      "demiisourqueen\n",
      "drummajor22\n",
      "3minmeet\n",
      "imagine\n",
      "\n",
      "daystogo\n",
      "olympics\n",
      "in\n",
      "dayofsilence\n",
      "ppftw\n",
      "whoopthatass\n",
      "abused\n",
      "grr\n",
      "truestory\n",
      "sadtweet\n",
      "wah\n",
      "immexican\n",
      "teamlesbian\n",
      "rulesofclass\n",
      "rulesofclass\n",
      "xo\n",
      "lush\n",
      "soproud\n",
      "blessings\n",
      "wow\n",
      "iwannasleep\n",
      "ifonlylifewasthateasylol\n",
      "aba\n",
      "aba\n",
      "singlesgettingtome\n",
      "nowplaying\n",
      "isthattomuchtoaskfor\n",
      "haiyegarmi\n",
      "ebooks\n",
      "kindle\n",
      "lund22\n",
      "ff\n",
      "weatherproblems\n",
      "rain\n",
      "bedtime\n",
      "meekmill\n",
      "video\n",
      "jobs\n",
      "careers\n",
      "teamadele\n",
      "keepadelechunkyeitherway\n",
      "lowkey\n",
      "lovinthedick\n",
      "js\n",
      "thanks\n",
      "mybiggestfearis\n",
      "freeny\n",
      "foreversad\n",
      "enjoy\n",
      "bratsarefun\n",
      "reasonsidontknowwhatloveis\n",
      "imdaddynigga\n",
      "chelseacanwinif\n",
      "greekontheloose\n",
      "muchlove\n",
      "feellikecrying\n",
      "mirfandaforlife\n",
      "stupidrain\n",
      "computingtoohard\n",
      "ow\n",
      "yourfirstmention\n",
      "stoned\n",
      "prayfor\n",
      "tokyo\n",
      "egypt\n",
      "jan2\n",
      "tahrir\n",
      "egitto3\n",
      "egypt\n",
      "jan2\n",
      "tahrir\n",
      "egitto3\n",
      "teamfollowback\n",
      "favenewhit\n",
      "good\n",
      "interns\n",
      "24\n",
      "missmygirls\n",
      "hardtogetnoticed\n",
      "horanhug\n",
      "inpain\n",
      "thankmenow\n",
      "alkaseltzer\n",
      "ifwomendidnotexist\n",
      "bctf\n",
      "sadtweet\n",
      "rip\n",
      "phonejob\n",
      "musion\n",
      "openfollow\n",
      "jfb\n",
      "bedtime\n",
      "soberbday\n",
      "yougetmajorpointsif\n",
      "skyf\n",
      "c4news\n",
      "itvnews\n",
      "\n",
      "\n",
      "sadtimes\n",
      "thebiblog\n",
      "imitatingthedancemoms\n",
      "triceps\n",
      "imabullshitterbutyouloveitwhenitellyou\n",
      "definition\n",
      "lawyer\n",
      "losangeles\n",
      "sunsetstrip\n",
      "dirtymind\n",
      "foreverabelieber\n",
      "husbandandwifereunited\n",
      "retweet\n",
      "retweet\n",
      "retweet\n",
      "retweet\n",
      "retweet\n",
      "retweet\n",
      "retweet\n",
      "retweet\n",
      "retweet\n",
      "retweet\n",
      "retweet\n",
      "retweet\n",
      "retweet\n",
      "ff\n",
      "oomf\n",
      "\n",
      "btrtour\n",
      "nocomments\n",
      "\n",
      "\n",
      "notcool\n",
      "imissyou\n",
      "abovetheinfluence\n",
      "thisislife\n",
      "fucklife\n",
      "ouch\n",
      "babybullet\n",
      "bandaid\n",
      "finger\n",
      "season4doyle\n",
      "season4doyle\n",
      "nsn\n",
      "dontfuckwithme\n",
      "dontfuckwithme\n",
      "anthonnnyyy\n",
      "\n",
      "hyfr\n",
      "4\n",
      "busybusybusyyy\n",
      "2\n",
      "getahobby\n",
      "imwatchingdssecrethot3gig\n",
      "\n",
      "pottermore\n",
      "greatmindsthinkalike\n",
      "\n",
      "powercomebackplease\n",
      "waystomakemehappy\n",
      "usingliteratureforcomicrelief\n",
      "gudnite\n",
      "wasteoflife\n",
      "thuglife\n",
      "smartnokialumia\n",
      "smartnokialumia\n",
      "alienware\n",
      "twpunkd\n",
      "goodluckgirls\n",
      "princessofva\n",
      "infiniteroyalty\n",
      "adele\n",
      "lovesong\n",
      "free\n",
      "iwn\n",
      "toweak\n",
      "waystomakemehappy\n",
      "needsleep\n",
      "wishicould\n",
      "loveyoutho\n",
      "stupidhoe\n",
      "bigboobproblems\n",
      "spades\n",
      "virgo\n",
      "twfanmily\n",
      "happy\n",
      "twitterbirds\n",
      "sweet\n",
      "twitteroff\n",
      "trancefamily\n",
      "internship\n",
      "babymakingsongs\n",
      "nottooshabby\n",
      "heyhealth\n",
      "capricorn's\n",
      "happytweet\n",
      "skinnybitch\n",
      "fatgirlprobz\n",
      "baconbaconbacon\n",
      "\n",
      "today\n",
      "promo\n",
      "surabaya\n",
      "globelumia\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thissucks\n",
      "truelove\n",
      "toosad\n",
      "\n",
      "trashyourextuesday\n",
      "betteroffwithoutyou\n",
      "highasfuck\n",
      "stupidasfuck\n",
      "creying\n",
      "someonetakeme\n",
      "\n",
      "sjpuns\n",
      "depressing\n",
      "oomf\n",
      "crying\n",
      "foodchat\n",
      "\n",
      "\n",
      "\n",
      "smh\n",
      "newmember\n",
      "hml\n",
      "fok\n",
      "solar\n",
      "rt\n",
      "selfesstemelowered\n",
      "livinglikeaking\n",
      "alwaysmakeschuckle\n",
      "ff\n",
      "fail\n",
      "tattoo\n",
      "\n",
      "amen\n",
      "simplelife\n",
      "ripnanny\n",
      "needyouhere\n",
      "boyfriendvideo\n",
      "bieberonthevoice\n",
      "nofriends\n",
      "boyfriend\n",
      "sleepdeprivedmax\n",
      "homelessforanhour\n",
      "firstyear\n",
      "tacler\n",
      "ugh\n",
      "thankgod\n",
      "my\n",
      "askbarts\n",
      "faredealexpress\n",
      "shotforme\n",
      "np\n",
      "3inabed\n",
      "worksmurk\n",
      "bosh\n",
      "tgif\n",
      "bestfriend\n",
      "rainyafternoon\n",
      "imserious\n",
      "birminghammail\n",
      "tommybrown\n",
      "meningitis\n",
      "sadfamilyhasthatonething\n",
      "hatework\n",
      "perfection\n",
      "followfriday\n",
      "thevoice\n",
      "slut\n",
      "cantwait\n",
      "special\n",
      "sale\n",
      "toomuchtohandle\n",
      "armyfriends\n",
      "stupidcalifornia\n",
      "paris\n",
      "wannagoback\n",
      "foreveralone\n",
      "\n",
      "asktin\n",
      "asktin\n",
      "asktin\n",
      "asktin\n",
      "asktin\n",
      "bestfriend\n",
      "sleepyeb\n",
      "kevinsad\n",
      "twitterpartytime\n",
      "chasingthesun\n",
      "thewantedonthevoice\n",
      "gradea\n",
      "bashtube\n",
      "boyfiend\n",
      "believe\n",
      "thisisgonnabeawesome\n",
      "nevergiveup\n",
      "riseabovehate\n",
      "imstillwaiting\n",
      "oneweekfast\n",
      "shoutoutto\n",
      "mustfollow\n",
      "swaggy\n",
      "follow\n",
      "socent\n",
      "lavishwonders\n",
      "kcg\n",
      "everythinglavish\n",
      "kream\n",
      "lavishwonders\n",
      "kcg\n",
      "everythinglavish\n",
      "kream\n",
      "battlewounds\n",
      "smartnokialumia\n",
      "socute\n",
      "introducemoi\n",
      "tvduk\n",
      "winning\n",
      "fuckmylife\n",
      "jk\n",
      "loveya\n",
      "hulksmash\n",
      "excited\n",
      "yegfood\n",
      "yegcoffee\n",
      "hahasilly\n",
      "lipservice\n",
      "random\n",
      "ladyantebellum\n",
      "sadtweet\n",
      "missnyoublakebarnes\n",
      "justsayin\n",
      "browngirlproblems\n",
      "hittinthatupnextyear\n",
      "yogapants\n",
      "hoodies\n",
      "vs\n",
      "uber22\n",
      "theluckyone\n",
      "embarrassed\n",
      "loveher\n",
      "goodstarttotheday\n",
      "istoleyourwaterbottle\n",
      "typicalloveproblem\n",
      "ratherdie\n",
      "butthatsjustme\n",
      "whatsgoingon\n",
      "onelove\n",
      "askzacefron\n",
      "wishmeluck\n",
      "ergh\n",
      "pointlessnamedropping\n",
      "princess\n",
      "boyfriend\n",
      "biebergohard\n",
      "imthetypeofgirlfriend\n",
      "model\n",
      "actor\n",
      "dlb\n",
      "4get\n",
      "alwaysagoodthing\n",
      "slid\n",
      "fuck42o\n",
      "flirtingdayswithyouaregreat\n",
      "revenge\n",
      "scared\n",
      "nervous\n",
      "excited\n",
      "omf\n",
      "nosickplease\n",
      "away\n",
      "sadtimes\n",
      "quran\n",
      "quran\n",
      "quran\n",
      "wenimoutahere\n",
      "whenyouout\n",
      "lolololololol\n",
      "jk\n",
      "loveyou\n",
      "grindin\n",
      "honey\n",
      "imagine\n",
      "bow\n",
      "whydoialwaysespeciallymissyouonfridaynights\n",
      "victoriassecretbody\n",
      "summer\n",
      "porchedesign\n",
      "porche\n",
      "carreragt\n",
      "blackberry\n",
      "adidasporche\n",
      "hateperfrancestudies\n",
      "airtel\n",
      "coimbatore\n",
      "oneeyedtweet\n",
      "please\n",
      "scorpianbowl\n",
      "happypeople\n",
      "startyourdaywithasmile\n",
      "nomad\n",
      "abdc\n",
      "fact\n",
      "mdubarmy\n",
      "fabdetox\n",
      "clungehunt\n",
      "porn\n",
      "nsfw\n",
      "naked\n",
      "smize\n",
      "antm\n",
      "oomf\n",
      "believe\n",
      "stupidbitch\n",
      "wherethefuckhasmylifegone\n",
      "loved\n",
      "wantsmedead\n",
      "evaporate\n",
      "pageantgirl\n",
      "idol\n",
      "smartnokialumia\n",
      "smartnokialumia\n",
      "bahaha\n",
      "antisocial\n",
      "thewantedonthevoice\n",
      "beenforever\n",
      "twitteroff\n",
      "directioners\n",
      "dfamily\n",
      "ihateslugs\n",
      "chickens\n",
      "hopingfobrowneggs\n",
      "salute\n",
      "hehe\n",
      "makesmehappy\n",
      "exampleincardiff\n",
      "sadtweeet\n",
      "pleasepray\n",
      "bday\n",
      "madeleinemccann\n",
      "findmadeleine\n",
      "problemscdo\n",
      "katepride\n",
      "katepride\n",
      "katepride\n",
      "katepride\n",
      "inlove\n",
      "otherhalf\n",
      "badbuzz\n",
      "yougetmajorpoints\n",
      "surprisesuprise\n",
      "waaah\n",
      "haveakitkat\n",
      "wooo\n",
      "mashup\n",
      "twatcanada\n",
      "abortion\n",
      "pleaseandthankyou\n",
      "beready\n",
      "desperatehouswives\n",
      "lynettescavo\n",
      "twinglish\n",
      "nomorestalking\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ineedtoseeit\n",
      "zacefron\n",
      "ineedtoseeit\n",
      "zacefron\n",
      "waystobeginsex\n",
      "ff\n",
      "bestbirthdayever\n",
      "thoughtsduringschool\n",
      "saywuuut\n",
      "craycray\n",
      "2million\n",
      "worried\n",
      "cantwait\n",
      "envytoo\n",
      "\n",
      "ff\n",
      "sagittarius\n",
      "ifitwasntforfootball\n",
      "ifindthatattractive\n",
      "happytweet\n",
      "happytweet\n",
      "\n",
      "lmao\n",
      "twoogle\n",
      "teamstoner\n",
      "facttt\n",
      "superbotloy\n",
      "directionerproblems\n",
      "fantastica\n",
      "hurry\n",
      "oomf\n",
      "bestfeelinginarelationship\n",
      "bestfeelinginarelationship\n",
      "bestfeelinginarelationship\n",
      "bestfeelinginarelationship\n",
      "bestfeelinginarelationship\n",
      "bestfeelinginarelationship\n",
      "bestfeelinginarelationship\n",
      "bestfeelinginarelationship\n",
      "bestfeelinginarelationship\n",
      "bestfeelinginarelationship\n",
      "bestfeelinginarelationship\n",
      "followme\n",
      "waystomakemehappy\n",
      "iactuallyhadplans\n",
      "fml\n",
      "2\n",
      "sundaysofasnackday\n",
      "crazygolf\n",
      "thevoice\n",
      "singlelife\n",
      "\n",
      "excited\n",
      "random\n",
      "looongblaaad\n",
      "nowplaying\n",
      "waystomakemehappy\n",
      "whatwouldidowithoutyou\n",
      "yougetmajorpointsif\n",
      "whoareyoutojudge\n",
      "youprobablyhavehairypits\n",
      "ew\n",
      "procrastinating\n",
      "dontwannastudy\n",
      "navybestfriend\n",
      "mlib\n",
      "highexpectations\n",
      "socialmedia\n",
      "mybiggestfearis\n",
      "swag\n",
      "inpain\n",
      "belami\n",
      "royals\n",
      "rexhex\n",
      "selfcentered\n",
      "sometimesijustwant\n",
      "prscamp\n",
      "awesomesauce\n",
      "goodnight\n",
      "goodmorning\n",
      "admire\n",
      "beauty\n",
      "you\n",
      "greatday\n",
      "karthink\n",
      "mivskxi\n",
      "ipl\n",
      "bsbthanniversary\n",
      "ff\n",
      "dontwannago\n",
      "irishfan\n",
      "checkout\n",
      "edwardgreen\n",
      "theory\n",
      "peterlee\n",
      "psychobunny\n",
      "thingsthatiwanttohappen\n",
      "breakfeastofchamps\n",
      "justsaying\n",
      "justsaying\n",
      "justsaying\n",
      "waystomakemehappy\n",
      "wait\n",
      "thiswillbehard\n",
      "ificouldiwould\n",
      "ificouldiwould\n",
      "peoplemoreinfluentialthanrihanna\n",
      "shit\n",
      "\n",
      "werecrybabies\n",
      "redsox\n",
      "baseball\n",
      "letsgohawks\n",
      "abwars\n",
      "new\n",
      "stress\n",
      "lt\n",
      "loveitwhenyougetangry\n",
      "listmetobefollowedbyliam\n",
      "seattle\n",
      "therapist\n",
      "jobs\n",
      "sohappy\n",
      "creamcrackered\n",
      "proudofmyself\n",
      "neversaynever\n",
      "legend\n",
      "bucadipepo\n",
      "yougetmajorpointsif\n",
      "loveyoubunchesandtonsmore\n",
      "swag\n",
      "liamfollow\n",
      "thisshouldbefun\n",
      "alldayeveryday\n",
      "foreveralone\n",
      "letsseehowthatgoes\n",
      "fuckoffxbox\n",
      "canucks\n",
      "dailytweet\n",
      "23\n",
      "becauseican\n",
      "halfspanishatheart\n",
      "4\n",
      "lovesia\n",
      "booo\n",
      "blessing\n",
      "keepbelieving\n",
      "special\n",
      "sale\n",
      "cfc\n",
      "bestfriends\n",
      "helpeachother\n",
      "cherylsoldiers\n",
      "sidewaystheplay\n",
      "pause\n",
      "blockedbulba\n",
      "nobber\n",
      "neversaynever\n",
      "2\n",
      "caylenfollowme\n",
      "massageneeded\n",
      "prom22\n",
      "taurus\n",
      "fuck\n",
      "lovemyteam\n",
      "happy\n",
      "teamiphone\n",
      "taughtmemanylifelesson\n",
      "happyzoe\n",
      "breathe\n",
      "dgaf\n",
      "ff\n",
      "followhim\n",
      "ff\n",
      "followhim\n",
      "mschapin\n",
      "\n",
      "lotus\n",
      "\n",
      "nationalcouplesday\n",
      "looksopretty\n",
      "haters\n",
      "ah\n",
      "somanymemories\n",
      "cottygirls\n",
      "yum\n",
      "fml\n",
      "embracingchange\n",
      "collegelife\n",
      "hisbff\n",
      "idgaf\n",
      "rt\n",
      "fml\n",
      "thejokesonyou\n",
      "tbt\n",
      "inlove\n",
      "hawkward\n",
      "stopracism\n",
      "tooearlyforthis\n",
      "ff\n",
      "traptv\n",
      "traptv\n",
      "traptv\n",
      "pinkether\n",
      "teamether\n",
      "foreveralone\n",
      "werk\n",
      "werk\n",
      "oomf\n",
      "byebyefridayfeeling\n",
      "throwbackthursday\n",
      "heartbroken\n",
      "rioja\n",
      "hopoff\n",
      "dowork\n",
      "ballout\n",
      "pacerball\n",
      "you'rewelcome\n",
      "sexplaylist\n",
      "pornstar\n",
      "marcus\n",
      "neverthoughtiwoulddoit\n",
      "waltdisneyworld\n",
      "gbam\n",
      "imagine\n",
      "imagine\n",
      "imagine\n",
      "yvrbeertweetup\n",
      "32\n",
      "rcb\n",
      "csk\n",
      "scorpio\n",
      "\n",
      "thoughtsduringschool\n",
      "thoughtsduringschool\n",
      "proudmother\n",
      "oomf\n",
      "wellmissyou\n",
      "cantsayno\n",
      "ff\n",
      "kiehls\n",
      "itscrazyhow\n",
      "lettinggo\n",
      "movingon\n",
      "tl\n",
      "revenge\n",
      "lawandorder\n",
      "yay\n",
      "teamfunsizedd\n",
      "nowimdoingnothing\n",
      "youhadmeat\n",
      "jerrymaguire\n",
      "citytillidie\n",
      "easiestpart\n",
      "theonethatauntiejobought\n",
      "leavityeah\n",
      "nodoubt\n",
      "cl\n",
      "chebar\n",
      "fans\n",
      "joelward\n",
      "fans\n",
      "joelward\n",
      "fans\n",
      "joelward\n",
      "fans\n",
      "joelward\n",
      "fans\n",
      "joelward\n",
      "fans\n",
      "joelward\n",
      "fans\n",
      "joelward\n",
      "fans\n",
      "joelward\n",
      "fans\n",
      "joelward\n",
      "fans\n",
      "joelward\n",
      "fans\n",
      "joelward\n",
      "fans\n",
      "joelward\n",
      "fans\n",
      "joelward\n",
      "fans\n",
      "joelward\n",
      "fans\n",
      "joelward\n",
      "fans\n",
      "joelward\n",
      "fans\n",
      "joelward\n",
      "fans\n",
      "joelward\n",
      "constantbadmood\n",
      "lionking\n",
      "guilty\n",
      "\n",
      "take\n",
      "perfectweathertoswim\n",
      "pbbteens4\n",
      "np\n",
      "ificanthaveyou\n",
      "oomf\n",
      "yolo\n",
      "muchlove\n",
      "bulkingnotsulking\n",
      "tweetwhatyoueat\n",
      "hungoverhouse\n",
      "feelingsleepy\n",
      "whydontyouloveme\n",
      "bromance\n",
      "madeinchelsea\n",
      "bromance\n",
      "madeinchelsea\n",
      "bromance\n",
      "madeinchelsea\n",
      "bromance\n",
      "madeinchelsea\n",
      "bromance\n",
      "madeinchelsea\n",
      "bromance\n",
      "madeinchelsea\n",
      "backtoreality\n",
      "cyaflorida\n",
      "willbebackasap\n",
      "fml\n",
      "quotesofwisdom\n",
      "followback\n",
      "followback\n",
      "              filewords    position                         correction  \\\n",
      "146         #followback      (9, 1)  SHCHWDFOJHWFOQWDJFQDCKIHBQEDFCIQH   \n",
      "151          #unitytour      (9, 6)  SHCHWDFOJHWFOQWDJFQDCKIHBQEDFCIQH   \n",
      "167            #happy42    (10, 10)  SHCHWDFOJHWFOQWDJFQDCKIHBQEDFCIQH   \n",
      "170            #phasell     (11, 2)  SHCHWDFOJHWFOQWDJFQDCKIHBQEDFCIQH   \n",
      "319          #predators    (19, 15)  SHCHWDFOJHWFOQWDJFQDCKIHBQEDFCIQH   \n",
      "...                 ...         ...                                ...   \n",
      "138352  #willbebackasap  (9958, 12)  SHCHWDFOJHWFOQWDJFQDCKIHBQEDFCIQH   \n",
      "138419             #fml  (9963, 16)  SHCHWDFOJHWFOQWDJFQDCKIHBQEDFCIQH   \n",
      "138530  #quotesofwisdom   (9973, 9)  SHCHWDFOJHWFOQWDJFQDCKIHBQEDFCIQH   \n",
      "138702      #followback   (9984, 6)  SHCHWDFOJHWFOQWDJFQDCKIHBQEDFCIQH   \n",
      "138715      #followback   (9985, 6)  SHCHWDFOJHWFOQWDJFQDCKIHBQEDFCIQH   \n",
      "\n",
      "            tokens alternative correction  \n",
      "146     [hashtagg]            #followback  \n",
      "151     [hashtagg]             #unitytour  \n",
      "167     [hashtagg]               #happy42  \n",
      "170     [hashtagg]               #phasell  \n",
      "319     [hashtagg]             #predators  \n",
      "...            ...                    ...  \n",
      "138352  [hashtagg]        #willbebackasap  \n",
      "138419  [hashtagg]                   #fml  \n",
      "138530  [hashtagg]        #quotesofwisdom  \n",
      "138702  [hashtagg]            #followback  \n",
      "138715  [hashtagg]            #followback  \n",
      "\n",
      "[1551 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "test_df = preprocess(test_file_name, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_file_name = 'test_preprocessed_without_tf_idf_and_spellcheck.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 138899 is out of bounds for axis 0 with size 138899",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-284-f3fbf7d44a60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwrite_new_test_file_from_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_test_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-263-3fc6ea5e9214>\u001b[0m in \u001b[0;36mwrite_new_test_file_from_df\u001b[1;34m(data, filename)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mfh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{},\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline_number\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mstill_in_same_tweet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# write each word in correction.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                     \u001b[0mfh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-263-3fc6ea5e9214>\u001b[0m in \u001b[0;36mstill_in_same_tweet\u001b[1;34m(data, j)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\" Method checks if the next word is in the same tweet\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpos1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpos2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpos1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpos2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\etienne\\anaconda2\\envs\\py37\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2023\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2024\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2025\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2027\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\etienne\\anaconda2\\envs\\py37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   3006\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3007\u001b[0m             \u001b[0mseries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3008\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3010\u001b[0m         \u001b[0mseries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 138899 is out of bounds for axis 0 with size 138899"
     ]
    }
   ],
   "source": [
    "write_new_test_file_from_df(test_df, new_test_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"\".join(u) for u in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'#' == '#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = create_reference_dictionary(test_file_name, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ll = resolve_abbreviations(ll, twitter_abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ll = resolve_obvious_mistakes(ll, twitter_obvious_mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ll = resolve_onomatopee(ll, onomatopees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ll = resolve_laugh(ll, laughs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot\n",
      "madam\n",
      "do not\n",
      "I am\n",
      "you all\n",
      "I would\n",
      "do not\n",
      "nhls\n",
      "\n",
      "\n",
      "I am\n",
      "jessicas\n",
      "that is\n",
      "that is\n",
      "I will\n",
      "lauries\n",
      "he is\n",
      "that is\n",
      "cannot\n",
      "will not\n",
      "am not\n",
      "do not\n",
      "I am\n",
      "I am\n",
      "that is\n",
      "did not\n",
      "\n",
      "do not\n",
      "let us\n",
      "I am\n",
      "I am\n",
      "I will\n",
      "carlins\n",
      "I am\n",
      "have not\n",
      "cannot\n",
      "I have\n",
      "I am\n",
      "do not\n",
      "it is\n",
      "I am\n",
      "I am\n",
      "you are\n",
      "I am\n",
      "sarahs\n",
      "jahmais\n",
      "does not\n",
      "I am\n",
      "I am\n",
      "there is\n",
      "there is\n",
      "cannot\n",
      "do not\n",
      "you are\n",
      "is not\n",
      "cannot\n",
      "I will\n",
      "I will\n",
      "I will\n",
      "I will\n",
      "I will\n",
      "\n",
      "\n",
      "you are\n",
      "cannot\n",
      "I am\n",
      "that is\n",
      "I am\n",
      "cannot\n",
      "you are\n",
      "how is\n",
      "where is\n",
      "boos\n",
      "who is\n",
      "I am\n",
      "liers\n",
      "do not\n",
      "do not\n",
      "it is\n",
      "I will\n",
      "it is\n",
      "I will\n",
      "did not\n",
      "he is\n",
      "remotes\n",
      "it is\n",
      "I am\n",
      "they are\n",
      "I am\n",
      "I am\n",
      "have not\n",
      "I am\n",
      "I am\n",
      "I will\n",
      "you have\n",
      "you are\n",
      "it is\n",
      "I am\n",
      "do not\n",
      "did not\n",
      "do not\n",
      "cannot\n",
      "have not\n",
      "\n",
      "\n",
      "will not\n",
      "\n",
      "mens\n",
      "could not\n",
      "donts\n",
      "it is\n",
      "do not\n",
      "is not\n",
      "would not\n",
      "I have\n",
      "I am\n",
      "is not\n",
      "do not\n",
      "she is\n",
      "\n",
      "\n",
      "I am\n",
      "I am\n",
      "I will\n",
      "it is\n",
      "has not\n",
      "will not\n",
      "do not\n",
      "we are\n",
      "you will\n",
      "kartinis\n",
      "kartinis\n",
      "I am\n",
      "could not\n",
      "I am\n",
      "do not\n",
      "is not\n",
      "I am\n",
      "do not\n",
      "I will\n",
      "does not\n",
      "I am\n",
      "you will\n",
      "it is\n",
      "you are\n",
      "we will\n",
      "she is\n",
      "I am\n",
      "I am\n",
      "are not\n",
      "what is\n",
      "cannot\n",
      "thanks\n",
      "he is\n",
      "I have\n",
      "I am\n",
      "you are\n",
      "cannot\n",
      "it is\n",
      "I am\n",
      "it is\n",
      "djs\n",
      "could not\n",
      "electras\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "cannot\n",
      "I have\n",
      "do not\n",
      "who is\n",
      "she is\n",
      "I am\n",
      "do not\n",
      "I am\n",
      "I am\n",
      "\n",
      "\n",
      "let us\n",
      "I will\n",
      "that is\n",
      "I will\n",
      "I will\n",
      "let us\n",
      "grandmas\n",
      "I will\n",
      "catabout\n",
      "it is\n",
      "\n",
      "\n",
      "\n",
      "mommas\n",
      "\n",
      "do not\n",
      "do not\n",
      "I am\n",
      "I will\n",
      "could not\n",
      "fiddles\n",
      "do not\n",
      "it is\n",
      "would not\n",
      "\n",
      "boys\n",
      "do not\n",
      "would not\n",
      "did not\n",
      "I will\n",
      "did not\n",
      "cannot\n",
      "do not\n",
      "cannot\n",
      "it is\n",
      "cannot\n",
      "there is\n",
      "do not\n",
      "cannot\n",
      "it is\n",
      "kitans\n",
      "let us\n",
      "I will\n",
      "I am\n",
      "osos\n",
      "do not\n",
      "let us\n",
      "let us\n",
      "do not\n",
      "I am\n",
      "cannot\n",
      "\n",
      "\n",
      "it is\n",
      "that is\n",
      "you are\n",
      "that is\n",
      "did not\n",
      "you will\n",
      "hubbys\n",
      "I will\n",
      "I am\n",
      "I have\n",
      "I am\n",
      "lions\n",
      "lions\n",
      "cannot\n",
      "it is\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "websters\n",
      "it is\n",
      "imad\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "she is\n",
      "I am\n",
      "there is\n",
      "is not\n",
      "I am\n",
      "I have\n",
      "it is\n",
      "do not\n",
      "\n",
      "he is\n",
      "it is\n",
      "I am\n",
      "mcdonalds\n",
      "that would\n",
      "he is\n",
      "you are\n",
      "we are\n",
      "I am\n",
      "let us\n",
      "where is\n",
      "am not\n",
      "mens\n",
      "you are\n",
      "I am\n",
      "cannot\n",
      "I am\n",
      "it is\n",
      "do not\n",
      "I will\n",
      "will not\n",
      "do not\n",
      "storekeepers\n",
      "storekeepers\n",
      "you are\n",
      "did not\n",
      "kalyns\n",
      "kalyns\n",
      "they are\n",
      "I am\n",
      "I am\n",
      "we are\n",
      "I am\n",
      "have not\n",
      "do not\n",
      "it is\n",
      "I will\n",
      "do not\n",
      "will not\n",
      "you are\n",
      "have not\n",
      "I have\n",
      "it is\n",
      "you all\n",
      "fultons\n",
      "fultons\n",
      "I am\n",
      "it is\n",
      "she is\n",
      "cannot\n",
      "I am\n",
      "is not\n",
      "you have\n",
      "you are\n",
      "I am\n",
      "we will\n",
      "I am\n",
      "I am\n",
      "there is\n",
      "he is\n",
      "\n",
      "do not\n",
      "I am\n",
      "it is\n",
      "we are\n",
      "does not\n",
      "does not\n",
      "I am\n",
      "let us\n",
      "cannot\n",
      "I will\n",
      "I will\n",
      "you all\n",
      "it is\n",
      "I will\n",
      "what is\n",
      "I have\n",
      "daryias\n",
      "I will\n",
      "ure\n",
      "cannot\n",
      "will not\n",
      "I will\n",
      "I am\n",
      "do not\n",
      "do not\n",
      "do not\n",
      "do not\n",
      "do not\n",
      "do not\n",
      "do not\n",
      "do not\n",
      "do not\n",
      "do not\n",
      "will not\n",
      "keeeps\n",
      "I am\n",
      "I am\n",
      "was not\n",
      "cannot\n",
      "cannot\n",
      "cannot\n",
      "I am\n",
      "I am\n",
      "heres\n",
      "they are\n",
      "I am\n",
      "I am\n",
      "do not\n",
      "mens\n",
      "I have\n",
      "cannot\n",
      "we are\n",
      "who is\n",
      "it will\n",
      "you will\n",
      "I have\n",
      "gereks\n",
      "you are\n",
      "\n",
      "#thats\n",
      "gods\n",
      "I will\n",
      "you are\n",
      "I am\n",
      "am not\n",
      "I am\n",
      "did not\n",
      "you are\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "it is\n",
      "I am\n",
      "americas\n",
      "I am\n",
      "I am\n",
      "we are\n",
      "would not\n",
      "do not\n",
      "countrys\n",
      "I am\n",
      "did not\n",
      "I am\n",
      "naww\n",
      "cannot\n",
      "it is\n",
      "I am\n",
      "I will\n",
      "it is\n",
      "you are\n",
      "did not\n",
      "did not\n",
      "was not\n",
      "it is\n",
      "I am\n",
      "\n",
      "\n",
      "I will\n",
      "cannot\n",
      "do not\n",
      "cannot\n",
      "cannot\n",
      "I am\n",
      "I am\n",
      "it is\n",
      "let us\n",
      "I am\n",
      "I am\n",
      "does not\n",
      "I am\n",
      "cannot\n",
      "is not\n",
      "it is\n",
      "I have\n",
      "cannot\n",
      "should have\n",
      "cannot\n",
      "weeks\n",
      "did not\n",
      "londons\n",
      "I will\n",
      "there is\n",
      "will not\n",
      "there is\n",
      "will not\n",
      "there is\n",
      "will not\n",
      "there is\n",
      "will not\n",
      "there is\n",
      "will not\n",
      "there is\n",
      "will not\n",
      "do not\n",
      "do not\n",
      "that is\n",
      "dbanj\n",
      "katies\n",
      "liams\n",
      "we have\n",
      "I am\n",
      "ure\n",
      "do not\n",
      "did not\n",
      "I will\n",
      "I am\n",
      "\n",
      "you are\n",
      "you are\n",
      "does not\n",
      "cannot\n",
      "did not\n",
      "would have\n",
      "I would\n",
      "cannot\n",
      "websters\n",
      "do not\n",
      "she is\n",
      "do not\n",
      "I am\n",
      "it is\n",
      "\n",
      "todays\n",
      "that is\n",
      "you are\n",
      "could not\n",
      "yeahim\n",
      "do not\n",
      "that is\n",
      "that is\n",
      "\n",
      "\n",
      "would not\n",
      "I am\n",
      "cannot\n",
      "let us\n",
      "it is\n",
      "have not\n",
      "do not\n",
      "will not\n",
      "kartinis\n",
      "waynes\n",
      "there is\n",
      "cannot\n",
      "I am\n",
      "he is\n",
      "cannot\n",
      "cds\n",
      "you are\n",
      "you are\n",
      "I am\n",
      "I am\n",
      "cannot\n",
      "it is\n",
      "kaylees\n",
      "it is\n",
      "it is\n",
      "I am\n",
      "cannot\n",
      "I am\n",
      "athletes\n",
      "you have\n",
      "I am\n",
      "he is\n",
      "that is\n",
      "was not\n",
      "does not\n",
      "wev\n",
      "wev\n",
      "have not\n",
      "cannot\n",
      "should have\n",
      "do not\n",
      "bbms\n",
      "do not\n",
      "it is\n",
      "#s\n",
      "kids\n",
      "kids\n",
      "do not\n",
      "it is\n",
      "\n",
      "\n",
      "I will\n",
      "have not\n",
      "I am\n",
      "should not\n",
      "do not\n",
      "I am\n",
      "it is\n",
      "I am\n",
      "do not\n",
      "it is\n",
      "has not\n",
      "it is\n",
      "I am\n",
      "does not\n",
      "I am\n",
      "is not\n",
      "\n",
      "I am\n",
      "I am\n",
      "you are\n",
      "have not\n",
      "do not\n",
      "does not\n",
      "I am\n",
      "that is\n",
      "someones\n",
      "that is\n",
      "you are\n",
      "cannot\n",
      "they are\n",
      "I would\n",
      "you are\n",
      "dor\n",
      "pjs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zorrels\n",
      "it is\n",
      "daddys\n",
      "it is\n",
      "it is\n",
      "pauls\n",
      "you are\n",
      "I will\n",
      "friends\n",
      "I have\n",
      "I am\n",
      "does not\n",
      "she will\n",
      "todays\n",
      "that is\n",
      "do not\n",
      "they will\n",
      "do not\n",
      "cannot\n",
      "it is\n",
      "did not\n",
      "there is\n",
      "mbs\n",
      "I have\n",
      "I am\n",
      "I am\n",
      "friends\n",
      "he is\n",
      "he is\n",
      "will not\n",
      "that is\n",
      "I am\n",
      "do not\n",
      "I am\n",
      "cannot\n",
      "do not\n",
      "could have\n",
      "did not\n",
      "I am\n",
      "that is\n",
      "does not\n",
      "we are\n",
      "cannot\n",
      "I have\n",
      "that is\n",
      "did not\n",
      "it is\n",
      "I have\n",
      "let us\n",
      "am not\n",
      "I have\n",
      "it is\n",
      "do not\n",
      "I am\n",
      "it is\n",
      "\n",
      "I will\n",
      "do not\n",
      "he is\n",
      "peoples\n",
      "mens\n",
      "amazons\n",
      "amazons\n",
      "he is\n",
      "I am\n",
      "I am\n",
      "he is\n",
      "I am\n",
      "I have\n",
      "nkono\n",
      "\n",
      "\n",
      "it is\n",
      "it is\n",
      "was not\n",
      "that is\n",
      "that is\n",
      "I am\n",
      "I am\n",
      "he is\n",
      "he is\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "\n",
      "\n",
      "gagas\n",
      "\n",
      "I am\n",
      "I will\n",
      "hos\n",
      "you are\n",
      "I am\n",
      "I would\n",
      "do not\n",
      "does not\n",
      "do not\n",
      "I am\n",
      "it is\n",
      "it is\n",
      "I am\n",
      "it is\n",
      "do not\n",
      "cannot\n",
      "\n",
      "\n",
      "you will\n",
      "I am\n",
      "I am\n",
      "\n",
      "it is\n",
      "you have\n",
      "anybodys\n",
      "I am\n",
      "were not\n",
      "#weallknowyouregoingtodowellyourenotfoolinganyone\n",
      "it will\n",
      "I am\n",
      "has not\n",
      "I am\n",
      "carmellas\n",
      "cannot\n",
      "would not\n",
      "it is\n",
      "heres\n",
      "where is\n",
      "I have\n",
      "that is\n",
      "I am\n",
      "\n",
      "do not\n",
      "I am\n",
      "xlris\n",
      "you are\n",
      "did not\n",
      "did not\n",
      "I am\n",
      "I am\n",
      "you are\n",
      "it is\n",
      "\n",
      "\n",
      "I am\n",
      "I have\n",
      "it is\n",
      "that is\n",
      "I am\n",
      "he is\n",
      "did not\n",
      "I will\n",
      "you will\n",
      "do not\n",
      "who is\n",
      "did not\n",
      "I am\n",
      "did not\n",
      "cannot\n",
      "I will\n",
      "mens\n",
      "\n",
      "d\n",
      "\n",
      "did not\n",
      "did not\n",
      "do not\n",
      "I have\n",
      "you have\n",
      "that is\n",
      "you are\n",
      "you have\n",
      "that is\n",
      "you are\n",
      "you have\n",
      "that is\n",
      "you are\n",
      "I am\n",
      "I am\n",
      "I will\n",
      "it is\n",
      "cannot\n",
      "you are\n",
      "I will\n",
      "gordonreeds\n",
      "have not\n",
      "I would\n",
      "you are\n",
      "do not\n",
      "I am\n",
      "do not\n",
      "I am\n",
      "socialites\n",
      "authors\n",
      "that is\n",
      "I am\n",
      "am not\n",
      "I am\n",
      "that is\n",
      "I am\n",
      "ryans\n",
      "we will\n",
      "do not\n",
      "that is\n",
      "I am\n",
      "you all\n",
      "do not\n",
      "I am\n",
      "I am\n",
      "do not\n",
      "do not\n",
      "it is\n",
      "collectors\n",
      "collectors\n",
      "I will\n",
      "websters\n",
      "cannot\n",
      "will not\n",
      "\n",
      "you all\n",
      "cannot\n",
      "I am\n",
      "\n",
      "\n",
      "\n",
      "I am\n",
      "cannot\n",
      "todays\n",
      "childs\n",
      "it is\n",
      "that is\n",
      "I have\n",
      "I will\n",
      "I will\n",
      "I will\n",
      "where is\n",
      "I am\n",
      "was not\n",
      "I am\n",
      "it is\n",
      "am not\n",
      "todays\n",
      "iniestas\n",
      "are not\n",
      "it is\n",
      "it is\n",
      "that is\n",
      "that is\n",
      "I am\n",
      "you are\n",
      "I am\n",
      "you are\n",
      "do not\n",
      "I will\n",
      "you are\n",
      "he is\n",
      "do not\n",
      "you are\n",
      "you are\n",
      "has not\n",
      "is not\n",
      "let us\n",
      "it is\n",
      "did not\n",
      "have not\n",
      "we are\n",
      "you are\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "she is\n",
      "what is\n",
      "sobs\n",
      "cannot\n",
      "do not\n",
      "did not\n",
      "did not\n",
      "does not\n",
      "cannot\n",
      "have not\n",
      "madam\n",
      "do not\n",
      "was not\n",
      "I am\n",
      "is not\n",
      "what is\n",
      "do not\n",
      "could have\n",
      "do not\n",
      "I am\n",
      "do not\n",
      "he is\n",
      "do not\n",
      "I am\n",
      "let us\n",
      "I am\n",
      "\n",
      "cannot\n",
      "we are\n",
      "I am\n",
      "what is\n",
      "I am\n",
      "you have\n",
      "do not\n",
      "I have\n",
      "johns\n",
      "johns\n",
      "kiras\n",
      "johns\n",
      "johns\n",
      "kiras\n",
      "I am\n",
      "it is\n",
      "I have\n",
      "I am\n",
      "charlizes\n",
      "should not\n",
      "you are\n",
      "shits\n",
      "websters\n",
      "do not\n",
      "we are\n",
      "you are\n",
      "he is\n",
      "I am\n",
      "do not\n",
      "I am\n",
      "he shall\n",
      "did not\n",
      "did not\n",
      "mens\n",
      "mens\n",
      "you are\n",
      "you are\n",
      "I will\n",
      "cannot\n",
      "cannot\n",
      "have not\n",
      "I will\n",
      "it is\n",
      "I am\n",
      "cannot\n",
      "you are\n",
      "do not\n",
      "bros\n",
      "cannot\n",
      "I will\n",
      "it is\n",
      "I will\n",
      "that is\n",
      "there is\n",
      "I am\n",
      "it is\n",
      "nialls\n",
      "do not\n",
      "you will\n",
      "you are\n",
      "did not\n",
      "I am\n",
      "do not\n",
      "do not\n",
      "I am\n",
      "you are\n",
      "how is\n",
      "bbming\n",
      "she is\n",
      "\n",
      "you are\n",
      "what is\n",
      "womens\n",
      "womens\n",
      "I am\n",
      "will not\n",
      "I am\n",
      "it is\n",
      "let us\n",
      "it is\n",
      "it is\n",
      "I am\n",
      "we will\n",
      "jumuah\n",
      "I am\n",
      "solomons\n",
      "you will\n",
      "I am\n",
      "\n",
      "do not\n",
      "I will\n",
      "widows\n",
      "widows\n",
      "it is\n",
      "who is\n",
      "who is\n",
      "there is\n",
      "I am\n",
      "he shall\n",
      "I would\n",
      "I am\n",
      "do not\n",
      "cannot\n",
      "I am\n",
      "\n",
      "will not\n",
      "I will\n",
      "do not\n",
      "he is\n",
      "we will\n",
      "cannot\n",
      "it is\n",
      "do not\n",
      "it is\n",
      "I am\n",
      "you have\n",
      "I will\n",
      "it is\n",
      "it will\n",
      "I am\n",
      "carols\n",
      "I am\n",
      "what is\n",
      "jessicas\n",
      "coltons\n",
      "I am\n",
      "do not\n",
      "I am\n",
      "does not\n",
      "she will\n",
      "favs\n",
      "do not\n",
      "do not\n",
      "it is\n",
      "do not\n",
      "yourve\n",
      "tellem\n",
      "cannot\n",
      "it is\n",
      "I am\n",
      "I am\n",
      "it is\n",
      "I am\n",
      "I have\n",
      "I am\n",
      "mccoys\n",
      "I will\n",
      "I will\n",
      "I am\n",
      "I have\n",
      "I am\n",
      "we are\n",
      "hazems\n",
      "do not\n",
      "you are\n",
      "I have\n",
      "I am\n",
      "cannot\n",
      "do not\n",
      "am not\n",
      "thanks\n",
      "do not\n",
      "cannot\n",
      "do not\n",
      "cannot\n",
      "it is\n",
      "cannot\n",
      "did not\n",
      "what is\n",
      "do not\n",
      "you are\n",
      "you are\n",
      "have not\n",
      "that is\n",
      "did not\n",
      "greys\n",
      "where is\n",
      "I have\n",
      "she will\n",
      "you will\n",
      "\n",
      "you are\n",
      "cannot\n",
      "there is\n",
      "cannot\n",
      "I will\n",
      "that is\n",
      "I am\n",
      "I am\n",
      "did not\n",
      "will not\n",
      "it is\n",
      "I will\n",
      "cannot\n",
      "\n",
      "you will\n",
      "you will\n",
      "I am\n",
      "are not\n",
      "I am\n",
      "dads\n",
      "you will\n",
      "do not\n",
      "cannot\n",
      "it is\n",
      "years\n",
      "I am\n",
      "cakesnshakes\n",
      "I am\n",
      "that is\n",
      "I am\n",
      "who is\n",
      "cannot\n",
      "tvs\n",
      "I am\n",
      "it is\n",
      "I am\n",
      "it is\n",
      "do not\n",
      "do not\n",
      "who is\n",
      "it is\n",
      "macphersons\n",
      "have not\n",
      "I have\n",
      "I am\n",
      "let us\n",
      "she is\n",
      "biggs\n",
      "\n",
      "\n",
      "that is\n",
      "it is\n",
      "\n",
      "\n",
      "have not\n",
      "were not\n",
      "do not\n",
      "you have\n",
      "do not\n",
      "he is\n",
      "it is\n",
      "it will\n",
      "does not\n",
      "it is\n",
      "I am\n",
      "I will\n",
      "do not\n",
      "do not\n",
      "I will\n",
      "he shall\n",
      "\n",
      "do not\n",
      "it is\n",
      "\n",
      "does not\n",
      "cannot\n",
      "do not\n",
      "do not\n",
      "I have\n",
      "I am\n",
      "\n",
      "I will\n",
      "I am\n",
      "do not\n",
      "\n",
      "I am\n",
      "I am\n",
      "everythings\n",
      "that is\n",
      "I am\n",
      "I am\n",
      "it is\n",
      "it is\n",
      "do not\n",
      "do not\n",
      "I am\n",
      "cannot\n",
      "I am\n",
      "everyones\n",
      "I am\n",
      "\n",
      "\n",
      "#ffs\n",
      "nialls\n",
      "I am\n",
      "we are\n",
      "he is\n",
      "cannot\n",
      "I am\n",
      "I will\n",
      "bras\n",
      "do not\n",
      "I have\n",
      "it is\n",
      "it is\n",
      "cannot\n",
      "zayns\n",
      "I will\n",
      "I have\n",
      "it is\n",
      "it is\n",
      "rikis\n",
      "I am\n",
      "have not\n",
      "do not\n",
      "let us\n",
      "he shall\n",
      "it is\n",
      "it is\n",
      "is not\n",
      "are not\n",
      "cannot\n",
      "it is\n",
      "will not\n",
      "I am\n",
      "that is\n",
      "I will\n",
      "you all\n",
      "I am\n",
      "I am\n",
      "that is\n",
      "cannot\n",
      "is not\n",
      "it is\n",
      "does not\n",
      "were not\n",
      "that is\n",
      "I am\n",
      "cannot\n",
      "that is\n",
      "I will\n",
      "I will\n",
      "cannot\n",
      "bigo\n",
      "you are\n",
      "\n",
      "\n",
      "I am\n",
      "I am\n",
      "cannot\n",
      "you are\n",
      "we are\n",
      "I will\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "cannot\n",
      "I am\n",
      "I am\n",
      "mums\n",
      "was not\n",
      "do not\n",
      "mens\n",
      "do not\n",
      "did not\n",
      "I will\n",
      "I am\n",
      "I am\n",
      "luthers\n",
      "luthers\n",
      "luthers\n",
      "luthers\n",
      "was not\n",
      "I am\n",
      "it is\n",
      "\n",
      "\n",
      "you are\n",
      "I am\n",
      "cannot\n",
      "I am\n",
      "it is\n",
      "they will\n",
      "it is\n",
      "I am\n",
      "have not\n",
      "do not\n",
      "did not\n",
      "did not\n",
      "cannot\n",
      "that is\n",
      "they will\n",
      "I am\n",
      "I will\n",
      "is not\n",
      "I am\n",
      "is not\n",
      "she will\n",
      "daddys\n",
      "cannot\n",
      "I will\n",
      "I will\n",
      "nandos\n",
      "I will\n",
      "I am\n",
      "cannot\n",
      "I am\n",
      "do not\n",
      "would not\n",
      "I am\n",
      "it is\n",
      "did not\n",
      "it is\n",
      "\n",
      "that is\n",
      "cannot\n",
      "I will\n",
      "I have\n",
      "\n",
      "I will\n",
      "do not\n",
      "will not\n",
      "cannot\n",
      "joshs\n",
      "is not\n",
      "is not\n",
      "will not\n",
      "anyones\n",
      "I am\n",
      "it is\n",
      "I am\n",
      "it is\n",
      "it is\n",
      "I will\n",
      "do not\n",
      "I am\n",
      "beginners\n",
      "beginners\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "it is\n",
      "it is\n",
      "\n",
      "when is\n",
      "I am\n",
      "have not\n",
      "do not\n",
      "cannot\n",
      "cannot\n",
      "there is\n",
      "do not\n",
      "I will\n",
      "did not\n",
      "I will\n",
      "he is\n",
      "megans\n",
      "ladys\n",
      "I would\n",
      "I am\n",
      "tasmans\n",
      "tasmans\n",
      "they are\n",
      "\n",
      "do not\n",
      "cannot\n",
      "I am\n",
      "it is\n",
      "you are\n",
      "you have\n",
      "I am\n",
      "it is\n",
      "I have\n",
      "did not\n",
      "do not\n",
      "that is\n",
      "she is\n",
      "I have\n",
      "it is\n",
      "you are\n",
      "I have\n",
      "cannot\n",
      "I am\n",
      "we are\n",
      "did not\n",
      "I have\n",
      "mitzvahs\n",
      "it is\n",
      "there is\n",
      "you are\n",
      "I am\n",
      "I am\n",
      "cannot\n",
      "do not\n",
      "do not\n",
      "I am\n",
      "I am\n",
      "ull\n",
      "elses\n",
      "bells\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "you are\n",
      "girlfriends\n",
      "nobodys\n",
      "I will\n",
      "there is\n",
      "I will\n",
      "it is\n",
      "why is\n",
      "do not\n",
      "what is\n",
      "that is\n",
      "that is\n",
      "they are\n",
      "I am\n",
      "do not\n",
      "kaylas\n",
      "did not\n",
      "it is\n",
      "it is\n",
      "am not\n",
      "does not\n",
      "\n",
      "let us\n",
      "I will\n",
      "mcgrawhills\n",
      "I will\n",
      "have not\n",
      "I am\n",
      "I am\n",
      "it will\n",
      "womens\n",
      "womens\n",
      "should have\n",
      "I will\n",
      "geographics\n",
      "it will\n",
      "mothers\n",
      "do not\n",
      "do not\n",
      "is not\n",
      "I am\n",
      "I would\n",
      "I have\n",
      "d\n",
      "you will\n",
      "cannot\n",
      "I am\n",
      "I am\n",
      "\n",
      "cannot\n",
      "I am\n",
      "it is\n",
      "cannot\n",
      "do not\n",
      "did not\n",
      "that is\n",
      "it is\n",
      "will not\n",
      "melissas\n",
      "would not\n",
      "it is\n",
      "am not\n",
      "do not\n",
      "I am\n",
      "I will\n",
      "I am\n",
      "I am\n",
      "do not\n",
      "there is\n",
      "it will\n",
      "cannot\n",
      "\n",
      "\n",
      "that is\n",
      "I am\n",
      "I am\n",
      "I have\n",
      "I will\n",
      "do not\n",
      "will not\n",
      "brothers\n",
      "do not\n",
      "I am\n",
      "there is\n",
      "\n",
      "do not\n",
      "it is\n",
      "let us\n",
      "it is\n",
      "he had\n",
      "cannot\n",
      "you have\n",
      "do not\n",
      "\n",
      "\n",
      "everyones\n",
      "should not\n",
      "do not\n",
      "girlfriends\n",
      "does not\n",
      "cannot\n",
      "we will\n",
      "that is\n",
      "I am\n",
      "did not\n",
      "it is\n",
      "I am\n",
      "I am\n",
      "\n",
      "it is\n",
      "I have\n",
      "I will\n",
      "you are\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "iv\n",
      "do not\n",
      "\n",
      "\n",
      "\n",
      "I am\n",
      "did not\n",
      "cannot\n",
      "I am\n",
      "do not\n",
      "I am\n",
      "it is\n",
      "you will\n",
      "it is\n",
      "I am\n",
      "it is\n",
      "I am\n",
      "I would\n",
      "I am\n",
      "it is\n",
      "does not\n",
      "does not\n",
      "does not\n",
      "does not\n",
      "does not\n",
      "you are\n",
      "would not\n",
      "I am\n",
      "you are\n",
      "I am\n",
      "we have\n",
      "we are\n",
      "that is\n",
      "I am\n",
      "I am\n",
      "do not\n",
      "I am\n",
      "do not\n",
      "cannot\n",
      "do not\n",
      "I have\n",
      "I have\n",
      "cannot\n",
      "did not\n",
      "cannot\n",
      "cannot\n",
      "dms\n",
      "I would\n",
      "it is\n",
      "I have\n",
      "am not\n",
      "do not\n",
      "I have\n",
      "I would\n",
      "you are\n",
      "cannot\n",
      "do not\n",
      "I am\n",
      "he shall\n",
      "cannot\n",
      "I am\n",
      "will not\n",
      "should not\n",
      "I am\n",
      "I will\n",
      "who is\n",
      "you will\n",
      "who is\n",
      "I will\n",
      "who is\n",
      "you will\n",
      "who is\n",
      "I will\n",
      "who is\n",
      "you will\n",
      "who is\n",
      "I will\n",
      "I have\n",
      "I am\n",
      "bobs\n",
      "I will\n",
      "I would\n",
      "cannot\n",
      "do not\n",
      "todays\n",
      "saturdays\n",
      "do not\n",
      "it is\n",
      "it is\n",
      "yday\n",
      "does not\n",
      "soyoungs\n",
      "do not\n",
      "kiddos\n",
      "cannot\n",
      "there is\n",
      "I will\n",
      "was not\n",
      "I am\n",
      "let us\n",
      "we are\n",
      "I am\n",
      "who is\n",
      "we are\n",
      "bds\n",
      "you are\n",
      "she is\n",
      "will not\n",
      "am not\n",
      "that is\n",
      "mcgrawhills\n",
      "I am\n",
      "do not\n",
      "who is\n",
      "I have\n",
      "was not\n",
      "I have\n",
      "cannot\n",
      "I am\n",
      "it is\n",
      "you will\n",
      "\n",
      "I am\n",
      "do not\n",
      "I will\n",
      "we are\n",
      "that is\n",
      "do not\n",
      "do not\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "childrens\n",
      "you are\n",
      "I will\n",
      "do not\n",
      "I am\n",
      "I am\n",
      "cannot\n",
      "was not\n",
      "it is\n",
      "cannot\n",
      "it would\n",
      "do not\n",
      "I have\n",
      "what is\n",
      "I am\n",
      "turtles\n",
      "cannot\n",
      "we are\n",
      "cannot\n",
      "I am\n",
      "did not\n",
      "does not\n",
      "there is\n",
      "\n",
      "womens\n",
      "you are\n",
      "I would\n",
      "I have\n",
      "has not\n",
      "she is\n",
      "I am\n",
      "do not\n",
      "I have\n",
      "do not\n",
      "that is\n",
      "do not\n",
      "you are\n",
      "\n",
      "\n",
      "heres\n",
      "tittles\n",
      "you would\n",
      "did not\n",
      "who is\n",
      "you all\n",
      "I am\n",
      "do not\n",
      "it is\n",
      "that is\n",
      "maynards\n",
      "I am\n",
      "I am\n",
      "could not\n",
      "he is\n",
      "\n",
      "I am\n",
      "\n",
      "you are\n",
      "it is\n",
      "\n",
      "\n",
      "I am\n",
      "did not\n",
      "did not\n",
      "you are\n",
      "chalkeys\n",
      "let us\n",
      "I have\n",
      "have not\n",
      "I am\n",
      "I am\n",
      "do not\n",
      "would not\n",
      "cannot\n",
      "I will\n",
      "I am\n",
      "cannot\n",
      "it is\n",
      "it is\n",
      "I will\n",
      "cannot\n",
      "cannot\n",
      "\n",
      "will not\n",
      "you would\n",
      "dya\n",
      "do not\n",
      "\n",
      "does not\n",
      "I am\n",
      "cannot\n",
      "do not\n",
      "songs\n",
      "biebers\n",
      "it is\n",
      "that is\n",
      "I am\n",
      "you have\n",
      "pws\n",
      "do not\n",
      "\n",
      "she is\n",
      "will not\n",
      "does not\n",
      "do not\n",
      "I am\n",
      "someones\n",
      "she is\n",
      "it is\n",
      "it is\n",
      "do not\n",
      "I have\n",
      "do not\n",
      "it is\n",
      "collectors\n",
      "it is\n",
      "collectors\n",
      "we are\n",
      "\n",
      "is not\n",
      "what is\n",
      "\n",
      "do not\n",
      "it is\n",
      "you are\n",
      "you are\n",
      "cannot\n",
      "todays\n",
      "do not\n",
      "I am\n",
      "it is\n",
      "berrys\n",
      "it is\n",
      "berrys\n",
      "it is\n",
      "berrys\n",
      "it is\n",
      "berrys\n",
      "it is\n",
      "berrys\n",
      "cannot\n",
      "rtn\n",
      "I am\n",
      "do not\n",
      "you are\n",
      "do not\n",
      "what is\n",
      "I am\n",
      "gon\n",
      "cannot\n",
      "drakequotess\n",
      "do not\n",
      "I am\n",
      "chiccos\n",
      "\n",
      "\n",
      "carpenters\n",
      "carpenters\n",
      "I am\n",
      "she will\n",
      "I am\n",
      "has not\n",
      "cannot\n",
      "I am\n",
      "do not\n",
      "it is\n",
      "cannot\n",
      "I am\n",
      "you will\n",
      "\n",
      "cannot\n",
      "I am\n",
      "you will\n",
      "I am\n",
      "will not\n",
      "I am\n",
      "do not\n",
      "mens\n",
      "\n",
      "\n",
      "he is\n",
      "I will\n",
      "I am\n",
      "I am\n",
      "we are\n",
      "I am\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fored\n",
      "\n",
      "doctors\n",
      "do not\n",
      "I have\n",
      "should not\n",
      "cannot\n",
      "I am\n",
      "ronaldos\n",
      "ronaldos\n",
      "I am\n",
      "that is\n",
      "you are\n",
      "is not\n",
      "it is\n",
      "I will\n",
      "let us\n",
      "how is\n",
      "I am\n",
      "you will\n",
      "was not\n",
      "do not\n",
      "I have\n",
      "I am\n",
      "you will\n",
      "I am\n",
      "I am\n",
      "it is\n",
      "that is\n",
      "you all\n",
      "are not\n",
      "I am\n",
      "cannot\n",
      "that is\n",
      "it is\n",
      "\n",
      "womens\n",
      "womens\n",
      "do not\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "peoples\n",
      "I am\n",
      "beginners\n",
      "I have\n",
      "cannot\n",
      "do not\n",
      "I would\n",
      "I am\n",
      "it is\n",
      "you are\n",
      "I will\n",
      "do not\n",
      "do not\n",
      "how is\n",
      "it is\n",
      "that is\n",
      "will not\n",
      "I am\n",
      "I am\n",
      "you all\n",
      "it is\n",
      "I have\n",
      "do not\n",
      "would not\n",
      "did not\n",
      "kangs\n",
      "somethings\n",
      "will not\n",
      "does not\n",
      "I am\n",
      "did not\n",
      "I am\n",
      "toms\n",
      "I am\n",
      "we will\n",
      "we will\n",
      "I will\n",
      "that is\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "that is\n",
      "cannot\n",
      "that is\n",
      "cannot\n",
      "I am\n",
      "I am\n",
      "could not\n",
      "I am\n",
      "I am\n",
      "there is\n",
      "cannot\n",
      "I am\n",
      "I will\n",
      "americas\n",
      "dolls\n",
      "it is\n",
      "is not\n",
      "do not\n",
      "cannot\n",
      "arnt\n",
      "theryre\n",
      "I am\n",
      "do not\n",
      "it is\n",
      "westlands\n",
      "it will\n",
      "I will\n",
      "what is\n",
      "that is\n",
      "where is\n",
      "do not\n",
      "I am\n",
      "do not\n",
      "it is\n",
      "nawh\n",
      "I am\n",
      "I am\n",
      "have not\n",
      "womens\n",
      "nurses\n",
      "nurses\n",
      "I am\n",
      "someones\n",
      "I will\n",
      "I am\n",
      "it is\n",
      "will not\n",
      "that is\n",
      "do not\n",
      "I am\n",
      "it is\n",
      "what is\n",
      "what is\n",
      "I am\n",
      "they will\n",
      "childrens\n",
      "followers\n",
      "I have\n",
      "I am\n",
      "I am\n",
      "we have\n",
      "you are\n",
      "I am\n",
      "will not\n",
      "mccalls\n",
      "womens\n",
      "womens\n",
      "amazons\n",
      "cannot\n",
      "that is\n",
      "cannot\n",
      "does not\n",
      "I would\n",
      "you are\n",
      "he is\n",
      "was not\n",
      "australias\n",
      "will not\n",
      "where is\n",
      "\n",
      "that is\n",
      "I am\n",
      "that is\n",
      "I am\n",
      "that is\n",
      "do not\n",
      "childs\n",
      "you all\n",
      "have not\n",
      "she is\n",
      "she is\n",
      "she is\n",
      "she is\n",
      "that is\n",
      "I will\n",
      "it is\n",
      "where is\n",
      "owners\n",
      "does not\n",
      "where is\n",
      "you are\n",
      "I am\n",
      "where is\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "wests\n",
      "I am\n",
      "do not\n",
      "I am\n",
      "that is\n",
      "did not\n",
      "I am\n",
      "let us\n",
      "do not\n",
      "you will\n",
      "I am\n",
      "\n",
      "I am\n",
      "he is\n",
      "\n",
      "there is\n",
      "it is\n",
      "yous\n",
      "will not\n",
      "do not\n",
      "I have\n",
      "did not\n",
      "they are\n",
      "that is\n",
      "you will\n",
      "deans\n",
      "that is\n",
      "what is\n",
      "cannot\n",
      "do not\n",
      "someones\n",
      "you are\n",
      "that is\n",
      "will not\n",
      "I have\n",
      "cannot\n",
      "\n",
      "\n",
      "would not\n",
      "you are\n",
      "you are\n",
      "nights\n",
      "I am\n",
      "were not\n",
      "that is\n",
      "do not\n",
      "did not\n",
      "will not\n",
      "I have\n",
      "womens\n",
      "womens\n",
      "I would\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "did not\n",
      "do not\n",
      "chius\n",
      "I am\n",
      "heres\n",
      "you have\n",
      "I am\n",
      "that is\n",
      "she is\n",
      "cannot\n",
      "could not\n",
      "that is\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "it is\n",
      "todays\n",
      "I am\n",
      "\n",
      "I have\n",
      "how is\n",
      "do not\n",
      "I am\n",
      "was not\n",
      "do not\n",
      "cannot\n",
      "cannot\n",
      "I am\n",
      "dmixture\n",
      "mcnallys\n",
      "I am\n",
      "I am\n",
      "do not\n",
      "I am\n",
      "would not\n",
      "did not\n",
      "I am\n",
      "I am\n",
      "that is\n",
      "it is\n",
      "could not\n",
      "did not\n",
      "what is\n",
      "I am\n",
      "I am\n",
      "could not\n",
      "I am\n",
      "could not\n",
      "cannot\n",
      "would have\n",
      "do not\n",
      "what is\n",
      "will not\n",
      "I am\n",
      "did not\n",
      "mens\n",
      "mens\n",
      "you are\n",
      "mias\n",
      "did not\n",
      "you are\n",
      "you will\n",
      "I am\n",
      "you are\n",
      "\n",
      "mummys\n",
      "\n",
      "I am\n",
      "does not\n",
      "I am\n",
      "I am\n",
      "I have\n",
      "\n",
      "will not\n",
      "will not\n",
      "will not\n",
      "will not\n",
      "will not\n",
      "will not\n",
      "did not\n",
      "I am\n",
      "readers\n",
      "I will\n",
      "individuals\n",
      "do not\n",
      "I am\n",
      "I am\n",
      "do not\n",
      "I am\n",
      "cannot\n",
      "that is\n",
      "that is\n",
      "cannot\n",
      "I have\n",
      "there is\n",
      "I am\n",
      "will not\n",
      "am not\n",
      "I have\n",
      "I am\n",
      "I am\n",
      "cannot\n",
      "do not\n",
      "I am\n",
      "you have\n",
      "that is\n",
      "I will\n",
      "schwartzs\n",
      "schwartzs\n",
      "it is\n",
      "feltons\n",
      "I am\n",
      "that is\n",
      "I am\n",
      "I am\n",
      "cannot\n",
      "does not\n",
      "cannot\n",
      "\n",
      "I am\n",
      "do not\n",
      "pilots\n",
      "pilots\n",
      "\n",
      "\n",
      "he is\n",
      "\n",
      "\n",
      "I will\n",
      "they are\n",
      "she is\n",
      "does not\n",
      "it is\n",
      "\n",
      "\n",
      "somebodys\n",
      "how is\n",
      "I am\n",
      "you are\n",
      "shannys\n",
      "I am\n",
      "where is\n",
      "I will\n",
      "\n",
      "\n",
      "it is\n",
      "it is\n",
      "has not\n",
      "was not\n",
      "I am\n",
      "do not\n",
      "do not\n",
      "I will\n",
      "you are\n",
      "let us\n",
      "that is\n",
      "he shall\n",
      "imma\n",
      "do not\n",
      "will not\n",
      "do not\n",
      "we are\n",
      "you will\n",
      "you are\n",
      "you are\n",
      "cannot\n",
      "it is\n",
      "will not\n",
      "thatll\n",
      "you will\n",
      "cannot\n",
      "I am\n",
      "that is\n",
      "\n",
      "it is\n",
      "do not\n",
      "wer\n",
      "would not\n",
      "it is\n",
      "I will\n",
      "I will\n",
      "I am\n",
      "do not\n",
      "did not\n",
      "will not\n",
      "do not\n",
      "do not\n",
      "it is\n",
      "cannot\n",
      "\n",
      "punkd\n",
      "I am\n",
      "you are\n",
      "you are\n",
      "do not\n",
      "you are\n",
      "you are\n",
      "do not\n",
      "you are\n",
      "you are\n",
      "do not\n",
      "coopers\n",
      "\n",
      "I am\n",
      "ure\n",
      "mens\n",
      "have not\n",
      "I am\n",
      "cannot\n",
      "is not\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "endore\n",
      "will not\n",
      "I am\n",
      "cannot\n",
      "were not\n",
      "\n",
      "\n",
      "I will\n",
      "I am\n",
      "you are\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "\n",
      "\n",
      "did not\n",
      "I will\n",
      "he is\n",
      "let us\n",
      "I am\n",
      "I am\n",
      "laptops\n",
      "I am\n",
      "\n",
      "do not\n",
      "I have\n",
      "I will\n",
      "do not\n",
      "do not\n",
      "I will\n",
      "deitels\n",
      "have not\n",
      "do not\n",
      "would not\n",
      "I am\n",
      "I am\n",
      "what is\n",
      "I am\n",
      "we are\n",
      "I am\n",
      "that is\n",
      "they are\n",
      "would have\n",
      "we are\n",
      "\n",
      "cannot\n",
      "do not\n",
      "is not\n",
      "asirafs\n",
      "do not\n",
      "primas\n",
      "cannot\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "\n",
      "\n",
      "#capricorns\n",
      "they are\n",
      "you are\n",
      "I would\n",
      "do not\n",
      "should not\n",
      "that is\n",
      "I am\n",
      "it is\n",
      "you are\n",
      "you are\n",
      "I am\n",
      "has not\n",
      "there is\n",
      "I will\n",
      "I would\n",
      "I would\n",
      "djs\n",
      "who is\n",
      "littletons\n",
      "littletons\n",
      "was not\n",
      "that is\n",
      "that is\n",
      "sisters\n",
      "it is\n",
      "it is\n",
      "I am\n",
      "\n",
      "\n",
      "you are\n",
      "it is\n",
      "it is\n",
      "do not\n",
      "that is\n",
      "that is\n",
      "folgers\n",
      "I am\n",
      "is not\n",
      "does not\n",
      "cannot\n",
      "I am\n",
      "do not\n",
      "have not\n",
      "you are\n",
      "I am\n",
      "I will\n",
      "I have\n",
      "I have\n",
      "cannot\n",
      "I will\n",
      "have not\n",
      "it is\n",
      "do not\n",
      "do not\n",
      "I will\n",
      "it is\n",
      "do not\n",
      "I will\n",
      "tang\n",
      "are not\n",
      "\n",
      "peoples\n",
      "will not\n",
      "has not\n",
      "websters\n",
      "he shall\n",
      "it is\n",
      "she is\n",
      "I am\n",
      "did not\n",
      "did not\n",
      "that is\n",
      "what is\n",
      "you are\n",
      "I am\n",
      "I am\n",
      "I will\n",
      "let us\n",
      "I will\n",
      "are not\n",
      "you will\n",
      "dennys\n",
      "dennys\n",
      "I am\n",
      "do not\n",
      "it is\n",
      "collectors\n",
      "rukias\n",
      "do not\n",
      "you are\n",
      "you are\n",
      "you are\n",
      "I am\n",
      "you are\n",
      "moms\n",
      "I am\n",
      "has not\n",
      "have not\n",
      "I am\n",
      "I will\n",
      "I will\n",
      "I would\n",
      "cannot\n",
      "where is\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "that is\n",
      "\n",
      "it is\n",
      "that is\n",
      "I have\n",
      "\n",
      "do not\n",
      "\n",
      "we will\n",
      "it is\n",
      "nations\n",
      "nations\n",
      "it is\n",
      "friends\n",
      "ur\n",
      "I am\n",
      "\n",
      "\n",
      "sams\n",
      "was not\n",
      "they have\n",
      "you are\n",
      "everybodys\n",
      "mcdonalds\n",
      "mothers\n",
      "it is\n",
      "do not\n",
      "that is\n",
      "\n",
      "\n",
      "that is\n",
      "cannot\n",
      "I am\n",
      "were not\n",
      "I have\n",
      "I am\n",
      "that is\n",
      "it is\n",
      "he is\n",
      "he is\n",
      "that is\n",
      "lits\n",
      "\n",
      "it is\n",
      "that is\n",
      "you are\n",
      "do not\n",
      "do not\n",
      "has not\n",
      "I am\n",
      "\n",
      "bitchs\n",
      "gets\n",
      "I have\n",
      "do not\n",
      "it is\n",
      "did not\n",
      "did not\n",
      "that is\n",
      "that is\n",
      "I will\n",
      "I am\n",
      "cannot\n",
      "have not\n",
      "do not\n",
      "that is\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "she is\n",
      "I have\n",
      "did not\n",
      "it is\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "it is\n",
      "cannot\n",
      "seasons\n",
      "\n",
      "do not\n",
      "you all\n",
      "that is\n",
      "I have\n",
      "will not\n",
      "it is\n",
      "I am\n",
      "acts\n",
      "\n",
      "do not\n",
      "are not\n",
      "we are\n",
      "I am\n",
      "does not\n",
      "will not\n",
      "you are\n",
      "she is\n",
      "kiarras\n",
      "you are\n",
      "fans\n",
      "weeks\n",
      "it is\n",
      "I am\n",
      "\n",
      "\n",
      "I will\n",
      "I am\n",
      "I am\n",
      "yup\n",
      "oneal\n",
      "I would\n",
      "that would\n",
      "that is\n",
      "womens\n",
      "womens\n",
      "could not\n",
      "I have\n",
      "do not\n",
      "it is\n",
      "it is\n",
      "I will\n",
      "do not\n",
      "do not\n",
      "have not\n",
      "it is\n",
      "it is\n",
      "you are\n",
      "that is\n",
      "have not\n",
      "I have\n",
      "he is\n",
      "we are\n",
      "do not\n",
      "it is\n",
      "I have\n",
      "I would\n",
      "I am\n",
      "I am\n",
      "should not\n",
      "\n",
      "sats\n",
      "we are\n",
      "cannot\n",
      "do not\n",
      "cannot\n",
      "I am\n",
      "did not\n",
      "is not\n",
      "it is\n",
      "you would\n",
      "could not\n",
      "I am\n",
      "it is\n",
      "is not\n",
      "that is\n",
      "does not\n",
      "is not\n",
      "will not\n",
      "cannot\n",
      "mens\n",
      "it is\n",
      "has not\n",
      "weeks\n",
      "could have\n",
      "would have\n",
      "I am\n",
      "I would\n",
      "what is\n",
      "where is\n",
      "I am\n",
      "have not\n",
      "did not\n",
      "cannot\n",
      "do not\n",
      "they are\n",
      "I would\n",
      "that is\n",
      "I am\n",
      "it is\n",
      "do not\n",
      "I am\n",
      "aldis\n",
      "\n",
      "\n",
      "I am\n",
      "it is\n",
      "heres\n",
      "do not\n",
      "I am\n",
      "scandinavias\n",
      "I am\n",
      "jos\n",
      "grammys\n",
      "heres\n",
      "did not\n",
      "it will\n",
      "you will\n",
      "you will\n",
      "he is\n",
      "it is\n",
      "\n",
      "cas\n",
      "does not\n",
      "I am\n",
      "he is\n",
      "it is\n",
      "it is\n",
      "cannot\n",
      "I am\n",
      "do not\n",
      "I am\n",
      "what is\n",
      "let us\n",
      "do not\n",
      "I am\n",
      "do not\n",
      "I am\n",
      "guysngals\n",
      "worlds\n",
      "worlds\n",
      "womens\n",
      "will not\n",
      "I am\n",
      "did not\n",
      "she is\n",
      "should not\n",
      "I have\n",
      "\n",
      "\n",
      "I am\n",
      "does not\n",
      "do not\n",
      "would not\n",
      "I am\n",
      "I am\n",
      "someones\n",
      "there is\n",
      "mamas\n",
      "papas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you all\n",
      "I am\n",
      "cannot\n",
      "weathers\n",
      "I am\n",
      "where is\n",
      "it is\n",
      "I am\n",
      "do not\n",
      "do not\n",
      "we will\n",
      "friends\n",
      "do not\n",
      "\n",
      "\n",
      "there is\n",
      "I will\n",
      "\n",
      "\n",
      "you have\n",
      "that is\n",
      "how is\n",
      "cannot\n",
      "it is\n",
      "have not\n",
      "I have\n",
      "directors\n",
      "you are\n",
      "directors\n",
      "you are\n",
      "I have\n",
      "writers\n",
      "\n",
      "I will\n",
      "do not\n",
      "it is\n",
      "do not\n",
      "I will\n",
      "kierkegaards\n",
      "do not\n",
      "fridays\n",
      "do not\n",
      "are not\n",
      "cannot\n",
      "I am\n",
      "I am\n",
      "what is\n",
      "let us\n",
      "do not\n",
      "I would\n",
      "cannot\n",
      "that would\n",
      "I will\n",
      "you will\n",
      "capn\n",
      "you have\n",
      "nites\n",
      "I would\n",
      "cannot\n",
      "I am\n",
      "ones\n",
      "he is\n",
      "it is\n",
      "I am\n",
      "gods\n",
      "who is\n",
      "I am\n",
      "it is\n",
      "it is\n",
      "do not\n",
      "you are\n",
      "I am\n",
      "I have\n",
      "\n",
      "that is\n",
      "I am\n",
      "I am\n",
      "could not\n",
      "do not\n",
      "\n",
      "\n",
      "it is\n",
      "it will\n",
      "is not\n",
      "I am\n",
      "there is\n",
      "it is\n",
      "I am\n",
      "I am\n",
      "does not\n",
      "\n",
      "\n",
      "juergensmeyers\n",
      "juergensmeyers\n",
      "winters\n",
      "winters\n",
      "do not\n",
      "it is\n",
      "that is\n",
      "it is\n",
      "was not\n",
      "will not\n",
      "it is\n",
      "is not\n",
      "cannot\n",
      "cannot\n",
      "I have\n",
      "that is\n",
      "I am\n",
      "we will\n",
      "\n",
      "\n",
      "you are\n",
      "that is\n",
      "I am\n",
      "it is\n",
      "should have\n",
      "that is\n",
      "cannot\n",
      "cannot\n",
      "cannot\n",
      "drivers\n",
      "drivers\n",
      "you are\n",
      "I am\n",
      "cannot\n",
      "I am\n",
      "it is\n",
      "friends\n",
      "did not\n",
      "will not\n",
      "\n",
      "that is\n",
      "is not\n",
      "I am\n",
      "I have\n",
      "do not\n",
      "what is\n",
      "I will\n",
      "cannot\n",
      "mommas\n",
      "do not\n",
      "I have\n",
      "do not\n",
      "you will\n",
      "did not\n",
      "it is\n",
      "exapples\n",
      "\n",
      "\n",
      "mens\n",
      "do not\n",
      "do not\n",
      "I am\n",
      "cannot\n",
      "how is\n",
      "kartinis\n",
      "do not\n",
      "I am\n",
      "you are\n",
      "names\n",
      "do not\n",
      "I will\n",
      "cannot\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "it is\n",
      "cannot\n",
      "I am\n",
      "there is\n",
      "that would\n",
      "mens\n",
      "\n",
      "I will\n",
      "southamptons\n",
      "it is\n",
      "\n",
      "cannot\n",
      "what is\n",
      "I will\n",
      "I have\n",
      "cannot\n",
      "I would\n",
      "was not\n",
      "I am\n",
      "have not\n",
      "they are\n",
      "I am\n",
      "I have\n",
      "I am\n",
      "I am\n",
      "will not\n",
      "will not\n",
      "that is\n",
      "I am\n",
      "it is\n",
      "I am\n",
      "we will\n",
      "have not\n",
      "I am\n",
      "\n",
      "you are\n",
      "do not\n",
      "I will\n",
      "do not\n",
      "is not\n",
      "bffs\n",
      "cannot\n",
      "do not\n",
      "cannot\n",
      "cannot\n",
      "it is\n",
      "do not\n",
      "must have\n",
      "did not\n",
      "I am\n",
      "\n",
      "I will\n",
      "I am\n",
      "I am\n",
      "cannot\n",
      "cannot\n",
      "I will\n",
      "cannot\n",
      "you are\n",
      "cannot\n",
      "I am\n",
      "\n",
      "\n",
      "\n",
      "did not\n",
      "we will\n",
      "she is\n",
      "do not\n",
      "do not\n",
      "that is\n",
      "did not\n",
      "it is\n",
      "there is\n",
      "I am\n",
      "I am\n",
      "\n",
      "\n",
      "you are\n",
      "cannot\n",
      "I will\n",
      "I am\n",
      "ur\n",
      "heres\n",
      "do not\n",
      "I will\n",
      "I will\n",
      "do not\n",
      "do not\n",
      "zayns\n",
      "it is\n",
      "yal\n",
      "I am\n",
      "do not\n",
      "I will\n",
      "womens\n",
      "womens\n",
      "we will\n",
      "who is\n",
      "do not\n",
      "it is\n",
      "\n",
      "\n",
      "I have\n",
      "I am\n",
      "\n",
      "that is\n",
      "that is\n",
      "cannot\n",
      "bobs\n",
      "did not\n",
      "what is\n",
      "what is\n",
      "do not\n",
      "will not\n",
      "do not\n",
      "you are\n",
      "they are\n",
      "do not\n",
      "ps\n",
      "I am\n",
      "we will\n",
      "whod\n",
      "I will\n",
      "do not\n",
      "would not\n",
      "nandos\n",
      "I will\n",
      "brandons\n",
      "you are\n",
      "we are\n",
      "I will\n",
      "that is\n",
      "dads\n",
      "I am\n",
      "I would\n",
      "lifes\n",
      "I would\n",
      "that is\n",
      "\n",
      "it is\n",
      "judds\n",
      "you are\n",
      "you are\n",
      "\n",
      "he is\n",
      "do not\n",
      "that is\n",
      "there is\n",
      "do not\n",
      "liams\n",
      "cannot\n",
      "\n",
      "\n",
      "do not\n",
      "it is\n",
      "I am\n",
      "I am\n",
      "iam\n",
      "\n",
      "theirs\n",
      "that is\n",
      "I am\n",
      "I am\n",
      "am not\n",
      "do not\n",
      "I have\n",
      "you are\n",
      "you are\n",
      "you have\n",
      "I have\n",
      "you are\n",
      "you have\n",
      "I have\n",
      "that is\n",
      "I have\n",
      "do not\n",
      "does not\n",
      "I am\n",
      "I am\n",
      "\n",
      "could not\n",
      "\n",
      "\n",
      "I will\n",
      "I will\n",
      "it is\n",
      "frommers\n",
      "frommers\n",
      "do not\n",
      "\n",
      "how did\n",
      "gods\n",
      "you are\n",
      "do not\n",
      "icons\n",
      "there is\n",
      "it is\n",
      "that is\n",
      "do not\n",
      "it is\n",
      "you will\n",
      "I will\n",
      "I am\n",
      "I am\n",
      "I am\n",
      "\n",
      "\n",
      "do not\n",
      "\n",
      "cannot\n",
      "I am\n",
      "am not\n",
      "do not\n",
      "is not\n",
      "do not\n",
      "it is\n",
      "it will\n",
      "I have\n",
      "are not\n",
      "defonot\n",
      "you are\n",
      "I will\n",
      "did not\n",
      "I will\n",
      "I will\n",
      "I have\n",
      "you are\n",
      "do not\n",
      "I will\n",
      "it is\n",
      "cannot\n",
      "have not\n",
      "should have\n",
      "you all\n",
      "I am\n",
      "she is\n",
      "it\n",
      "he is\n",
      "lions\n",
      "fans\n",
      "he is\n",
      "I am\n",
      "\n",
      "will not\n",
      "was not\n",
      "that is\n",
      "\n",
      "benders\n",
      "will not\n",
      "womens\n",
      "womens\n",
      "do not\n",
      "I am\n",
      "cannot\n",
      "I am\n",
      "would not\n",
      "I am\n",
      "do not\n",
      "I have\n",
      "I am\n",
      "\n",
      "I am\n",
      "do not\n",
      "I am\n",
      "oteris\n",
      "does not\n",
      "it is\n",
      "bichis\n",
      "you are\n",
      "are not\n",
      "I am\n",
      "lous\n",
      "I am\n",
      "I am\n",
      "cannot\n",
      "who is\n",
      "I am\n",
      "I am\n",
      "do not\n",
      "it is\n",
      "that is\n",
      "I am\n",
      "did not\n",
      "do not\n",
      "it is\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "do not\n",
      "\n",
      "heres\n",
      "cannot\n",
      "\n",
      "he shall\n",
      "I am\n",
      "did not\n",
      "I am\n",
      "I have\n",
      "do not\n",
      "let us\n",
      "let us\n",
      "should have\n",
      "they are\n",
      "taxpayers\n",
      "it is\n",
      "would not\n",
      "that is\n",
      "do not\n",
      "cannot\n",
      "is not\n",
      "do not\n",
      "I will\n",
      "it is\n",
      "cannot\n",
      "I am\n",
      "I am\n",
      "did not\n",
      "I am\n",
      "I am\n",
      "you will\n",
      "I am\n",
      "bostons\n",
      "I am\n",
      "il\n",
      "I am\n",
      "it is\n",
      "cannot\n",
      "I will\n",
      "is not\n",
      "I am\n",
      "do not\n",
      "you are\n",
      "you all\n",
      "do not\n",
      "that is\n",
      "it is\n",
      "I am\n",
      "cannot\n",
      "greysons\n",
      "todays\n",
      "todays\n",
      "I am\n",
      "that is\n",
      "I am\n",
      "mozarts\n",
      "schs\n",
      "it is\n",
      "I am\n",
      "I will\n",
      "they are\n",
      "do not\n",
      "that is\n",
      "I am\n",
      "let us\n",
      "let us\n",
      "that is\n",
      "what is\n",
      "I have\n",
      "you are\n",
      "she will\n",
      "I am\n",
      "I am\n",
      "that is\n",
      "what is\n",
      "do not\n",
      "I am\n",
      "I am\n",
      "does not\n",
      "that is\n",
      "could not\n",
      "churchs\n",
      "was not\n",
      "there is\n",
      "let us\n",
      "have not\n",
      "I will\n",
      "was not\n",
      "I will\n",
      "do not\n",
      "I am\n",
      "I have\n",
      "do not\n",
      "I am\n",
      "cannot\n",
      "that is\n",
      "it is\n",
      "will not\n",
      "I will\n",
      "I am\n",
      "I am\n",
      "everythings\n",
      "have not\n",
      "I have\n",
      "you are\n",
      "sturgess\n",
      "she will\n",
      "I am\n",
      "I am\n",
      "did not\n",
      "that is\n",
      "scarlets\n",
      "lifes\n",
      "I am\n",
      "do not\n",
      "\n",
      "americas\n",
      "\n",
      "americas\n",
      "\n",
      "americas\n",
      "do not\n",
      "are not\n",
      "I am\n",
      "do not\n",
      "I am\n",
      "we will\n",
      "we will\n",
      "it is\n",
      "she will\n",
      "you will\n",
      "that is\n",
      "I will\n",
      "you are\n",
      "it is\n",
      "I am\n",
      "I am\n",
      "\n",
      "\n",
      "do not\n",
      "it is\n",
      "you are\n",
      "did not\n",
      "sihmas\n",
      "there is\n",
      "cannot\n",
      "I will\n",
      "agencys\n",
      "do not\n",
      "do not\n",
      "I am\n",
      "I will\n",
      "I am\n",
      "flybosss\n",
      "baybees\n",
      "I am\n",
      "I have\n",
      "someones\n",
      "kaegis\n",
      "cannot\n",
      "ps\n",
      "let us\n",
      "ys\n",
      "I would\n",
      "do not\n",
      "speedys\n",
      "I would\n",
      "you have\n",
      "did not\n",
      "cannot\n",
      "you have\n",
      "do not\n",
      "have not\n",
      "cannot\n",
      "cannot\n",
      "I have\n",
      "I am\n",
      "who is\n",
      "I am\n",
      "they have\n",
      "that is\n",
      "womens\n",
      "womens\n",
      "\n",
      "\n",
      "you are\n",
      "I will\n",
      "do not\n",
      "boys\n",
      "it is\n",
      "ull\n",
      "it is\n",
      "I will\n",
      "it is\n",
      "I am\n",
      "it is\n",
      "I am\n",
      "it is\n",
      "I am\n",
      "it is\n",
      "I am\n",
      "it is\n",
      "I am\n",
      "it is\n",
      "I am\n",
      "it is\n",
      "I am\n",
      "it is\n",
      "I am\n",
      "it is\n",
      "I am\n",
      "I will\n",
      "I am\n",
      "she is\n",
      "I will\n",
      "I am\n",
      "\n",
      "that is\n",
      "did not\n",
      "have not\n",
      "you have\n",
      "they are\n",
      "I have\n",
      "I have\n",
      "that is\n",
      "I will\n",
      "I will\n",
      "that is\n",
      "is not\n",
      "I am\n",
      "I have\n",
      "that is\n",
      "what is\n",
      "I am\n",
      "they are\n",
      "friendlys\n",
      "that is\n",
      "I am\n",
      "I am\n",
      "you have\n",
      "\n",
      "I am\n",
      "do not\n",
      "it is\n",
      "\n",
      "do not\n",
      "it is\n",
      "I will\n",
      "do not\n",
      "I will\n",
      "does not\n",
      "it will\n",
      "I will\n",
      "cannot\n",
      "\n",
      "do not\n",
      "\n",
      "you have\n",
      "do not\n",
      "let us\n",
      "do not\n",
      "do not\n",
      "did not\n",
      "serpents\n",
      "we are\n",
      "I am\n",
      "cannot\n",
      "do not\n",
      "I am\n",
      "cannot\n",
      "peoples\n",
      "it is\n",
      "I am\n",
      "I am\n",
      "I would\n",
      "I am\n",
      "I am\n",
      "it is\n",
      "it is\n",
      "do not\n",
      "cannot\n",
      "I am\n",
      "I am\n",
      "cannot\n",
      "do not\n",
      "it is\n",
      "you are\n",
      "cannot\n",
      "that is\n",
      "cats\n",
      "it is\n",
      "I am\n",
      "should not\n",
      "I am\n",
      "that is\n",
      "what is\n",
      "do not\n",
      "let us\n",
      "did not\n",
      "it is\n",
      "I am\n",
      "I am\n",
      "cheddars\n",
      "I will\n",
      "I am\n",
      "wahlbergs\n",
      "do not\n",
      "you are\n",
      "it is\n",
      "I am\n",
      "I am\n",
      "what is\n",
      "todays\n",
      "do not\n",
      "tonights\n",
      "did not\n",
      "was not\n",
      "lovecrafts\n",
      "I am\n",
      "that is\n",
      "do not\n",
      "\n",
      "\n",
      "do not\n",
      "was not\n",
      "cores\n",
      "there is\n",
      "\n",
      "I am\n",
      "she is\n",
      "I am\n",
      "I am\n",
      "he is\n",
      "would not\n",
      "have not\n",
      "I am\n",
      "I am\n",
      "it is\n",
      "I am\n",
      "you are\n",
      "do not\n",
      "it is\n",
      "I am\n",
      "do not\n",
      "they are\n",
      "let us\n",
      "#yourewelcome\n",
      "they are\n",
      "it is\n",
      "I will\n",
      "ure\n",
      "citys\n",
      "he is\n",
      "I am\n",
      "I am\n",
      "do not\n",
      "I have\n",
      "I will\n",
      "you are\n",
      "murphys\n",
      "I am\n",
      "I would\n",
      "you are\n",
      "I am\n",
      "I will\n",
      "you will\n",
      "does not\n",
      "I am\n",
      "cannot\n",
      "it is\n",
      "that is\n",
      "does not\n",
      "does not\n",
      "you are\n",
      "I am\n",
      "do not\n",
      "scholls\n",
      "mens\n",
      "scholls\n",
      "do not\n",
      "do not\n",
      "I am\n",
      "I am\n",
      "bones\n",
      "weeklys\n",
      "do not\n",
      "I am\n",
      "will not\n",
      "would not\n",
      "I am\n",
      "I have\n",
      "do not\n",
      "I am\n",
      "is not\n",
      "are not\n",
      "\n",
      "that is\n",
      "I have\n",
      "it is\n",
      "it is\n",
      "do not\n",
      "\n",
      "we are\n",
      "you will\n",
      "you will\n",
      "did not\n",
      "we have\n",
      "cannot\n",
      "I am\n",
      "I am\n",
      "it is\n",
      "do not\n",
      "cannot\n",
      "I am\n",
      "everythings\n",
      "will not\n",
      "that is\n",
      "am not\n",
      "\n",
      "\n",
      "\n",
      "womens\n",
      "womens\n",
      "I am\n",
      "I am\n",
      "cannot\n",
      "that is\n",
      "\n",
      "should not\n",
      "have not\n",
      "do not\n",
      "I will\n",
      "did not\n",
      "do not\n",
      "it is\n",
      "\n",
      "it is\n",
      "it is\n",
      "you would\n",
      "you all\n",
      "do not\n",
      "do not\n",
      "cannot\n",
      "I am\n",
      "it is\n",
      "do not\n",
      "I would\n",
      "do not\n",
      "that is\n",
      "I have\n",
      "I am\n",
      "jeriss\n",
      "were not\n",
      "I am\n",
      "do not\n",
      "I am\n",
      "she is\n",
      "is not\n",
      "I will\n",
      "shamans\n",
      "do not\n",
      "I am\n",
      "does not\n",
      "cannot\n",
      "it is\n",
      "it is\n",
      "it is\n",
      "it is\n",
      "it is\n",
      "it is\n",
      "do not\n",
      "what is\n",
      "scotlands\n",
      "scotlands\n",
      "that is\n",
      "we will\n",
      "I am\n",
      "idlis\n",
      "who is\n",
      "that is\n",
      "you will\n",
      "I will\n",
      "pennys\n",
      "\n",
      "\n",
      "rajs\n",
      "I am\n",
      "I am\n",
      "would not\n",
      "will not\n",
      "do not\n",
      "do not\n",
      "do not\n",
      "she is\n",
      "do not\n",
      "do not\n",
      "what is\n",
      "cannot\n",
      "he is\n",
      "are not\n",
      "he is\n",
      "I am\n",
      "did not\n",
      "I am\n",
      "cannot\n",
      "that is\n",
      "I am\n",
      "cannot\n",
      "bahasas\n",
      "I am\n",
      "it is\n",
      "you are\n",
      "it is\n",
      "it is\n",
      "I am\n",
      "cannot\n",
      "that is\n",
      "you are\n",
      "that is\n",
      "you are\n",
      "that is\n",
      "cannot\n",
      "I am\n",
      "you are\n",
      "it is\n"
     ]
    }
   ],
   "source": [
    "new_ll = resolve_punctuation(ll, contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_ll = find_repeat_indices(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ll = correct_repeat_word(ll, rep_ll, onomatopees, twitter_abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ll = resolve_correction_into_words(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ll = remove_stopwords_from_dataframe(ll, stopword_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ll = stem_and_lemmatize_data(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-174-cf9633d6b678>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwrite_fname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"stemmed_and_lemmatized_pos_train.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwrite_new_test_file_from_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_fname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-157-3fc6ea5e9214>\u001b[0m in \u001b[0;36mwrite_new_test_file_from_df\u001b[1;34m(data, filename)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mstill_in_same_tweet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# write each word in correction.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                     \u001b[0mfh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# write each token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                     \u001b[0mfh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "write_fname = \"stemmed_and_lemmatized_pos_train.txt\"\n",
    "write_new_test_file_from_df(ll, write_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = compute_tf_idf_analysis(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user          0.612182\n",
       "exclmt        0.396509\n",
       "you           0.303175\n",
       "to            0.225644\n",
       "the           0.197906\n",
       "                ...   \n",
       "mufcfamily    0.000007\n",
       "muff          0.000007\n",
       "mugelo        0.000007\n",
       "mugger        0.000007\n",
       "letchu        0.000007\n",
       "Length: 46455, dtype: float64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## query = '\\'2\\''\n",
    "ll.query('filewords == {}'.format(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = '\\'haha\\''\n",
    "df.query(\"filewords == {}\".format(query))\n",
    "#ap_ind =ll.index[ll.filewords.str.contains('\\'')].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filewords</th>\n",
       "      <th>position</th>\n",
       "      <th>correction</th>\n",
       "      <th>tokens</th>\n",
       "      <th>alternative correction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>can't</td>\n",
       "      <td>(1, 10)</td>\n",
       "      <td>[can't]</td>\n",
       "      <td>[]</td>\n",
       "      <td>can't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>ma'am</td>\n",
       "      <td>(3, 2)</td>\n",
       "      <td>[ma'am]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ma'am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>don't</td>\n",
       "      <td>(5, 16)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>don't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>i'm</td>\n",
       "      <td>(7, 11)</td>\n",
       "      <td>[i'm]</td>\n",
       "      <td>[]</td>\n",
       "      <td>i'm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>y'all</td>\n",
       "      <td>(10, 3)</td>\n",
       "      <td>[y'al]</td>\n",
       "      <td>[rrrpp]</td>\n",
       "      <td>y'all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138809</th>\n",
       "      <td>that's</td>\n",
       "      <td>(9992, 1)</td>\n",
       "      <td>[that's]</td>\n",
       "      <td>[]</td>\n",
       "      <td>that's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138813</th>\n",
       "      <td>can't</td>\n",
       "      <td>(9992, 5)</td>\n",
       "      <td>[can't]</td>\n",
       "      <td>[]</td>\n",
       "      <td>can't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138825</th>\n",
       "      <td>i'm</td>\n",
       "      <td>(9994, 1)</td>\n",
       "      <td>[i'm]</td>\n",
       "      <td>[]</td>\n",
       "      <td>i'm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138827</th>\n",
       "      <td>you're</td>\n",
       "      <td>(9994, 3)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>you're</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138845</th>\n",
       "      <td>it's</td>\n",
       "      <td>(9996, 2)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>it's</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3673 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       filewords   position correction   tokens alternative correction\n",
       "29         can't    (1, 10)    [can't]       []                  can't\n",
       "57         ma'am     (3, 2)    [ma'am]       []                  ma'am\n",
       "100        don't    (5, 16)         []       []                  don't\n",
       "127          i'm    (7, 11)      [i'm]       []                    i'm\n",
       "160        y'all    (10, 3)     [y'al]  [rrrpp]                  y'all\n",
       "...          ...        ...        ...      ...                    ...\n",
       "138809    that's  (9992, 1)   [that's]       []                 that's\n",
       "138813     can't  (9992, 5)    [can't]       []                  can't\n",
       "138825       i'm  (9994, 1)      [i'm]       []                    i'm\n",
       "138827    you're  (9994, 3)         []       []                 you're\n",
       "138845      it's  (9996, 2)         []       []                   it's\n",
       "\n",
       "[3673 rows x 5 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.loc[ap_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = test_df.iat[29, 0]\n",
    "test_df.iat[29, 2] = contractions[q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_idx = ll.filewords.str.contains('\\'')\n",
    "omg_idx = ll.index[ll.filewords == 'omg'].tolist()\n",
    "\n",
    "llex = ll.loc[49]\n",
    "llap=ll[ap_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1717,\n",
       " 3677,\n",
       " 3892,\n",
       " 8966,\n",
       " 9199,\n",
       " 11229,\n",
       " 15056,\n",
       " 15228,\n",
       " 28951,\n",
       " 30438,\n",
       " 32583,\n",
       " 32604,\n",
       " 36830,\n",
       " 42426,\n",
       " 42488,\n",
       " 45050,\n",
       " 46701,\n",
       " 49153,\n",
       " 51714,\n",
       " 51914,\n",
       " 58391,\n",
       " 63133,\n",
       " 77288,\n",
       " 82132,\n",
       " 92042,\n",
       " 93759,\n",
       " 94304,\n",
       " 102494,\n",
       " 102622,\n",
       " 103705,\n",
       " 104778,\n",
       " 109995,\n",
       " 110597,\n",
       " 133444,\n",
       " 134526,\n",
       " 135407,\n",
       " 135659,\n",
       " 140308,\n",
       " 141264,\n",
       " 142579,\n",
       " 144063,\n",
       " 146980,\n",
       " 148028,\n",
       " 148104,\n",
       " 148310,\n",
       " 148862,\n",
       " 150184,\n",
       " 154509,\n",
       " 154798,\n",
       " 154799,\n",
       " 154800,\n",
       " 155631,\n",
       " 157304,\n",
       " 158598,\n",
       " 160278,\n",
       " 160417,\n",
       " 160626,\n",
       " 162998,\n",
       " 164360,\n",
       " 166321,\n",
       " 167443,\n",
       " 168615,\n",
       " 170788,\n",
       " 172561,\n",
       " 175648,\n",
       " 180736,\n",
       " 181469,\n",
       " 181471,\n",
       " 184207,\n",
       " 189621,\n",
       " 190590,\n",
       " 196376,\n",
       " 199203,\n",
       " 202886,\n",
       " 204177,\n",
       " 209616,\n",
       " 209845,\n",
       " 212615,\n",
       " 213969,\n",
       " 217970,\n",
       " 219923,\n",
       " 220829,\n",
       " 224281,\n",
       " 233502,\n",
       " 234029,\n",
       " 237166,\n",
       " 238932,\n",
       " 244248,\n",
       " 247254,\n",
       " 247346,\n",
       " 249300,\n",
       " 249493,\n",
       " 251993,\n",
       " 255209,\n",
       " 256900,\n",
       " 258613,\n",
       " 265313,\n",
       " 265500,\n",
       " 267780,\n",
       " 269401,\n",
       " 270693,\n",
       " 273809,\n",
       " 276696,\n",
       " 278117,\n",
       " 278717,\n",
       " 279835,\n",
       " 280304,\n",
       " 282672,\n",
       " 283392,\n",
       " 284323,\n",
       " 288780,\n",
       " 289969,\n",
       " 291565,\n",
       " 297934,\n",
       " 297935,\n",
       " 298089,\n",
       " 304045,\n",
       " 304046,\n",
       " 304047,\n",
       " 304956,\n",
       " 307450,\n",
       " 308280,\n",
       " 308662,\n",
       " 310907,\n",
       " 311309,\n",
       " 315422,\n",
       " 320430,\n",
       " 321808,\n",
       " 324232,\n",
       " 326079,\n",
       " 328459,\n",
       " 328782,\n",
       " 329138,\n",
       " 330389,\n",
       " 337275,\n",
       " 338090,\n",
       " 338868,\n",
       " 341906,\n",
       " 345776,\n",
       " 356657,\n",
       " 356888,\n",
       " 357004,\n",
       " 360066,\n",
       " 366329,\n",
       " 367313,\n",
       " 367316,\n",
       " 372971,\n",
       " 381301,\n",
       " 381338,\n",
       " 382144,\n",
       " 382976,\n",
       " 383458,\n",
       " 385169,\n",
       " 388956,\n",
       " 390805,\n",
       " 397275,\n",
       " 400110,\n",
       " 400280,\n",
       " 400665,\n",
       " 400966,\n",
       " 401512,\n",
       " 403865,\n",
       " 405182,\n",
       " 406934,\n",
       " 408158,\n",
       " 410401,\n",
       " 410920,\n",
       " 420081,\n",
       " 420889,\n",
       " 420987,\n",
       " 423679,\n",
       " 424549,\n",
       " 425532,\n",
       " 430469,\n",
       " 431509,\n",
       " 436394,\n",
       " 438403,\n",
       " 438452,\n",
       " 443033,\n",
       " 446328,\n",
       " 447719,\n",
       " 449112,\n",
       " 451127,\n",
       " 458317,\n",
       " 459201,\n",
       " 464349,\n",
       " 467574,\n",
       " 468144,\n",
       " 468397,\n",
       " 468832,\n",
       " 469518,\n",
       " 470613,\n",
       " 477631,\n",
       " 477639,\n",
       " 479731,\n",
       " 483622,\n",
       " 487875,\n",
       " 488079,\n",
       " 496035,\n",
       " 496351,\n",
       " 497299,\n",
       " 497590,\n",
       " 500061,\n",
       " 501051,\n",
       " 502815,\n",
       " 503306,\n",
       " 503908,\n",
       " 504235,\n",
       " 505759,\n",
       " 508149,\n",
       " 511449,\n",
       " 516230,\n",
       " 520218,\n",
       " 522701,\n",
       " 522862,\n",
       " 527217,\n",
       " 528375,\n",
       " 531084,\n",
       " 536190,\n",
       " 537463,\n",
       " 544357,\n",
       " 547639,\n",
       " 548189,\n",
       " 552258,\n",
       " 559806,\n",
       " 562612,\n",
       " 564852,\n",
       " 565717,\n",
       " 566968,\n",
       " 569028,\n",
       " 569381,\n",
       " 569756,\n",
       " 570367,\n",
       " 575373,\n",
       " 584518,\n",
       " 585411,\n",
       " 588201,\n",
       " 594167,\n",
       " 594685,\n",
       " 596493,\n",
       " 597586,\n",
       " 597763,\n",
       " 601818,\n",
       " 602366,\n",
       " 602499,\n",
       " 603713,\n",
       " 603716,\n",
       " 610644,\n",
       " 611073,\n",
       " 611945,\n",
       " 613183,\n",
       " 613184,\n",
       " 614112,\n",
       " 623040,\n",
       " 624525,\n",
       " 630339,\n",
       " 635931,\n",
       " 639904,\n",
       " 640511,\n",
       " 641376,\n",
       " 645989,\n",
       " 651093,\n",
       " 653057,\n",
       " 653639,\n",
       " 654388,\n",
       " 654398,\n",
       " 655480,\n",
       " 658996,\n",
       " 659649,\n",
       " 661207,\n",
       " 661232,\n",
       " 662182,\n",
       " 663892,\n",
       " 663947,\n",
       " 665886,\n",
       " 667170,\n",
       " 671239,\n",
       " 671461,\n",
       " 672313,\n",
       " 672968,\n",
       " 674406,\n",
       " 675209,\n",
       " 676593,\n",
       " 677094,\n",
       " 677425,\n",
       " 680409,\n",
       " 680461,\n",
       " 684837,\n",
       " 689562,\n",
       " 692459,\n",
       " 694208,\n",
       " 695038,\n",
       " 695673,\n",
       " 697943,\n",
       " 700438,\n",
       " 701548,\n",
       " 702250,\n",
       " 714383,\n",
       " 714507,\n",
       " 716046,\n",
       " 718030,\n",
       " 718192,\n",
       " 718301,\n",
       " 720816,\n",
       " 721029,\n",
       " 721910,\n",
       " 727377,\n",
       " 737089,\n",
       " 738210,\n",
       " 738461,\n",
       " 739870,\n",
       " 743713,\n",
       " 743734,\n",
       " 746801,\n",
       " 746806,\n",
       " 754908,\n",
       " 756308,\n",
       " 757539,\n",
       " 760999,\n",
       " 762240,\n",
       " 762748,\n",
       " 765089,\n",
       " 768862,\n",
       " 769656,\n",
       " 777768,\n",
       " 778353,\n",
       " 778418,\n",
       " 779433,\n",
       " 785489,\n",
       " 791438,\n",
       " 791974,\n",
       " 793595,\n",
       " 794872,\n",
       " 799804,\n",
       " 800820,\n",
       " 803152,\n",
       " 804664,\n",
       " 804794,\n",
       " 805147,\n",
       " 808119,\n",
       " 809160,\n",
       " 810658,\n",
       " 820991,\n",
       " 821372,\n",
       " 823391,\n",
       " 828826,\n",
       " 831328,\n",
       " 832586,\n",
       " 832686,\n",
       " 832964,\n",
       " 834731,\n",
       " 837179,\n",
       " 840904,\n",
       " 841662,\n",
       " 845864,\n",
       " 848114,\n",
       " 851768,\n",
       " 853283,\n",
       " 853399,\n",
       " 853564,\n",
       " 857225,\n",
       " 864652,\n",
       " 865551,\n",
       " 866125,\n",
       " 872063,\n",
       " 873506,\n",
       " 882581,\n",
       " 886217,\n",
       " 888859,\n",
       " 891387,\n",
       " 902441,\n",
       " 905116,\n",
       " 905191,\n",
       " 905738,\n",
       " 909214,\n",
       " 911263,\n",
       " 912913,\n",
       " 918215,\n",
       " 919453,\n",
       " 921224,\n",
       " 922627,\n",
       " 927298,\n",
       " 928989,\n",
       " 929532,\n",
       " 931576,\n",
       " 931690,\n",
       " 931698,\n",
       " 937227,\n",
       " 940479,\n",
       " 941498,\n",
       " 942446,\n",
       " 945780,\n",
       " 949974,\n",
       " 950653,\n",
       " 952537,\n",
       " 954148,\n",
       " 955752,\n",
       " 958926,\n",
       " 967834,\n",
       " 967963,\n",
       " 974748,\n",
       " 976464,\n",
       " 983703,\n",
       " 985105,\n",
       " 987180,\n",
       " 988509,\n",
       " 989895,\n",
       " 994750,\n",
       " 997827,\n",
       " 999174,\n",
       " 1004017,\n",
       " 1006596,\n",
       " 1007845,\n",
       " 1008367,\n",
       " 1009866,\n",
       " 1010041,\n",
       " 1013215,\n",
       " 1015736,\n",
       " 1018200,\n",
       " 1018871,\n",
       " 1019313,\n",
       " 1020758,\n",
       " 1022277,\n",
       " 1023894,\n",
       " 1024111,\n",
       " 1024440,\n",
       " 1025223,\n",
       " 1032670,\n",
       " 1032726,\n",
       " 1041798,\n",
       " 1042720,\n",
       " 1050101,\n",
       " 1052577,\n",
       " 1055222,\n",
       " 1055301,\n",
       " 1056403,\n",
       " 1058285,\n",
       " 1059764,\n",
       " 1060680,\n",
       " 1061513,\n",
       " 1061514,\n",
       " 1061515,\n",
       " 1062460,\n",
       " 1063287,\n",
       " 1063483,\n",
       " 1073098,\n",
       " 1077674,\n",
       " 1077944,\n",
       " 1081249,\n",
       " 1081411,\n",
       " 1081512,\n",
       " 1082687,\n",
       " 1097117,\n",
       " 1099806,\n",
       " 1101091,\n",
       " 1103211,\n",
       " 1112195,\n",
       " 1115048,\n",
       " 1125031,\n",
       " 1126973,\n",
       " 1127798,\n",
       " 1128700,\n",
       " 1128906,\n",
       " 1132973,\n",
       " 1133655,\n",
       " 1136549,\n",
       " 1138313,\n",
       " 1141308,\n",
       " 1146875,\n",
       " 1150838,\n",
       " 1155499,\n",
       " 1155820,\n",
       " 1160755,\n",
       " 1162509,\n",
       " 1164927,\n",
       " 1167969,\n",
       " 1169079,\n",
       " 1174007,\n",
       " 1176337,\n",
       " 1180134,\n",
       " 1180295,\n",
       " 1182009,\n",
       " 1188473,\n",
       " 1190144,\n",
       " 1191411,\n",
       " 1191757,\n",
       " 1193525,\n",
       " 1193526,\n",
       " 1193527,\n",
       " 1193528,\n",
       " 1194467,\n",
       " 1195214,\n",
       " 1197981,\n",
       " 1199791,\n",
       " 1201337,\n",
       " 1201653,\n",
       " 1202530,\n",
       " 1207783,\n",
       " 1208475,\n",
       " 1211713,\n",
       " 1211918,\n",
       " 1212817,\n",
       " 1213389,\n",
       " 1214296,\n",
       " 1215102,\n",
       " 1220618,\n",
       " 1222896,\n",
       " 1227179,\n",
       " 1227188,\n",
       " 1229465,\n",
       " 1231816,\n",
       " 1233863,\n",
       " 1236724,\n",
       " 1238324,\n",
       " 1240269,\n",
       " 1253310,\n",
       " 1264119,\n",
       " 1268846,\n",
       " 1270968,\n",
       " 1274524,\n",
       " 1283957,\n",
       " 1284481,\n",
       " 1289410,\n",
       " 1294343]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omg_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = [49, 53]\n",
    "for i in e:\n",
    "    ll.iat[i, 2] = \"!!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filewords                      !\n",
       "position                  (2, 7)\n",
       "correction                    !!\n",
       "tokens                         0\n",
       "alternative correction         !\n",
       "Name: 49, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.loc[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc = pd.DataFrame({\"filewords\":\"!!\",\"position\": [(0, 0)], \"correction\" : \"!!\",\"tokens\": 0,\"alternative correction\": \"!!\"})\n",
    "nllex = pd.concat([llex, conc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filewords</th>\n",
       "      <th>position</th>\n",
       "      <th>correction</th>\n",
       "      <th>tokens</th>\n",
       "      <th>alternative correction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>!</td>\n",
       "      <td>(2, 7)</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>!</td>\n",
       "      <td>(2, 11)</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>!</td>\n",
       "      <td>(3, 15)</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>!</td>\n",
       "      <td>(4, 10)</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>!</td>\n",
       "      <td>(4, 11)</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298194</th>\n",
       "      <td>!</td>\n",
       "      <td>(99988, 10)</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298213</th>\n",
       "      <td>!</td>\n",
       "      <td>(99989, 5)</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298335</th>\n",
       "      <td>!</td>\n",
       "      <td>(99998, 6)</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298341</th>\n",
       "      <td>!</td>\n",
       "      <td>(99998, 12)</td>\n",
       "      <td>!</td>\n",
       "      <td>0</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!!</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>!!</td>\n",
       "      <td>0</td>\n",
       "      <td>!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52892 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        filewords     position correction  tokens alternative correction\n",
       "49              !       (2, 7)          !       0                      !\n",
       "53              !      (2, 11)          !       0                      !\n",
       "70              !      (3, 15)          !       0                      !\n",
       "81              !      (4, 10)          !       0                      !\n",
       "82              !      (4, 11)          !       0                      !\n",
       "...           ...          ...        ...     ...                    ...\n",
       "1298194         !  (99988, 10)          !       0                      !\n",
       "1298213         !   (99989, 5)          !       0                      !\n",
       "1298335         !   (99998, 6)          !       0                      !\n",
       "1298341         !  (99998, 12)          !       0                      !\n",
       "0              !!       (0, 0)         !!       0                     !!\n",
       "\n",
       "[52892 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nllex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filewords</th>\n",
       "      <th>position</th>\n",
       "      <th>correction</th>\n",
       "      <th>tokens</th>\n",
       "      <th>alternative correction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>won't</td>\n",
       "      <td>(1, 7)</td>\n",
       "      <td>won't</td>\n",
       "      <td>0</td>\n",
       "      <td>won't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>don't</td>\n",
       "      <td>(3, 6)</td>\n",
       "      <td>don't</td>\n",
       "      <td>0</td>\n",
       "      <td>don't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>hardee's</td>\n",
       "      <td>(7, 8)</td>\n",
       "      <td>hardee's</td>\n",
       "      <td>0</td>\n",
       "      <td>hardee's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>i'll</td>\n",
       "      <td>(8, 3)</td>\n",
       "      <td>i'll</td>\n",
       "      <td>0</td>\n",
       "      <td>i'll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>wasn't</td>\n",
       "      <td>(20, 3)</td>\n",
       "      <td>wasn't</td>\n",
       "      <td>0</td>\n",
       "      <td>wasn't</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298241</th>\n",
       "      <td>'</td>\n",
       "      <td>(99990, 26)</td>\n",
       "      <td>'</td>\n",
       "      <td>0</td>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298243</th>\n",
       "      <td>'</td>\n",
       "      <td>(99990, 28)</td>\n",
       "      <td>'</td>\n",
       "      <td>0</td>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298288</th>\n",
       "      <td>what's</td>\n",
       "      <td>(99995, 3)</td>\n",
       "      <td>what's</td>\n",
       "      <td>0</td>\n",
       "      <td>what's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298318</th>\n",
       "      <td>i'm</td>\n",
       "      <td>(99997, 9)</td>\n",
       "      <td>i'm</td>\n",
       "      <td>0</td>\n",
       "      <td>i'm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298330</th>\n",
       "      <td>i'm</td>\n",
       "      <td>(99998, 1)</td>\n",
       "      <td>i'm</td>\n",
       "      <td>0</td>\n",
       "      <td>i'm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37742 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        filewords     position correction  tokens alternative correction\n",
       "31          won't       (1, 7)      won't       0                  won't\n",
       "61          don't       (3, 6)      don't       0                  don't\n",
       "124      hardee's       (7, 8)   hardee's       0               hardee's\n",
       "133          i'll       (8, 3)       i'll       0                   i'll\n",
       "269        wasn't      (20, 3)     wasn't       0                 wasn't\n",
       "...           ...          ...        ...     ...                    ...\n",
       "1298241         '  (99990, 26)          '       0                      '\n",
       "1298243         '  (99990, 28)          '       0                      '\n",
       "1298288    what's   (99995, 3)     what's       0                 what's\n",
       "1298318       i'm   (99997, 9)        i'm       0                    i'm\n",
       "1298330       i'm   (99998, 1)        i'm       0                    i'm\n",
       "\n",
       "[37742 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = {\"col1\":['a', 'b', 'c'], \"col2\": [1, 2, 3]}\n",
    "di2 = {\"letters\":['a', 'b', 'c'], \"numbers\": [1, 2, 3]}\n",
    "test = {'filewords': ['<user>', 'i', 'dunno', 'justin', 'read', 'my'], 'position': [(0, 0), (0, 1), (0, 2), (0, 3), (0, 4), (0, 5)], 'correction': ['<user>', 'i', 'dunno', 'justin', 'read', 'my'], 'tokens': [0, 0, 0, 0, 0, 0], 'alternative correction': ['<user>', 'i', 'dunno', 'justin', 'read', 'my']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filewords</th>\n",
       "      <th>position</th>\n",
       "      <th>correction</th>\n",
       "      <th>tokens</th>\n",
       "      <th>alternative correction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;user&gt;</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>&lt;user&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;user&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>i</td>\n",
       "      <td>0</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dunno</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>dunno</td>\n",
       "      <td>0</td>\n",
       "      <td>dunno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>justin</td>\n",
       "      <td>(0, 3)</td>\n",
       "      <td>justin</td>\n",
       "      <td>0</td>\n",
       "      <td>justin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>read</td>\n",
       "      <td>(0, 4)</td>\n",
       "      <td>read</td>\n",
       "      <td>0</td>\n",
       "      <td>read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>my</td>\n",
       "      <td>(0, 5)</td>\n",
       "      <td>my</td>\n",
       "      <td>0</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filewords position correction  tokens alternative correction\n",
       "0    <user>   (0, 0)     <user>       0                 <user>\n",
       "1         i   (0, 1)          i       0                      i\n",
       "2     dunno   (0, 2)      dunno       0                  dunno\n",
       "3    justin   (0, 3)     justin       0                 justin\n",
       "4      read   (0, 4)       read       0                   read\n",
       "5        my   (0, 5)         my       0                     my"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_spelling_mistakes(filename, dictionary):\n",
    "    \"\"\" This function reads a file and detects words that do not exist in the english dictionary.\n",
    "        Dictionary is a list of words e.g take them from nltk.\n",
    "    \"\"\"\n",
    "    # Create a set of words\n",
    "    file_words =  set(open(filename).read().split())\n",
    "    file_non_existing_words = file_words.difference(dictionary)\n",
    "    # The problem with this is that is doesn't take\n",
    "    return file_words, file_non_existing_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_words, alt_non_words = find_spelling_mistakes(fname, alt_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "awords = list(alt_words)\n",
    "anwords = list(alt_non_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lwords = list(words)\n",
    "lnwords = list(non_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44889"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lnwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40705"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['funnyyy', 'capricciosa', 'maho', '#leggo', 'songs', '#cmgrchat', 'leaders', 'cancels', '#modernfamily', 'futures', 'cubans', 'spears', '#jaythan', 'condo', 'gv', 'westie', \"#capricorn's\", 'cabell', \"committee's\", 'waylon', '#wpc12', 'freee', '#tulisa', '#needthesummerbody', 'iyou', 'slices', 'brings', '#thatsmyboys', '#schitzogiveaway', '1kings', 'sex-c', 'allows', '#lifestyle', 'gooodnigghttt', '#indiawants1d', 'watsons', 'thereee', 'ba3yn', 'ashrald', 'snacks', 'khen', 'montages', 'solano', 'blujays', \"esa's\", '#todayskicks', 'nanaman', '20.32', 'riddim', 'penalties', '#notashamed', 'puffs', 'all-access', 'wonderbra', 'threequel', '#pens', '#onlyjokingiam', 'zai', 'donnneee', 'offerings', 'wooaah', 'bakein', '#ss4ina', 'belgian', 'fvck', 'areolas', 'mathai', 'fixes', '#cfceasypickings', 'scrubs', '2minutes', 'blackpool', 'lilbeedoee', '1200', '#delenakiss', \"levi's\", '#nofilter', 'yilina', 'toes', '#xfactormemories', '#youlikedit', 'clownn', 'ipar', 'audley', 'referring', '#sendinggoodthots', 'fryday', 'mimis', '#retweetif', 'rmmber', 'ayy', 'jollibee', '#gonnasmashit', 'panabaker', 'snah', '#bmegplanet', 'arguin', '#nighttimerunning', \"hayden's\", 'sharpens']\n"
     ]
    }
   ],
   "source": [
    "len(lnwords)\n",
    "print(lnwords[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['funnyyy', 'capricciosa', 'maho', '#leggo', '#cmgrchat', 'cubans', '#modernfamily', '#jaythan', 'condo', 'gv', 'westie', \"#capricorn's\", 'cabell', \"committee's\", 'waylon', '#wpc12', 'freee', '#tulisa', '#needthesummerbody', 'iyou', '#thatsmyboys', '#schitzogiveaway', '1kings', 'sex-c', '#indiawants1d', '#lifestyle', 'gooodnigghttt', 'watsons', 'thereee', 'ba3yn', 'ashrald', 'khen', 'solano', 'blujays', \"esa's\", '#todayskicks', 'nanaman', '20.32', 'riddim', '#notashamed', 'wonderbra', 'all-access', 'threequel', '#pens', '#onlyjokingiam', 'zai', 'donnneee', 'wooaah', 'bakein', '#ss4ina', 'belgian', 'fvck', 'areolas', 'mathai', '#cfceasypickings', '2minutes', 'blackpool', 'lilbeedoee', '1200', '#delenakiss', \"levi's\", '#nofilter', 'yilina', '#xfactormemories', '#youlikedit', 'clownn', 'ipar', 'audley', '#sendinggoodthots', 'fryday', 'mimis', '#retweetif', 'rmmber', 'ayy', 'jollibee', '#gonnasmashit', 'panabaker', 'snah', '#bmegplanet', 'arguin', '#nighttimerunning', \"hayden's\", 'monsss', 'mayans', 'rivera', 'lmaaaoo', 'chiminology', 'ashlee', '#liamtobeatboxonnext1dalbum', 'llanas', 'potatopower', 'paro', 'kbsworldtv', '#raceintervalpowerups', 'dormia', '#musicvid', 'pepa', 'renee', 'cadillac', 'acorde']\n"
     ]
    }
   ],
   "source": [
    "print(anwords[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags_user_url(word_list):\n",
    "    \"\"\" Removes all hashtag, users and url words from a list of words. \"\"\"\n",
    "    pathological_characters = ['#', '<']\n",
    "    corrected_list = []\n",
    "    for word in word_list:\n",
    "        if word[0] in pathological_characters:\n",
    "            continue\n",
    "        else:\n",
    "            corrected_list.append(word)\n",
    "    return corrected_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = list(anwords)\n",
    "cw = remove_hashtags_user_url(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['funnyyy', 'capricciosa', 'maho', 'cubans', 'condo', 'gv', 'westie', 'cabell', \"committee's\", 'waylon', 'freee', 'iyou', '1kings', 'sex-c', 'gooodnigghttt', 'watsons', 'thereee', 'ba3yn', 'ashrald', 'khen', 'solano', 'blujays', \"esa's\", 'nanaman', '20.32', 'riddim', 'wonderbra', 'all-access', 'threequel', 'zai', 'donnneee', 'wooaah', 'bakein', 'belgian', 'fvck', 'areolas', 'mathai', '2minutes', 'blackpool', 'lilbeedoee', '1200', \"levi's\", 'yilina', 'clownn', 'ipar', 'audley', 'fryday', 'mimis', 'rmmber', 'ayy', 'jollibee', 'panabaker', 'snah', 'arguin', \"hayden's\", 'monsss', 'mayans', 'rivera', 'lmaaaoo', 'chiminology', 'ashlee', 'llanas', 'potatopower', 'paro', 'kbsworldtv', 'dormia', 'pepa', 'renee', 'cadillac', 'acorde', 'chermainea', 'eventho', 'andrewx', 'vcr', '763-548-4658', 'imy', 'shelia', 'bobdim', 'bentley', 'uglyyy', 'eatchother', 'keepp', \"l'immenso\", 'postitive', 'foof', 'girlsxx', 'selff', 'mena', 'blaaa', 'buuurrnss', 'twitteros', 'angello', 'merlefest', 'barder', 'liyah', '2o1o', 'wh', 'g10', 'recks', 'wthout']\n"
     ]
    }
   ],
   "source": [
    "print(cw[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = 'hello'\n",
    "h2 = 'helllo'\n",
    "spell = SpellChecker()\n",
    "len(spell.unknown([h1])) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33679"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<user>\n",
      "i\n",
      "dunno\n",
      "justin\n",
      "read\n",
      "my\n",
      "mention\n",
      "or\n",
      "not\n",
      "only\n"
     ]
    }
   ],
   "source": [
    "for i, w in enumerate(ll.filewords):\n",
    "    if i == 10: break\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strawberrybanana\n",
      "1\n",
      "diplock\n",
      "4\n",
      "cheeseburger\n",
      "1\n",
      "tyre\n",
      "8\n",
      "woooweee\n",
      "1\n",
      "jug\n",
      "16\n",
      "#skynews\n",
      "1\n",
      "#supportgroup\n",
      "1\n",
      "#hateonit\n",
      "1\n",
      "mom\n",
      "9\n",
      "marcello\n",
      "2\n",
      "thurs\n",
      "5\n",
      "blew\n",
      "6\n",
      "#weneedit\n",
      "1\n",
      "trainer\n",
      "4\n",
      "birthday\n",
      "1\n",
      "if\n",
      "28\n",
      "eye-opening\n",
      "1\n",
      "assumes\n",
      "2\n",
      "carole\n",
      "6\n",
      "herbert\n",
      "4\n",
      "rich\n",
      "25\n",
      "unit\n",
      "5\n",
      "#lifeofabrunette\n",
      "1\n",
      "out\n",
      "9\n",
      "north\n",
      "41\n",
      "frankie\n",
      "1\n",
      "#yoosu\n",
      "1\n",
      "shortie\n",
      "1\n",
      "alktheer\n",
      "1\n",
      "sheron\n",
      "4\n",
      "domino\n",
      "4\n",
      "snagging\n",
      "1\n",
      "island\n",
      "4\n",
      "#lovelycouple\n",
      "1\n",
      "#socialmix\n",
      "1\n",
      "fantastic\n",
      "3\n",
      "goole\n",
      "4\n",
      "graduation\n",
      "1\n",
      "like\n",
      "3\n",
      "ellie\n",
      "5\n",
      "slurry\n",
      "2\n",
      "watching\n",
      "2\n",
      "receiving\n",
      "2\n",
      "granite\n",
      "20\n",
      "pamella\n",
      "2\n",
      "stent\n",
      "3\n",
      "throng\n",
      "4\n",
      "for sure\n",
      "1\n",
      "#goldcardmember\n",
      "1\n",
      "#xelha\n",
      "1\n",
      "hurry\n",
      "1\n",
      "brother\n",
      "1\n",
      "mere\n",
      "12\n",
      "amaze\n",
      "1\n",
      "night\n",
      "5\n",
      "#getheremay\n",
      "1\n",
      "#noitsnotfordrugsoranythingbad\n",
      "1\n",
      "indescribable\n",
      "1\n",
      "the\n",
      "151\n",
      "where\n",
      "7\n",
      "hihihiii\n",
      "1\n",
      "coca-cola\n",
      "1\n",
      "igadgetware\n",
      "1\n",
      "nakedly\n",
      "1\n",
      "jeeta\n",
      "1\n",
      "away\n",
      "50\n",
      "three\n",
      "3\n",
      "twice\n",
      "4\n",
      "epicness\n",
      "1\n",
      "sparksnoteing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-478106ea92b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Get a list of `likely` options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\etienne\\anaconda2\\envs\\py37\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36mcandidates\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;31m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distance\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m             \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__edit_distance_alt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\etienne\\anaconda2\\envs\\py37\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36m__edit_distance_alt\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_if_should_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         ]\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0me2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtmp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medit_distance_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_if_should_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\etienne\\anaconda2\\envs\\py37\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_if_should_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         ]\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0me2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtmp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medit_distance_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_if_should_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\etienne\\anaconda2\\envs\\py37\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36mknown\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 corpus \"\"\"\n\u001b[0;32m    187\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mENSURE_UNICODE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_case_sensitive\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         return set(\n\u001b[0;32m    190\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\etienne\\anaconda2\\envs\\py37\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 corpus \"\"\"\n\u001b[0;32m    187\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mENSURE_UNICODE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_case_sensitive\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         return set(\n\u001b[0;32m    190\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(len(spell.candidates(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n",
      "e\n",
      "s\n",
      "t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 'test'\n",
    "u = 'tess'\n",
    "count = 0\n",
    "while count < len(t):\n",
    "    print(t[count])\n",
    "    count += 1\n",
    "u==t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_identifier(word, spell):\n",
    "    \"\"\" Function takes a word and will answer whether there are repeated characters in order to correct them\n",
    "        if they are misspelled.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    # if word == spellchecker.correction(word):\n",
    "    while count < len(word):\n",
    "        if count + 1 < len(word):\n",
    "            if word[count] == word[count + 1] and len(spell.unknown([word])) == 1:\n",
    "                return True\n",
    "        count += 1\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repeat_indices(data):\n",
    "    \"\"\" Function that returns a list of indices for each word that has repeating letters (in data).\"\"\"\n",
    "    rep = []\n",
    "    spell = SpellChecker()\n",
    "    for i, w in enumerate(data.filewords):\n",
    "        if repeat_identifier(w, spell):\n",
    "            rep.append(i)\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 20,\n",
       " 21,\n",
       " 50,\n",
       " 52,\n",
       " 54,\n",
       " 66,\n",
       " 80,\n",
       " 89,\n",
       " 90,\n",
       " 99,\n",
       " 102,\n",
       " 112,\n",
       " 124,\n",
       " 133,\n",
       " 145,\n",
       " 147,\n",
       " 159,\n",
       " 163,\n",
       " 171,\n",
       " 176,\n",
       " 186,\n",
       " 198,\n",
       " 205,\n",
       " 212,\n",
       " 218,\n",
       " 229,\n",
       " 230,\n",
       " 250,\n",
       " 261,\n",
       " 271,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 294,\n",
       " 298,\n",
       " 300,\n",
       " 301,\n",
       " 316,\n",
       " 334,\n",
       " 337,\n",
       " 339,\n",
       " 352,\n",
       " 357,\n",
       " 362,\n",
       " 363,\n",
       " 368,\n",
       " 371,\n",
       " 377,\n",
       " 385,\n",
       " 389,\n",
       " 390,\n",
       " 402,\n",
       " 425,\n",
       " 438,\n",
       " 448,\n",
       " 462,\n",
       " 463,\n",
       " 469,\n",
       " 470,\n",
       " 473,\n",
       " 475,\n",
       " 491,\n",
       " 496,\n",
       " 523,\n",
       " 530,\n",
       " 537,\n",
       " 551,\n",
       " 562,\n",
       " 572,\n",
       " 579,\n",
       " 585,\n",
       " 599,\n",
       " 655,\n",
       " 669,\n",
       " 688,\n",
       " 699,\n",
       " 702,\n",
       " 703,\n",
       " 707,\n",
       " 709,\n",
       " 712,\n",
       " 717,\n",
       " 722,\n",
       " 724,\n",
       " 725,\n",
       " 736,\n",
       " 738,\n",
       " 752,\n",
       " 754,\n",
       " 763,\n",
       " 765,\n",
       " 771,\n",
       " 778,\n",
       " 780,\n",
       " 790,\n",
       " 796,\n",
       " 801,\n",
       " 810,\n",
       " 811,\n",
       " 812,\n",
       " 816,\n",
       " 817,\n",
       " 822,\n",
       " 834,\n",
       " 841,\n",
       " 844,\n",
       " 846,\n",
       " 854,\n",
       " 861,\n",
       " 865,\n",
       " 901,\n",
       " 933,\n",
       " 938,\n",
       " 941,\n",
       " 942,\n",
       " 952,\n",
       " 954,\n",
       " 958,\n",
       " 968,\n",
       " 969,\n",
       " 972,\n",
       " 974,\n",
       " 986,\n",
       " 1001,\n",
       " 1021,\n",
       " 1029,\n",
       " 1035,\n",
       " 1040,\n",
       " 1046,\n",
       " 1054,\n",
       " 1061,\n",
       " 1064,\n",
       " 1070,\n",
       " 1079,\n",
       " 1082,\n",
       " 1088,\n",
       " 1093,\n",
       " 1113,\n",
       " 1120,\n",
       " 1175,\n",
       " 1230,\n",
       " 1232,\n",
       " 1241,\n",
       " 1242,\n",
       " 1256,\n",
       " 1261,\n",
       " 1277,\n",
       " 1297,\n",
       " 1300,\n",
       " 1318,\n",
       " 1320,\n",
       " 1326,\n",
       " 1348,\n",
       " 1357,\n",
       " 1365,\n",
       " 1367,\n",
       " 1390,\n",
       " 1395,\n",
       " 1405,\n",
       " 1425,\n",
       " 1437,\n",
       " 1443,\n",
       " 1446,\n",
       " 1450,\n",
       " 1451,\n",
       " 1455,\n",
       " 1481,\n",
       " 1498,\n",
       " 1502,\n",
       " 1505,\n",
       " 1525,\n",
       " 1529,\n",
       " 1535,\n",
       " 1540,\n",
       " 1543,\n",
       " 1546,\n",
       " 1549,\n",
       " 1554,\n",
       " 1566,\n",
       " 1567,\n",
       " 1573,\n",
       " 1576,\n",
       " 1608,\n",
       " 1609,\n",
       " 1612,\n",
       " 1620,\n",
       " 1621,\n",
       " 1622,\n",
       " 1645,\n",
       " 1647,\n",
       " 1669,\n",
       " 1675,\n",
       " 1676,\n",
       " 1677,\n",
       " 1686,\n",
       " 1697,\n",
       " 1725,\n",
       " 1750,\n",
       " 1764,\n",
       " 1785,\n",
       " 1786,\n",
       " 1790,\n",
       " 1793,\n",
       " 1794,\n",
       " 1804,\n",
       " 1805,\n",
       " 1806,\n",
       " 1809,\n",
       " 1812,\n",
       " 1816,\n",
       " 1817,\n",
       " 1822,\n",
       " 1823,\n",
       " 1826,\n",
       " 1829,\n",
       " 1832,\n",
       " 1840,\n",
       " 1841,\n",
       " 1844,\n",
       " 1855,\n",
       " 1861,\n",
       " 1865,\n",
       " 1867,\n",
       " 1871,\n",
       " 1873,\n",
       " 1875,\n",
       " 1892,\n",
       " 1896,\n",
       " 1902,\n",
       " 1920,\n",
       " 1948,\n",
       " 1955,\n",
       " 1956,\n",
       " 1976,\n",
       " 1988,\n",
       " 1992,\n",
       " 1993,\n",
       " 2005,\n",
       " 2008,\n",
       " 2013,\n",
       " 2019,\n",
       " 2022,\n",
       " 2034,\n",
       " 2051,\n",
       " 2057,\n",
       " 2068,\n",
       " 2074,\n",
       " 2092,\n",
       " 2094,\n",
       " 2104,\n",
       " 2107,\n",
       " 2152,\n",
       " 2157,\n",
       " 2169,\n",
       " 2172,\n",
       " 2183,\n",
       " 2184,\n",
       " 2190,\n",
       " 2215,\n",
       " 2225,\n",
       " 2229,\n",
       " 2237,\n",
       " 2242,\n",
       " 2244,\n",
       " 2246,\n",
       " 2252,\n",
       " 2261,\n",
       " 2280,\n",
       " 2284,\n",
       " 2292,\n",
       " 2294,\n",
       " 2297,\n",
       " 2303,\n",
       " 2314,\n",
       " 2334,\n",
       " 2341,\n",
       " 2354,\n",
       " 2358,\n",
       " 2364,\n",
       " 2403,\n",
       " 2413,\n",
       " 2421,\n",
       " 2444,\n",
       " 2454,\n",
       " 2463,\n",
       " 2482,\n",
       " 2491,\n",
       " 2498,\n",
       " 2520,\n",
       " 2532,\n",
       " 2535,\n",
       " 2537,\n",
       " 2548,\n",
       " 2564,\n",
       " 2588,\n",
       " 2600,\n",
       " 2607,\n",
       " 2612,\n",
       " 2613,\n",
       " 2615,\n",
       " 2617,\n",
       " 2619,\n",
       " 2636,\n",
       " 2639,\n",
       " 2642,\n",
       " 2643,\n",
       " 2650,\n",
       " 2656,\n",
       " 2661,\n",
       " 2671,\n",
       " 2684,\n",
       " 2697,\n",
       " 2701,\n",
       " 2704,\n",
       " 2708,\n",
       " 2709,\n",
       " 2711,\n",
       " 2715,\n",
       " 2720,\n",
       " 2742,\n",
       " 2767,\n",
       " 2780,\n",
       " 2799,\n",
       " 2801,\n",
       " 2808,\n",
       " 2833,\n",
       " 2834,\n",
       " 2851,\n",
       " 2866,\n",
       " 2877,\n",
       " 2881,\n",
       " 2884,\n",
       " 2888,\n",
       " 2889,\n",
       " 2891,\n",
       " 2896,\n",
       " 2903,\n",
       " 2904,\n",
       " 2910,\n",
       " 2912,\n",
       " 2931,\n",
       " 2936,\n",
       " 2938,\n",
       " 2965,\n",
       " 2971,\n",
       " 2974,\n",
       " 2982,\n",
       " 2983,\n",
       " 2997,\n",
       " 3002,\n",
       " 3006,\n",
       " 3035,\n",
       " 3039,\n",
       " 3050,\n",
       " 3051,\n",
       " 3061,\n",
       " 3071,\n",
       " 3077,\n",
       " 3082,\n",
       " 3086,\n",
       " 3097,\n",
       " 3106,\n",
       " 3109,\n",
       " 3115,\n",
       " 3118,\n",
       " 3121,\n",
       " 3123,\n",
       " 3125,\n",
       " 3139,\n",
       " 3167,\n",
       " 3169,\n",
       " 3184,\n",
       " 3186,\n",
       " 3195,\n",
       " 3199,\n",
       " 3200,\n",
       " 3238,\n",
       " 3242,\n",
       " 3246,\n",
       " 3266,\n",
       " 3273,\n",
       " 3278,\n",
       " 3281,\n",
       " 3286,\n",
       " 3306,\n",
       " 3313,\n",
       " 3325,\n",
       " 3336,\n",
       " 3352,\n",
       " 3354,\n",
       " 3360,\n",
       " 3364,\n",
       " 3366,\n",
       " 3378,\n",
       " 3380,\n",
       " 3385,\n",
       " 3391,\n",
       " 3400,\n",
       " 3405,\n",
       " 3419,\n",
       " 3432,\n",
       " 3433,\n",
       " 3435,\n",
       " 3440,\n",
       " 3453,\n",
       " 3455,\n",
       " 3460,\n",
       " 3461,\n",
       " 3462,\n",
       " 3463,\n",
       " 3465,\n",
       " 3479,\n",
       " 3489,\n",
       " 3494,\n",
       " 3498,\n",
       " 3506,\n",
       " 3577,\n",
       " 3606,\n",
       " 3612,\n",
       " 3615,\n",
       " 3644,\n",
       " 3645,\n",
       " 3650,\n",
       " 3659,\n",
       " 3666,\n",
       " 3674,\n",
       " 3712,\n",
       " 3720,\n",
       " 3726,\n",
       " 3744,\n",
       " 3752,\n",
       " 3758,\n",
       " 3766,\n",
       " 3790,\n",
       " 3795,\n",
       " 3806,\n",
       " 3809,\n",
       " 3814,\n",
       " 3826,\n",
       " 3830,\n",
       " 3845,\n",
       " 3850,\n",
       " 3854,\n",
       " 3857,\n",
       " 3864,\n",
       " 3867,\n",
       " 3872,\n",
       " 3873,\n",
       " 3881,\n",
       " 3913,\n",
       " 3915,\n",
       " 3928,\n",
       " 3930,\n",
       " 3935,\n",
       " 3936,\n",
       " 3942,\n",
       " 3943,\n",
       " 3967,\n",
       " 3992,\n",
       " 4009,\n",
       " 4020,\n",
       " 4026,\n",
       " 4027,\n",
       " 4031,\n",
       " 4032,\n",
       " 4041,\n",
       " 4046,\n",
       " 4052,\n",
       " 4055,\n",
       " 4061,\n",
       " 4066,\n",
       " 4069,\n",
       " 4079,\n",
       " 4080,\n",
       " 4086,\n",
       " 4087,\n",
       " 4093,\n",
       " 4142,\n",
       " 4144,\n",
       " 4146,\n",
       " 4199,\n",
       " 4212,\n",
       " 4217,\n",
       " 4219,\n",
       " 4221,\n",
       " 4231,\n",
       " 4248,\n",
       " 4252,\n",
       " 4282,\n",
       " 4283,\n",
       " 4292,\n",
       " 4293,\n",
       " 4305,\n",
       " 4315,\n",
       " 4317,\n",
       " 4322,\n",
       " 4325,\n",
       " 4332,\n",
       " 4343,\n",
       " 4345,\n",
       " 4348,\n",
       " 4349,\n",
       " 4350,\n",
       " 4384,\n",
       " 4397,\n",
       " 4402,\n",
       " 4404,\n",
       " 4410,\n",
       " 4421,\n",
       " 4422,\n",
       " 4426,\n",
       " 4429,\n",
       " 4435,\n",
       " 4437,\n",
       " 4493,\n",
       " 4494,\n",
       " 4503,\n",
       " 4512,\n",
       " 4518,\n",
       " 4529,\n",
       " 4540,\n",
       " 4547,\n",
       " 4549,\n",
       " 4563,\n",
       " 4605,\n",
       " 4617,\n",
       " 4618,\n",
       " 4626,\n",
       " 4634,\n",
       " 4645,\n",
       " 4650,\n",
       " 4674,\n",
       " 4679,\n",
       " 4683,\n",
       " 4687,\n",
       " 4695,\n",
       " 4720,\n",
       " 4729,\n",
       " 4733,\n",
       " 4745,\n",
       " 4748,\n",
       " 4750,\n",
       " 4759,\n",
       " 4781,\n",
       " 4782,\n",
       " 4795,\n",
       " 4796,\n",
       " 4799,\n",
       " 4802,\n",
       " 4812,\n",
       " 4820,\n",
       " 4828,\n",
       " 4832,\n",
       " 4839,\n",
       " 4843,\n",
       " 4845,\n",
       " 4846,\n",
       " 4850,\n",
       " 4853,\n",
       " 4860,\n",
       " 4873,\n",
       " 4876,\n",
       " 4883,\n",
       " 4892,\n",
       " 4900,\n",
       " 4905,\n",
       " 4906,\n",
       " 4921,\n",
       " 4923,\n",
       " 4924,\n",
       " 4932,\n",
       " 4943,\n",
       " 4965,\n",
       " 4970,\n",
       " 4975,\n",
       " 4979,\n",
       " 4980,\n",
       " 4990,\n",
       " 5004,\n",
       " 5005,\n",
       " 5015,\n",
       " 5029,\n",
       " 5030,\n",
       " 5034,\n",
       " 5047,\n",
       " 5051,\n",
       " 5062,\n",
       " 5063,\n",
       " 5085,\n",
       " 5093,\n",
       " 5102,\n",
       " 5133,\n",
       " 5134,\n",
       " 5138,\n",
       " 5149,\n",
       " 5157,\n",
       " 5169,\n",
       " 5183,\n",
       " 5195,\n",
       " 5196,\n",
       " 5202,\n",
       " 5206,\n",
       " 5212,\n",
       " 5224,\n",
       " 5225,\n",
       " 5231,\n",
       " 5235,\n",
       " 5238,\n",
       " 5241,\n",
       " 5244,\n",
       " 5256,\n",
       " 5258,\n",
       " 5262,\n",
       " 5264,\n",
       " 5270,\n",
       " 5271,\n",
       " 5288,\n",
       " 5299,\n",
       " 5305,\n",
       " 5317,\n",
       " 5321,\n",
       " 5324,\n",
       " 5327,\n",
       " 5337,\n",
       " 5340,\n",
       " 5349,\n",
       " 5359,\n",
       " 5369,\n",
       " 5372,\n",
       " 5385,\n",
       " 5390,\n",
       " 5392,\n",
       " 5412,\n",
       " 5417,\n",
       " 5421,\n",
       " 5422,\n",
       " 5428,\n",
       " 5440,\n",
       " 5441,\n",
       " 5443,\n",
       " 5447,\n",
       " 5450,\n",
       " 5451,\n",
       " 5458,\n",
       " 5472,\n",
       " 5488,\n",
       " 5499,\n",
       " 5513,\n",
       " 5518,\n",
       " 5527,\n",
       " 5529,\n",
       " 5533,\n",
       " 5534,\n",
       " 5548,\n",
       " 5558,\n",
       " 5562,\n",
       " 5566,\n",
       " 5582,\n",
       " 5584,\n",
       " 5593,\n",
       " 5595,\n",
       " 5607,\n",
       " 5649,\n",
       " 5653,\n",
       " 5659,\n",
       " 5662,\n",
       " 5667,\n",
       " 5687,\n",
       " 5708,\n",
       " 5729,\n",
       " 5750,\n",
       " 5771,\n",
       " 5789,\n",
       " 5794,\n",
       " 5800,\n",
       " 5806,\n",
       " 5809,\n",
       " 5811,\n",
       " 5817,\n",
       " 5819,\n",
       " 5822,\n",
       " 5831,\n",
       " 5840,\n",
       " 5869,\n",
       " 5876,\n",
       " 5890,\n",
       " 5891,\n",
       " 5898,\n",
       " 5904,\n",
       " 5914,\n",
       " 5935,\n",
       " 5949,\n",
       " 5966,\n",
       " 5970,\n",
       " 5980,\n",
       " 5992,\n",
       " 6011,\n",
       " 6020,\n",
       " 6023,\n",
       " 6028,\n",
       " 6044,\n",
       " 6048,\n",
       " 6050,\n",
       " 6062,\n",
       " 6070,\n",
       " 6071,\n",
       " 6074,\n",
       " 6090,\n",
       " 6116,\n",
       " 6127,\n",
       " 6139,\n",
       " 6140,\n",
       " 6152,\n",
       " 6165,\n",
       " 6173,\n",
       " 6190,\n",
       " 6198,\n",
       " 6217,\n",
       " 6222,\n",
       " 6226,\n",
       " 6231,\n",
       " 6250,\n",
       " 6258,\n",
       " 6260,\n",
       " 6272,\n",
       " 6280,\n",
       " 6282,\n",
       " 6283,\n",
       " 6292,\n",
       " 6297,\n",
       " 6312,\n",
       " 6325,\n",
       " 6332,\n",
       " 6334,\n",
       " 6336,\n",
       " 6350,\n",
       " 6364,\n",
       " 6365,\n",
       " 6382,\n",
       " 6383,\n",
       " 6400,\n",
       " 6401,\n",
       " 6418,\n",
       " 6423,\n",
       " 6424,\n",
       " 6425,\n",
       " 6442,\n",
       " 6445,\n",
       " 6447,\n",
       " 6448,\n",
       " 6449,\n",
       " 6451,\n",
       " 6465,\n",
       " 6479,\n",
       " 6496,\n",
       " 6501,\n",
       " 6509,\n",
       " 6510,\n",
       " 6513,\n",
       " 6526,\n",
       " 6531,\n",
       " 6545,\n",
       " 6553,\n",
       " 6557,\n",
       " 6567,\n",
       " 6572,\n",
       " 6582,\n",
       " 6598,\n",
       " 6632,\n",
       " 6641,\n",
       " 6656,\n",
       " 6660,\n",
       " 6663,\n",
       " 6691,\n",
       " 6692,\n",
       " 6704,\n",
       " 6711,\n",
       " 6716,\n",
       " 6724,\n",
       " 6741,\n",
       " 6746,\n",
       " 6767,\n",
       " 6772,\n",
       " 6782,\n",
       " 6790,\n",
       " 6793,\n",
       " 6801,\n",
       " 6803,\n",
       " 6804,\n",
       " 6813,\n",
       " 6823,\n",
       " 6833,\n",
       " 6838,\n",
       " 6852,\n",
       " 6857,\n",
       " 6869,\n",
       " 6872,\n",
       " 6888,\n",
       " 6891,\n",
       " 6919,\n",
       " 6925,\n",
       " 6939,\n",
       " 6940,\n",
       " 6949,\n",
       " 6961,\n",
       " 6974,\n",
       " 6995,\n",
       " 6996,\n",
       " 6998,\n",
       " 6999,\n",
       " 7000,\n",
       " 7005,\n",
       " 7009,\n",
       " 7013,\n",
       " 7020,\n",
       " 7023,\n",
       " 7027,\n",
       " 7035,\n",
       " 7047,\n",
       " 7054,\n",
       " 7055,\n",
       " 7067,\n",
       " 7069,\n",
       " 7070,\n",
       " 7072,\n",
       " 7095,\n",
       " 7128,\n",
       " 7130,\n",
       " 7134,\n",
       " 7144,\n",
       " 7168,\n",
       " 7177,\n",
       " 7179,\n",
       " 7182,\n",
       " 7184,\n",
       " 7186,\n",
       " 7187,\n",
       " 7195,\n",
       " 7218,\n",
       " 7223,\n",
       " 7228,\n",
       " 7241,\n",
       " 7245,\n",
       " 7251,\n",
       " 7271,\n",
       " 7274,\n",
       " 7275,\n",
       " 7300,\n",
       " 7301,\n",
       " 7321,\n",
       " 7342,\n",
       " 7348,\n",
       " 7354,\n",
       " 7357,\n",
       " 7369,\n",
       " 7372,\n",
       " 7376,\n",
       " 7378,\n",
       " 7380,\n",
       " 7390,\n",
       " 7400,\n",
       " 7402,\n",
       " 7405,\n",
       " 7412,\n",
       " 7415,\n",
       " 7416,\n",
       " 7421,\n",
       " 7431,\n",
       " 7432,\n",
       " 7436,\n",
       " 7469,\n",
       " 7482,\n",
       " 7485,\n",
       " 7486,\n",
       " 7508,\n",
       " 7537,\n",
       " 7548,\n",
       " 7551,\n",
       " 7556,\n",
       " 7560,\n",
       " 7563,\n",
       " 7565,\n",
       " 7568,\n",
       " 7580,\n",
       " 7594,\n",
       " 7597,\n",
       " 7612,\n",
       " 7621,\n",
       " 7622,\n",
       " 7628,\n",
       " 7633,\n",
       " 7641,\n",
       " 7643,\n",
       " 7664,\n",
       " 7669,\n",
       " 7678,\n",
       " 7685,\n",
       " 7686,\n",
       " 7699,\n",
       " 7700,\n",
       " 7704,\n",
       " 7709,\n",
       " 7728,\n",
       " 7748,\n",
       " 7753,\n",
       " 7763,\n",
       " 7774,\n",
       " 7781,\n",
       " 7790,\n",
       " 7793,\n",
       " 7796,\n",
       " 7801,\n",
       " 7807,\n",
       " 7816,\n",
       " 7822,\n",
       " 7826,\n",
       " 7831,\n",
       " 7832,\n",
       " 7842,\n",
       " 7845,\n",
       " 7852,\n",
       " 7854,\n",
       " 7884,\n",
       " 7889,\n",
       " 7896,\n",
       " 7906,\n",
       " 7907,\n",
       " 7912,\n",
       " 7914,\n",
       " 7930,\n",
       " 7936,\n",
       " 7938,\n",
       " 7944,\n",
       " 7949,\n",
       " 7954,\n",
       " 7955,\n",
       " 7956,\n",
       " 7970,\n",
       " 7976,\n",
       " 7984,\n",
       " 8000,\n",
       " 8003,\n",
       " 8020,\n",
       " 8023,\n",
       " 8027,\n",
       " 8041,\n",
       " 8062,\n",
       " 8070,\n",
       " 8071,\n",
       " 8080,\n",
       " 8086,\n",
       " 8089,\n",
       " 8092,\n",
       " 8099,\n",
       " 8107,\n",
       " 8127,\n",
       " 8139,\n",
       " 8147,\n",
       " 8148,\n",
       " 8153,\n",
       " 8155,\n",
       " 8160,\n",
       " 8162,\n",
       " 8166,\n",
       " 8172,\n",
       " 8178,\n",
       " 8180,\n",
       " 8194,\n",
       " 8195,\n",
       " 8203,\n",
       " 8204,\n",
       " 8220,\n",
       " 8228,\n",
       " 8237,\n",
       " 8247,\n",
       " 8257,\n",
       " 8266,\n",
       " 8269,\n",
       " 8271,\n",
       " 8297,\n",
       " 8319,\n",
       " 8323,\n",
       " 8328,\n",
       " 8336,\n",
       " 8345,\n",
       " 8346,\n",
       " 8352,\n",
       " 8356,\n",
       " 8357,\n",
       " 8358,\n",
       " 8359,\n",
       " 8371,\n",
       " 8377,\n",
       " 8388,\n",
       " 8389,\n",
       " 8394,\n",
       " 8425,\n",
       " 8432,\n",
       " 8448,\n",
       " ...]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_repeat_word(data, indices, onomatopee, abbreviations):\n",
    "    \"\"\" Function will correct the words, making sure to check among abbreviations and \n",
    "        onomatopees first before using the spell checker.\"\"\"\n",
    "    \n",
    "    abbs = split_list_of_tuples(abbreviations)\n",
    "    onos = split_list_of_tuples(onomatopee)\n",
    "    spell = SpellChecker()\n",
    "    for ind in indices:\n",
    "        w = data.iat[ind, 0]\n",
    "        cand_rep = construct_candidates_for_repeat_word(w)\n",
    "        found = False\n",
    "        for candidate in cand_rep:\n",
    "            if candidate in abbs:\n",
    "                # Check if the candidate is an abbreviation.\n",
    "                found = True\n",
    "                abb_index = abbs.index(candidate)\n",
    "                data.iat[ind, 2] = abbreviations[abb_index][1]\n",
    "                \n",
    "                tokens = data.iat[ind, 3]\n",
    "                tokens.append('rrrpp')\n",
    "                tokens.append('abbrvt')\n",
    "                \n",
    "                if len(abbreviations[abb_index]) == 3:\n",
    "                    tokens.append(abbreviations[abb_index][2])\n",
    "                    \n",
    "            elif candidate in onos:\n",
    "                # Check if the canditate is an onomatopee.\n",
    "                found = True\n",
    "                ono_index = onos.index(candidate)\n",
    "                data.iat[ind, 2] = onomatopee[ono_index][1]\n",
    "                \n",
    "                tokens = data.iat[ind, 3]\n",
    "                tokens.append('rrrpp')\n",
    "                tokens.append('ooo')\n",
    "                \n",
    "                if len(abbreviations[ono_index]) == 3:\n",
    "                    tokens.append(abbreviations[ono_index][2])\n",
    "            \n",
    "            elif len(spell.unknown([candidate])) != 1:  # Checks if the spelling is correct.\n",
    "                found = True\n",
    "                \n",
    "                data.iat[ind, 2] = candidate\n",
    "                \n",
    "                tokens = data.iat[ind, 3]\n",
    "                tokens.append('rrrpp')\n",
    "                \n",
    "        if not found:\n",
    "            # Take the candidate with the least amount of repetitions in it.\n",
    "            data.iat[ind, 2] = cand_rep[-1]\n",
    "            tokens = data.iat[ind, 3]\n",
    "            tokens.append('rrrpp')\n",
    "               \n",
    "    return data       \n",
    "                \n",
    "                \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' ' in 'ha ha ho'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list_of_corrections(data):\n",
    "    ls = []\n",
    "    for i, corr in enumerate(data.correction):\n",
    "        if \" \" in corr:\n",
    "            for word in corr.split():\n",
    "                ls.append(word)\n",
    "        else: ls.append(corr)\n",
    "            \n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_remainder(data, candidates = 3):\n",
    "    \n",
    "    spell = SpellChecker()\n",
    "    corrections = make_list_of_corrections(data)\n",
    "    mistakes = spell.unknown(corrections)\n",
    "    \n",
    "    for mistake in mistakes:\n",
    "        if len(spell.candidates(mistake)) <= candidates:\n",
    "            correct = spell.correction(mistake)\n",
    "            mis_ind = data.index[data.filewords == mistake].tolist()\n",
    "            \n",
    "            for ind in mis_ind:\n",
    "                data.iat[ind, 2] = correct\n",
    "        else: continue\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filewords</th>\n",
       "      <th>position</th>\n",
       "      <th>correction</th>\n",
       "      <th>tokens</th>\n",
       "      <th>alternative correction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>looved</td>\n",
       "      <td>(2, 8)</td>\n",
       "      <td>looved</td>\n",
       "      <td>0</td>\n",
       "      <td>looved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>facebook</td>\n",
       "      <td>(5, 6)</td>\n",
       "      <td>facebook</td>\n",
       "      <td>0</td>\n",
       "      <td>facebook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   filewords position correction  tokens alternative correction\n",
       "50    looved   (2, 8)     looved       0                 looved\n",
       "90  facebook   (5, 6)   facebook       0               facebook"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.loc[rep_ll]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repeat_markers(word):\n",
    "    \"\"\" Given a word with repeating characters, this method will compute\n",
    "        two lists :\n",
    "        repeat_markers : a list of indices where a repetion will occur\n",
    "        repeat_numbers : a list of such number of repetitions\n",
    "        \n",
    "        Example : the word 'heeloo' will return [1, 4] as repeat markers, and\n",
    "                  [1, 1] as repeat numbers. \n",
    "                  \"\"\"\n",
    "    count = 0\n",
    "    # initialise the indicators of repeatedness in the word\n",
    "    # the repeat_markers will be a list of the first indices where a character will repeat.\n",
    "    # The repeat_numbers will denote the amount of repetitions.\n",
    "    repeat_markers = []\n",
    "    repeat_numbers = []\n",
    "    \n",
    "    # Read the word and identify the repeating characters.\n",
    "    while count < len(word):\n",
    "        if count + 1 < len(word):\n",
    "            if word[count] == word[count + 1]:               \n",
    "                repeater = count\n",
    "                while repeater + 1 < len(word) and word[repeater] == word[repeater+1]:\n",
    "                    repeater += 1\n",
    "                repeat_numbers.append(repeater - count)\n",
    "                repeat_markers.append(count)\n",
    "                count = repeater\n",
    "        count += 1\n",
    "    \n",
    "    return repeat_markers, repeat_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_candidates_for_repeat_word(word):\n",
    "    \"\"\" Method takes a word as a string in input, this word must have at least one repeating character\n",
    "        otherwise an error will occur.\n",
    "        This method will return a list of words that have less repeating charcters in them (in decreasing order).\n",
    "        \"\"\"\n",
    "    \n",
    "    repm, repn = find_repeat_markers(word)\n",
    "    # We take repn, and return an array of all the possibilities we can have for repeated letters :\n",
    "    # For instance in heeloo, we have repn = [1, 1] therefore all_possibilities will be \n",
    "    # np.array([[0,0], [1, 0], [0, 1], [1, 1]])\n",
    "    all_possibilities = np.array(np.meshgrid(*[range(x+1) for x in repn])).T.reshape(-1, len(repn))\n",
    "    \n",
    "    # We sort this array so that it goes from the least changes to be made, to the more changes to be made:\n",
    "    # So we sort w.r.t the sum of elements in each row\n",
    "    \n",
    "    idx = np.argsort(np.sum(all_possibilities, 1))\n",
    "    sorted_possibilities = np.take(all_possibilities, idx, axis=0)[::-1]  # In decreasing order\n",
    "    \n",
    "    candidates = []\n",
    "    # Create the list of repeated characters\n",
    "    repeated_chars = []\n",
    "    for i in repm:\n",
    "        repeated_chars.append(word[i])\n",
    "    \n",
    "    for ind in range(len(sorted_possibilities)):\n",
    "        case = sorted_possibilities[ind]\n",
    "        # Create the instance of repeated characters\n",
    "        rep_chars_in_candidate = []\n",
    "        for i, s in enumerate(repeated_chars):\n",
    "            rep_chars_in_candidate.append(s * (case[i] + 1))\n",
    "        # Now create the candidate\n",
    "        candidate = word[:repm[0]]\n",
    "        for index in range(len(repeated_chars)):\n",
    "            candidate += rep_chars_in_candidate[index]\n",
    "            if index + 1 != len(rep_chars_in_candidate):\n",
    "                candidate += word[repm[index] + repn[index] + 1: repm[index + 1]]  # Take the part of the word inbetween repetitions\n",
    "            else: continue\n",
    "        \n",
    "        candidate += word[repm[-1] + repn[-1] + 1:]\n",
    "        candidates.append(candidate)\n",
    "        \n",
    "    return candidates\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = 'saaallutt'\n",
    "t2 = 'dacodaac'\n",
    "t3 = 'aaaaaaaaaaaaaaaaa'\n",
    "t4 = 'enoooormeeee'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1, r2 = find_repeat_candidates(t4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 3]\n",
      " [3 2]\n",
      " [2 3]\n",
      " [3 1]\n",
      " [2 2]\n",
      " [1 3]\n",
      " [3 0]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [0 3]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 0]]\n",
      "['oooo', 'eeee']\n",
      "['oooo', 'eee']\n",
      "['ooo', 'eeee']\n",
      "['oooo', 'ee']\n",
      "['ooo', 'eee']\n",
      "['oo', 'eeee']\n",
      "['oooo', 'e']\n",
      "['ooo', 'ee']\n",
      "['oo', 'eee']\n",
      "['o', 'eeee']\n",
      "['ooo', 'e']\n",
      "['oo', 'ee']\n",
      "['o', 'eee']\n",
      "['oo', 'e']\n",
      "['o', 'ee']\n",
      "['o', 'e']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['enoooormeeee',\n",
       " 'enoooormeee',\n",
       " 'enooormeeee',\n",
       " 'enoooormee',\n",
       " 'enooormeee',\n",
       " 'enoormeeee',\n",
       " 'enoooorme',\n",
       " 'enooormee',\n",
       " 'enoormeee',\n",
       " 'enormeeee',\n",
       " 'enooorme',\n",
       " 'enoormee',\n",
       " 'enormeee',\n",
       " 'enoorme',\n",
       " 'enormee',\n",
       " 'enorme']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_candidates_for_repeat_word(t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 8]\n",
      "[4, 4]\n"
     ]
    }
   ],
   "source": [
    "print(r1)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 1)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['te', 'tee']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = [['t', 'e'], ['t', 'e', 'e']]\n",
    "hu= []\n",
    "for it in h:\n",
    "    hu.append(\"\".join(it))\n",
    "hu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(np.meshgrid([0, 1, 2], [0, 1, 2], [0, 1, 2])).T.reshape(-1, 3)\n",
    "shape = (3, 3, 3)\n",
    "q = np.array(np.meshgrid(*[range(x) for x in shape])).T.reshape(-1, len(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 2, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 1, 0],\n",
       "       [1, 2, 0],\n",
       "       [2, 0, 0],\n",
       "       [2, 1, 0],\n",
       "       [2, 2, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [0, 2, 1],\n",
       "       [1, 0, 1],\n",
       "       [1, 1, 1],\n",
       "       [1, 2, 1],\n",
       "       [2, 0, 1],\n",
       "       [2, 1, 1],\n",
       "       [2, 2, 1],\n",
       "       [0, 0, 2],\n",
       "       [0, 1, 2],\n",
       "       [0, 2, 2],\n",
       "       [1, 0, 2],\n",
       "       [1, 1, 2],\n",
       "       [1, 2, 2],\n",
       "       [2, 0, 2],\n",
       "       [2, 1, 2],\n",
       "       [2, 2, 2]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(q, 1)\n",
    "idx = np.argsort(np.sum(q, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4,\n",
       "       4, 5, 5, 5, 6])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.take(q, idx, axis=0), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 2],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [0, 1, 2],\n",
       "       [0, 0, 2],\n",
       "       [0, 1, 2],\n",
       "       [0, 2, 2],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [0, 1, 2],\n",
       "       [0, 1, 1],\n",
       "       [1, 1, 1],\n",
       "       [1, 1, 2],\n",
       "       [0, 1, 2],\n",
       "       [1, 1, 2],\n",
       "       [1, 2, 2],\n",
       "       [0, 0, 2],\n",
       "       [0, 1, 2],\n",
       "       [0, 2, 2],\n",
       "       [0, 1, 2],\n",
       "       [1, 1, 2],\n",
       "       [1, 2, 2],\n",
       "       [0, 2, 2],\n",
       "       [1, 2, 2],\n",
       "       [2, 2, 2]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[0, 0, 0],\n",
       "         [1, 1, 1],\n",
       "         [2, 2, 2]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [1, 1, 1],\n",
       "         [2, 2, 2]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [1, 1, 1],\n",
       "         [2, 2, 2]]]),\n",
       " array([[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1]],\n",
       " \n",
       "        [[2, 2, 2],\n",
       "         [2, 2, 2],\n",
       "         [2, 2, 2]]]),\n",
       " array([[[0, 1, 2],\n",
       "         [0, 1, 2],\n",
       "         [0, 1, 2]],\n",
       " \n",
       "        [[0, 1, 2],\n",
       "         [0, 1, 2],\n",
       "         [0, 1, 2]],\n",
       " \n",
       "        [[0, 1, 2],\n",
       "         [0, 1, 2],\n",
       "         [0, 1, 2]]])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.meshgrid([0, 1, 2], [0, 1, 2], [0, 1, 2])\n",
    "#shape = (3, 3, 3)\n",
    "#np.meshgrid(*[range(x) for x in shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "a\n",
      "0\n",
      "b\n",
      "1\n",
      "c\n"
     ]
    }
   ],
   "source": [
    "a = ['a', 'b', 'c']\n",
    "for i, c in enumerate(a, -1):\n",
    "    print(i)\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'heeelloo'\n",
    "b = bytearray('elo', encoding='utf8')\n",
    "pows=[2, 0, 3]\n",
    "#np.array(['e', 'l', 'o'])*np.array([3, 2, 1])\n",
    "u = ['e', 'l', 'o']\n",
    "new_u = []\n",
    "for ind, rep in enumerate(u):\n",
    "    new_u.append(rep *pows[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ee', '', 'ooo']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-96e52e98d81e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "i = [1, 2, 3]\n",
    "i[len(i)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
